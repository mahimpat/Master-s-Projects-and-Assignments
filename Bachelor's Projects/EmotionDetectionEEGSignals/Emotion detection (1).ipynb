{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\Users\\anaconda\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#import pyeeg as pe\n",
    "import pickle as pickle\n",
    "import pandas as pd\n",
    "import math\n",
    "import keras\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = [1,2,3,4,6,11,13,17,19,20,21,25,29,31] #14 Channels chosen to fit Emotiv Epoch+\n",
    "band = [4,8,12,16,25,45] #5 bands\n",
    "window_size = 256 #Averaging band power of 2 sec\n",
    "step_size = 16 #Each 0.125 sec update once\n",
    "sample_rate = 128 #Sampling rate of 128 Hz\n",
    "subjectList = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31','32']\n",
    "#List of subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFT_Processing (sub, channel, band, window_size, step_size, sample_rate):\n",
    "    '''\n",
    "    arguments:  string subject\n",
    "                list channel indice\n",
    "                list band\n",
    "                int window size for FFT\n",
    "                int step size for FFT\n",
    "                int sample rate for FFT\n",
    "    return:     void\n",
    "    '''\n",
    "    meta = []\n",
    "    with open('data\\s' + sub + '.dat', 'rb') as file:\n",
    "\n",
    "        subject = pickle.load(file, encoding='latin1') \n",
    "\n",
    "        for i in range (0,40):\n",
    "            # loop over 0-39 trails\n",
    "            data = subject[\"data\"][i]\n",
    "            labels = subject[\"labels\"][i]\n",
    "            start = 0;\n",
    "\n",
    "            while start + window_size < data.shape[1]:\n",
    "                meta_array = []\n",
    "                meta_data = [] \n",
    "                for j in channel:\n",
    "                    X = data[j][start : start + window_size] #Slice raw data over 2 sec, at interval of 0.125 sec\n",
    "                    Y = pe.bin_power(X, band, sample_rate) #FFT over 2 sec of channel j, in seq of theta, alpha, low beta, high beta, gamma\n",
    "                    meta_data = meta_data + list(Y[0])\n",
    "\n",
    "                meta_array.append(np.array(meta_data))\n",
    "                meta_array.append(labels)\n",
    "\n",
    "                meta.append(np.array(meta_array))    \n",
    "                start = start + step_size\n",
    "                \n",
    "        meta = np.array(meta)\n",
    "        np.save('out\\s' + sub, meta, allow_pickle=True, fix_imports=True)\n",
    "\n",
    "def testing (M, L, model):\n",
    "    '''\n",
    "    arguments:  M: testing dataset\n",
    "                L: testing dataset label\n",
    "                model: scikit-learn model\n",
    "\n",
    "    return:     void\n",
    "    '''\n",
    "    output = model.predict(M[0:78080:32])\n",
    "    label = L[0:78080:32]\n",
    "\n",
    "    k = 0\n",
    "    l = 0\n",
    "\n",
    "    for i in range(len(label)):\n",
    "        k = k + (output[i] - label[i])*(output[i] - label[i]) #square difference \n",
    "\n",
    "        #a good guess\n",
    "        if (output[i] > 5 and label[i] > 5):\n",
    "            l = l + 1\n",
    "        elif (output[i] < 5 and label[i] <5):\n",
    "            l = l + 1\n",
    "\n",
    "    print (\"l2 error:\", k/len(label), \"classification accuracy:\", l / len(label),l, len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subjects in subjectList:\n",
    "    FFT_Processing (subjects, channel, band, window_size, step_size, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[array([ 613.66362104,  914.26452527,  581.10762003, 1325.82260571,\n",
       "       1289.39769516,  839.21372127, 1020.22180882,  793.12165806,\n",
       "       1395.90024294, 1342.07164326,  890.22676913,  872.67062005,\n",
       "        649.00027674, 1160.70520659, 1055.46821267,  468.74283227,\n",
       "        767.80856074,  652.33631512,  909.57716459,  940.76174179,\n",
       "        754.40253592, 1432.08782218,  741.79624272, 1302.84450579,\n",
       "        974.41136969,  735.55965696, 1080.67015958,  709.27921696,\n",
       "       1154.02044992, 1642.84431039,  749.28884007, 1272.20363317,\n",
       "        599.54120355, 1135.77364453, 1761.77593244,  698.95217605,\n",
       "        724.67448708,  576.40362949, 1528.90720979, 1383.73889684,\n",
       "        890.30364949,  772.30796174,  724.55453683, 1566.1412682 ,\n",
       "       1292.4622915 ,  488.08159447,  997.251733  ,  619.23197239,\n",
       "       1589.99446067, 1117.32566876,  500.74996638,  865.42960649,\n",
       "        533.3237301 , 1371.97901653, 1129.39305592,  879.29988967,\n",
       "       1075.75997835,  715.70829405, 1501.93433071, 1261.14900426,\n",
       "       1040.33439535,  981.16228357,  783.24178555, 1227.57510417,\n",
       "       1263.2048632 ,  990.49692363, 1092.85645041,  937.05417068,\n",
       "       1249.39416909, 1051.72100876]),\n",
       "        array([7.71, 7.6 , 6.9 , 7.83])],\n",
       "       [array([ 630.95783354,  855.12229968,  542.23944976, 1315.47422965,\n",
       "       1423.39847568,  814.3483235 ,  981.98991877,  691.53771966,\n",
       "       1299.17364212, 1397.73680738,  874.28686909,  803.67942251,\n",
       "        642.22103988, 1279.84943249, 1272.21592466,  409.70935898,\n",
       "        777.08572406,  582.75132998,  915.01573041,  926.1352815 ,\n",
       "        833.49138744, 1420.58777084,  781.42871521, 1283.35636054,\n",
       "        963.99753027,  750.57902313, 1135.89477224,  691.53633744,\n",
       "       1275.30231922, 1686.34960131,  772.41588586, 1380.32389324,\n",
       "        598.18330177, 1205.15610763, 1788.32553968,  695.81760655,\n",
       "        816.9153284 ,  622.66590532, 1461.14636215, 1416.62765899,\n",
       "        847.38266788,  846.99661442,  768.64827711, 1505.09381649,\n",
       "       1480.40263568,  622.06198789, 1020.24073512,  659.0542104 ,\n",
       "       1564.75503883, 1306.3871023 ,  604.82748754,  890.10942222,\n",
       "        538.04566785, 1343.57995989, 1140.54271712,  989.93603225,\n",
       "       1079.6667971 ,  738.11917709, 1414.55606272, 1241.15197734,\n",
       "       1067.9138    , 1035.70601895,  751.7080667 , 1163.56009794,\n",
       "       1275.85756493,  960.48652908, 1096.32928936,  864.96937816,\n",
       "       1266.38240655, 1116.82747561]),\n",
       "        array([7.71, 7.6 , 6.9 , 7.83])],\n",
       "       [array([ 669.99869543,  931.94508636,  599.1499754 , 1314.55951038,\n",
       "       1264.35397439,  784.05199169,  937.73431731,  772.20359521,\n",
       "       1229.18215395, 1299.6146466 ,  925.46825261,  821.07566378,\n",
       "        769.61043181, 1220.59211756, 1074.24699882,  404.16367997,\n",
       "        680.88588538,  662.05135553,  827.5067835 ,  890.36692533,\n",
       "        874.06725047, 1443.02401786,  756.06028408, 1280.15090535,\n",
       "        983.34667737,  788.00295304, 1109.09486989,  672.52929476,\n",
       "       1197.68501288, 1697.14031956,  809.96936576, 1416.16271257,\n",
       "        565.77195882, 1161.2362173 , 1726.3116339 ,  821.95545598,\n",
       "        904.63019862,  447.83170938, 1505.38096275, 1322.56861587,\n",
       "        923.23376304,  850.38244913,  730.98628214, 1460.75026177,\n",
       "       1269.85969328,  746.65204217, 1045.93149277,  595.3492293 ,\n",
       "       1465.76605528, 1161.26982426,  633.60965484,  872.51183876,\n",
       "        534.43065182, 1326.41060421, 1093.58606018,  991.0975784 ,\n",
       "       1087.23619143,  718.40370423, 1409.66097905, 1271.19029528,\n",
       "       1011.98331522,  905.24561886,  850.15498784, 1199.20557573,\n",
       "       1309.02145321, 1059.58217799, 1114.91713762,  972.79673081,\n",
       "       1298.65455024, 1047.06213905]),\n",
       "        array([7.71, 7.6 , 6.9 , 7.83])],\n",
       "       ...,\n",
       "       [array([ 686.77752349,  857.78759602,  713.64727421, 1023.07013617,\n",
       "        961.57213962,  714.01553681,  877.88751134,  637.08516607,\n",
       "        922.8411563 , 1096.90090349,  745.55049212,  867.21046925,\n",
       "        679.62706684, 1489.45651306, 1928.92038623,  687.11977943,\n",
       "        582.44860106,  371.63753308,  947.78547003, 1035.71866983,\n",
       "        551.50704984,  602.69440023,  385.81973792,  694.49570118,\n",
       "        962.71327838,  812.3122605 , 1014.78336804,  613.53691214,\n",
       "       1019.01576328, 1262.89677016,  806.15097381, 1070.63575506,\n",
       "        741.72423191,  877.35417411, 1453.36087241,  589.16005212,\n",
       "        784.36961946,  581.08356526, 1072.40713448, 1232.66673166,\n",
       "        578.28921091,  801.40773975,  466.67820067, 1221.91572528,\n",
       "       1177.71818162,  842.26671001,  677.37254822,  561.96620464,\n",
       "       1336.9012815 , 1152.56608231,  639.96893047,  662.75641786,\n",
       "        664.17366698,  912.05013577,  909.32043238,  675.73728681,\n",
       "        749.66531354,  800.94555062, 1023.31404579, 1454.64609748,\n",
       "        636.40833962,  725.92069938,  688.65651743,  793.63595135,\n",
       "       1102.99576873,  802.24981399, 1525.58538493,  964.07582478,\n",
       "       1109.60738819,  998.35808583]),\n",
       "        array([5.1 , 7.12, 6.17, 5.97])],\n",
       "       [array([ 733.84628117,  835.22663231,  637.52971245, 1014.81724136,\n",
       "        978.02074225,  707.75212973,  867.24340784,  610.12892052,\n",
       "        997.1827823 , 1054.97984935,  909.60248796,  877.20644388,\n",
       "        597.64281676, 1457.10540346, 1937.78453997,  676.3621791 ,\n",
       "        577.61699653,  381.27337729,  885.78473215, 1060.47727726,\n",
       "        549.6249822 ,  636.88565551,  389.86599303,  725.04411617,\n",
       "       1024.54498893,  841.24505667,  977.99030922,  629.21561599,\n",
       "        954.57562734, 1180.45433356,  813.10765092, 1042.38913349,\n",
       "        762.06099245,  859.40795092, 1424.01164939,  629.69768265,\n",
       "        693.13985348,  671.69729834, 1053.26732567, 1238.27022703,\n",
       "        570.96094491,  781.84460174,  582.78765979, 1230.19053183,\n",
       "       1199.41910553,  869.15918396,  607.68354391,  624.49129501,\n",
       "       1366.12199524, 1189.21020595,  626.11847227,  588.37972492,\n",
       "        612.06126693,  886.12245807,  947.77074791,  656.32318293,\n",
       "        615.45884624,  784.90313597, 1096.88752978, 1380.71377742,\n",
       "        655.13925679,  655.02950422,  677.20678023,  891.88434816,\n",
       "       1056.15704114,  798.08144612, 1468.86305073,  908.8631646 ,\n",
       "       1072.45441054,  915.10939511]),\n",
       "        array([5.1 , 7.12, 6.17, 5.97])],\n",
       "       [array([ 641.07641201,  972.67741993,  617.19260041, 1019.70053622,\n",
       "        930.06990908,  605.11247527, 1049.40684848,  494.82579298,\n",
       "        997.11281718, 1068.86575519,  763.5372757 ,  944.26144175,\n",
       "        598.45883452, 1329.53173024, 1807.16035144,  643.63027203,\n",
       "        626.44261589,  408.2609213 ,  871.78716371, 1058.72704027,\n",
       "        578.33283827,  677.44641439,  397.26935582,  703.97971467,\n",
       "       1032.09297897,  748.90955906, 1194.99298256,  582.03902673,\n",
       "        947.82106257, 1123.08930312,  753.01673784, 1104.93963288,\n",
       "        768.16817444,  946.65615256, 1263.01847035,  581.82652111,\n",
       "        542.57991369,  706.73002277, 1038.95696153, 1157.41409482,\n",
       "        549.87928111,  749.72117086,  595.72092959, 1276.64580934,\n",
       "       1216.94704621,  928.25422117,  577.0387479 ,  635.52870542,\n",
       "       1405.61572741, 1153.70557805,  713.51603912,  496.88007342,\n",
       "        498.48351006,  918.79855424,  934.27381017,  717.04539697,\n",
       "        583.6175617 ,  739.21417897, 1081.63094902, 1388.09581382,\n",
       "        672.68173815,  584.84402581,  624.85877725,  839.20886328,\n",
       "       1048.9297573 ,  801.7171394 , 1138.63878461,  745.02231688,\n",
       "        941.81115026,  918.35550243]),\n",
       "        array([5.1 , 7.12, 6.17, 5.97])]], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f='out/s01.npy'\n",
    "data1=np.load(f,allow_pickle=True)\n",
    "\n",
    "data1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset: (468480, 70) (468480, 4)\n",
      "testing dataset: (78080, 70) (78080, 4)\n",
      "validation dataset: (78080, 70) (78080, 4)\n"
     ]
    }
   ],
   "source": [
    "#for subjects in subjectList:\n",
    "data_training = []\n",
    "label_training = []\n",
    "data_testing = []\n",
    "label_testing = []\n",
    "data_validation = []\n",
    "label_validation = []\n",
    "\n",
    "for subjects in subjectList:\n",
    "\n",
    "    with open('out\\s' + subjects + '.npy', 'rb') as file:\n",
    "        sub = np.load(file,allow_pickle=True)\n",
    "        for i in range (0,sub.shape[0]):\n",
    "            if i % 8 == 0:\n",
    "                data_testing.append(sub[i][0])\n",
    "                label_testing.append(sub[i][1])\n",
    "            elif i % 8 == 1:\n",
    "                data_validation.append(sub[i][0])\n",
    "                label_validation.append(sub[i][1])\n",
    "            else:\n",
    "                data_training.append(sub[i][0])\n",
    "                label_training.append(sub[i][1])\n",
    "\n",
    "np.save('out\\data_training', np.array(data_training), allow_pickle=True, fix_imports=True)\n",
    "np.save('out\\label_training', np.array(label_training), allow_pickle=True, fix_imports=True)\n",
    "print(\"training dataset:\", np.array(data_training).shape, np.array(label_training).shape)\n",
    "\n",
    "np.save('out\\data_testing', np.array(data_testing), allow_pickle=True, fix_imports=True)\n",
    "np.save('out\\label_testing', np.array(label_testing), allow_pickle=True, fix_imports=True)\n",
    "print(\"testing dataset:\", np.array(data_testing).shape, np.array(label_testing).shape)\n",
    "\n",
    "np.save('out\\data_validation', np.array(data_validation), allow_pickle=True, fix_imports=True)\n",
    "np.save('out\\label_validation', np.array(label_validation), allow_pickle=True, fix_imports=True)\n",
    "print(\"validation dataset:\", np.array(data_validation).shape, np.array(label_validation).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label is\n",
      "[[7.71 7.6  6.9  7.83]\n",
      " [7.71 7.6  6.9  7.83]\n",
      " [7.71 7.6  6.9  7.83]\n",
      " ...\n",
      " [7.15 4.03 9.   1.88]\n",
      " [7.15 4.03 9.   1.88]\n",
      " [7.15 4.03 9.   1.88]]\n",
      "data is\n",
      "[[ 669.99869543  931.94508636  599.1499754  ...  972.79673081\n",
      "  1298.65455024 1047.06213905]\n",
      " [ 701.67756136  959.02316075  504.41996142 ...  816.40842458\n",
      "  1282.61150713  940.88526074]\n",
      " [ 652.07814405  902.87545767  499.70784158 ...  794.93117558\n",
      "  1267.56250579 1080.44138408]\n",
      " ...\n",
      " [3239.23440751 2384.01274117 1432.64610359 ...  774.66545023\n",
      "  1615.84091962 2155.45755093]\n",
      " [1752.31624323 1081.01059277 1026.92090319 ...  851.46706987\n",
      "  1752.27215441 2021.39173282]\n",
      " [1647.55771023 1110.96362628 1021.01389992 ...  727.87091476\n",
      "  1919.71245753 2201.23100598]]\n"
     ]
    }
   ],
   "source": [
    "with open('out\\data_training.npy', 'rb') as fileTrain:\n",
    "    X  = np.load(fileTrain)\n",
    "    \n",
    "with open('out\\label_training.npy', 'rb') as fileTrainL:\n",
    "    Y  = np.load(fileTrainL)\n",
    "    \n",
    "print(\"label is\")    \n",
    "print(Y) \n",
    "\n",
    "print(\"data is\")\n",
    "print(X)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "mx=StandardScaler()\n",
    "X=mx.fit_transform(X)\n",
    "\n",
    "mx1=StandardScaler()\n",
    "Y=mx1.fit_transform(Y)\n",
    "\n",
    "Arousal_Train = np.ravel(Y[:, [0]])\n",
    "Valence_Train = np.ravel(Y[:, [1]])\n",
    "Domain_Train = np.ravel(Y[:, [2]])\n",
    "Like_Train = np.ravel(Y[:, [3]])\n",
    "\n",
    "\n",
    "with open('out\\data_validation.npy', 'rb') as fileTrain:\n",
    "    M  = np.load(fileTrain)\n",
    "    \n",
    "with open('out\\label_validation.npy', 'rb') as fileTrainL:\n",
    "    N  = np.load(fileTrainL)\n",
    "    \n",
    "mx3=StandardScaler()\n",
    "M=mx3.fit_transform(M)\n",
    "\n",
    "mx4=StandardScaler()\n",
    "N=mx4.fit_transform(N)    \n",
    "\n",
    "\n",
    "Arousal_Test = np.ravel(N[:, [0]])\n",
    "Valence_Test = np.ravel(N[:, [1]])\n",
    "Domain_Test = np.ravel(N[:, [2]])\n",
    "Like_Test = np.ravel(N[:, [3]])\n",
    "\n",
    "Z = np.ravel(Y[:, [1]])\n",
    "\n",
    "L = np.ravel(N[:, [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07077573, 0.09592068, 0.06082402, ..., 0.09702524, 0.1420525 ,\n",
       "        0.12527664],\n",
       "       [0.08616171, 0.11479897, 0.08108622, ..., 0.12895436, 0.1343576 ,\n",
       "        0.12305626],\n",
       "       [0.11127172, 0.10372576, 0.08633638, ..., 0.12147443, 0.11264148,\n",
       "        0.11188543],\n",
       "       ...,\n",
       "       [0.17611752, 0.06707506, 0.06382216, ..., 0.0382224 , 0.0541397 ,\n",
       "        0.07736363],\n",
       "       [0.20602547, 0.10511949, 0.05024369, ..., 0.02780425, 0.05650616,\n",
       "        0.06173489],\n",
       "       [0.17583567, 0.09009285, 0.05633646, ..., 0.02489085, 0.05812248,\n",
       "        0.0715824 ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label is\n",
      "[[7.71 7.6  6.9  7.83]\n",
      " [7.71 7.6  6.9  7.83]\n",
      " [7.71 7.6  6.9  7.83]\n",
      " ...\n",
      " [7.15 4.03 9.   1.88]\n",
      " [7.15 4.03 9.   1.88]\n",
      " [7.15 4.03 9.   1.88]]\n",
      "data is\n",
      "[[ 669.99869543  931.94508636  599.1499754  ...  972.79673081\n",
      "  1298.65455024 1047.06213905]\n",
      " [ 701.67756136  959.02316075  504.41996142 ...  816.40842458\n",
      "  1282.61150713  940.88526074]\n",
      " [ 652.07814405  902.87545767  499.70784158 ...  794.93117558\n",
      "  1267.56250579 1080.44138408]\n",
      " ...\n",
      " [3239.23440751 2384.01274117 1432.64610359 ...  774.66545023\n",
      "  1615.84091962 2155.45755093]\n",
      " [1752.31624323 1081.01059277 1026.92090319 ...  851.46706987\n",
      "  1752.27215441 2021.39173282]\n",
      " [1647.55771023 1110.96362628 1021.01389992 ...  727.87091476\n",
      "  1919.71245753 2201.23100598]]\n",
      "[7.6  7.6  7.6  ... 4.03 4.03 4.03]\n"
     ]
    }
   ],
   "source": [
    "with open('out\\data_training.npy', 'rb') as fileTrain:\n",
    "    X  = np.load(fileTrain,allow_pickle=True)\n",
    "    \n",
    "with open('out\\label_training.npy', 'rb') as fileTrainL:\n",
    "    Y  = np.load(fileTrainL,allow_pickle=True)\n",
    "print(\"label is\")    \n",
    "print(Y) \n",
    "\n",
    "print(\"data is\")\n",
    "print(X)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X = normalize(X)\n",
    "Z = np.ravel(Y[:, [1]])\n",
    "\n",
    "\n",
    "print(Z)\n",
    "\n",
    "Arousal_Train = np.ravel(Y[:, [0]])\n",
    "Valence_Train = np.ravel(Y[:, [1]])\n",
    "Domain_Train = np.ravel(Y[:, [2]])\n",
    "Like_Train = np.ravel(Y[:, [3]])\n",
    "ZZ=np.concatenate((Y[:, [1]],Y[:, [0]]),axis=1)\n",
    "\n",
    "\n",
    "with open('out\\data_validation.npy', 'rb') as fileTrain:\n",
    "    M  = np.load(fileTrain,allow_pickle=True)\n",
    "    \n",
    "with open('out\\label_validation.npy', 'rb') as fileTrainL:\n",
    "    N  = np.load(fileTrainL,allow_pickle=True)\n",
    "    \n",
    "    \n",
    "M = normalize(M)\n",
    "L = np.ravel(N[:, [1]])\n",
    "LL=np.concatenate((N[:, [1]],N[:, [0]]),axis=1)\n",
    "\n",
    "with open('out\\data_testing.npy', 'rb') as fileTrain:\n",
    "    A  = np.load(fileTrain,allow_pickle=True)\n",
    "    \n",
    "with open('out\\label_testing.npy', 'rb') as fileTrainL:\n",
    "    C  = np.load(fileTrainL,allow_pickle=True)\n",
    "    \n",
    "    \n",
    "A = normalize(M)\n",
    "B = np.ravel(C[:, [1]])\n",
    "\n",
    "\n",
    "Arousal_Test = np.ravel(N[:, [0]])\n",
    "Valence_Test = np.ravel(N[:, [1]])\n",
    "Domain_Test = np.ravel(N[:, [2]])\n",
    "Like_Test = np.ravel(N[:, [3]])\n",
    "\n",
    "BB=np.concatenate((N[:, [1]],N[:, [0]]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('out\\data_training.npy', 'rb') as fileTrain:\n",
    "    X  = np.load(fileTrain,allow_pickle=True)\n",
    "    \n",
    "with open('out\\label_training.npy', 'rb') as fileTrainL:\n",
    "    Y  = np.load(fileTrainL,allow_pickle=True)\n",
    "    \n",
    "with open('out\\data_validation.npy', 'rb') as fileTrain:\n",
    "    M  = np.load(fileTrain,allow_pickle=True)\n",
    "    \n",
    "with open('out\\label_validation.npy', 'rb') as fileTrainL:\n",
    "    N  = np.load(fileTrainL,allow_pickle=True)\n",
    "    \n",
    "Z = np.ravel(Y[:, [1]])\n",
    "\n",
    "L = np.ravel(N[:, [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZZ=list(map(tuple, ZZ))\n",
    "BB=list(map(tuple, BB))\n",
    "LL=list(map(tuple, LL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_training = ZZ[0:468480:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7.6, 7.71),\n",
       " (7.6, 7.71),\n",
       " (7.6, 7.71),\n",
       " (7.6, 7.71),\n",
       " (7.6, 7.71),\n",
       " (7.6, 7.71),\n",
       " (7.6, 7.71),\n",
       " (7.6, 7.71),\n",
       " (7.6, 7.71),\n",
       " (7.6, 7.71),\n",
       " (7.6, 7.71),\n",
       " (7.6, 7.71),\n",
       " (7.31, 8.1),\n",
       " (7.31, 8.1),\n",
       " (7.31, 8.1),\n",
       " (7.31, 8.1),\n",
       " (7.31, 8.1),\n",
       " (7.31, 8.1),\n",
       " (7.31, 8.1),\n",
       " (7.31, 8.1),\n",
       " (7.31, 8.1),\n",
       " (7.31, 8.1),\n",
       " (7.31, 8.1),\n",
       " (7.54, 8.58),\n",
       " (7.54, 8.58),\n",
       " (7.54, 8.58),\n",
       " (7.54, 8.58),\n",
       " (7.54, 8.58),\n",
       " (7.54, 8.58),\n",
       " (7.54, 8.58),\n",
       " (7.54, 8.58),\n",
       " (7.54, 8.58),\n",
       " (7.54, 8.58),\n",
       " (7.54, 8.58),\n",
       " (7.54, 8.58),\n",
       " (6.01, 4.94),\n",
       " (6.01, 4.94),\n",
       " (6.01, 4.94),\n",
       " (6.01, 4.94),\n",
       " (6.01, 4.94),\n",
       " (6.01, 4.94),\n",
       " (6.01, 4.94),\n",
       " (6.01, 4.94),\n",
       " (6.01, 4.94),\n",
       " (6.01, 4.94),\n",
       " (6.01, 4.94),\n",
       " (3.92, 6.96),\n",
       " (3.92, 6.96),\n",
       " (3.92, 6.96),\n",
       " (3.92, 6.96),\n",
       " (3.92, 6.96),\n",
       " (3.92, 6.96),\n",
       " (3.92, 6.96),\n",
       " (3.92, 6.96),\n",
       " (3.92, 6.96),\n",
       " (3.92, 6.96),\n",
       " (3.92, 6.96),\n",
       " (3.92, 6.96),\n",
       " (3.92, 8.27),\n",
       " (3.92, 8.27),\n",
       " (3.92, 8.27),\n",
       " (3.92, 8.27),\n",
       " (3.92, 8.27),\n",
       " (3.92, 8.27),\n",
       " (3.92, 8.27),\n",
       " (3.92, 8.27),\n",
       " (3.92, 8.27),\n",
       " (3.92, 8.27),\n",
       " (3.92, 8.27),\n",
       " (3.73, 7.44),\n",
       " (3.73, 7.44),\n",
       " (3.73, 7.44),\n",
       " (3.73, 7.44),\n",
       " (3.73, 7.44),\n",
       " (3.73, 7.44),\n",
       " (3.73, 7.44),\n",
       " (3.73, 7.44),\n",
       " (3.73, 7.44),\n",
       " (3.73, 7.44),\n",
       " (3.73, 7.44),\n",
       " (3.73, 7.44),\n",
       " (2.55, 7.32),\n",
       " (2.55, 7.32),\n",
       " (2.55, 7.32),\n",
       " (2.55, 7.32),\n",
       " (2.55, 7.32),\n",
       " (2.55, 7.32),\n",
       " (2.55, 7.32),\n",
       " (2.55, 7.32),\n",
       " (2.55, 7.32),\n",
       " (2.55, 7.32),\n",
       " (2.55, 7.32),\n",
       " (3.29, 4.04),\n",
       " (3.29, 4.04),\n",
       " (3.29, 4.04),\n",
       " (3.29, 4.04),\n",
       " (3.29, 4.04),\n",
       " (3.29, 4.04),\n",
       " (3.29, 4.04),\n",
       " (3.29, 4.04),\n",
       " (3.29, 4.04),\n",
       " (3.29, 4.04),\n",
       " (3.29, 4.04),\n",
       " (4.86, 1.99),\n",
       " (4.86, 1.99),\n",
       " (4.86, 1.99),\n",
       " (4.86, 1.99),\n",
       " (4.86, 1.99),\n",
       " (4.86, 1.99),\n",
       " (4.86, 1.99),\n",
       " (4.86, 1.99),\n",
       " (4.86, 1.99),\n",
       " (4.86, 1.99),\n",
       " (4.86, 1.99),\n",
       " (4.86, 1.99),\n",
       " (2.36, 2.99),\n",
       " (2.36, 2.99),\n",
       " (2.36, 2.99),\n",
       " (2.36, 2.99),\n",
       " (2.36, 2.99),\n",
       " (2.36, 2.99),\n",
       " (2.36, 2.99),\n",
       " (2.36, 2.99),\n",
       " (2.36, 2.99),\n",
       " (2.36, 2.99),\n",
       " (2.36, 2.99),\n",
       " (2.77, 2.71),\n",
       " (2.77, 2.71),\n",
       " (2.77, 2.71),\n",
       " (2.77, 2.71),\n",
       " (2.77, 2.71),\n",
       " (2.77, 2.71),\n",
       " (2.77, 2.71),\n",
       " (2.77, 2.71),\n",
       " (2.77, 2.71),\n",
       " (2.77, 2.71),\n",
       " (2.77, 2.71),\n",
       " (2.77, 2.71),\n",
       " (3.12, 1.95),\n",
       " (3.12, 1.95),\n",
       " (3.12, 1.95),\n",
       " (3.12, 1.95),\n",
       " (3.12, 1.95),\n",
       " (3.12, 1.95),\n",
       " (3.12, 1.95),\n",
       " (3.12, 1.95),\n",
       " (3.12, 1.95),\n",
       " (3.12, 1.95),\n",
       " (3.12, 1.95),\n",
       " (2.24, 4.18),\n",
       " (2.24, 4.18),\n",
       " (2.24, 4.18),\n",
       " (2.24, 4.18),\n",
       " (2.24, 4.18),\n",
       " (2.24, 4.18),\n",
       " (2.24, 4.18),\n",
       " (2.24, 4.18),\n",
       " (2.24, 4.18),\n",
       " (2.24, 4.18),\n",
       " (2.24, 4.18),\n",
       " (2.24, 4.18),\n",
       " (8.08, 3.17),\n",
       " (8.08, 3.17),\n",
       " (8.08, 3.17),\n",
       " (8.08, 3.17),\n",
       " (8.08, 3.17),\n",
       " (8.08, 3.17),\n",
       " (8.08, 3.17),\n",
       " (8.08, 3.17),\n",
       " (8.08, 3.17),\n",
       " (8.08, 3.17),\n",
       " (8.08, 3.17),\n",
       " (7.44, 6.81),\n",
       " (7.44, 6.81),\n",
       " (7.44, 6.81),\n",
       " (7.44, 6.81),\n",
       " (7.44, 6.81),\n",
       " (7.44, 6.81),\n",
       " (7.44, 6.81),\n",
       " (7.44, 6.81),\n",
       " (7.44, 6.81),\n",
       " (7.44, 6.81),\n",
       " (7.44, 6.81),\n",
       " (6.91, 2.46),\n",
       " (6.91, 2.46),\n",
       " (6.91, 2.46),\n",
       " (6.91, 2.46),\n",
       " (6.91, 2.46),\n",
       " (6.91, 2.46),\n",
       " (6.91, 2.46),\n",
       " (6.91, 2.46),\n",
       " (6.91, 2.46),\n",
       " (6.91, 2.46),\n",
       " (6.91, 2.46),\n",
       " (6.91, 2.46),\n",
       " (7.15, 7.23),\n",
       " (7.15, 7.23),\n",
       " (7.15, 7.23),\n",
       " (7.15, 7.23),\n",
       " (7.15, 7.23),\n",
       " (7.15, 7.23),\n",
       " (7.15, 7.23),\n",
       " (7.15, 7.23),\n",
       " (7.15, 7.23),\n",
       " (7.15, 7.23),\n",
       " (7.15, 7.23),\n",
       " (8.0, 7.17),\n",
       " (8.0, 7.17),\n",
       " (8.0, 7.17),\n",
       " (8.0, 7.17),\n",
       " (8.0, 7.17),\n",
       " (8.0, 7.17),\n",
       " (8.0, 7.17),\n",
       " (8.0, 7.17),\n",
       " (8.0, 7.17),\n",
       " (8.0, 7.17),\n",
       " (8.0, 7.17),\n",
       " (8.0, 7.17),\n",
       " (7.91, 8.26),\n",
       " (7.91, 8.26),\n",
       " (7.91, 8.26),\n",
       " (7.91, 8.26),\n",
       " (7.91, 8.26),\n",
       " (7.91, 8.26),\n",
       " (7.91, 8.26),\n",
       " (7.91, 8.26),\n",
       " (7.91, 8.26),\n",
       " (7.91, 8.26),\n",
       " (7.91, 8.26),\n",
       " (7.95, 9.0),\n",
       " (7.95, 9.0),\n",
       " (7.95, 9.0),\n",
       " (7.95, 9.0),\n",
       " (7.95, 9.0),\n",
       " (7.95, 9.0),\n",
       " (7.95, 9.0),\n",
       " (7.95, 9.0),\n",
       " (7.95, 9.0),\n",
       " (7.95, 9.0),\n",
       " (7.95, 9.0),\n",
       " (7.95, 9.0),\n",
       " (2.08, 7.09),\n",
       " (2.08, 7.09),\n",
       " (2.08, 7.09),\n",
       " (2.08, 7.09),\n",
       " (2.08, 7.09),\n",
       " (2.08, 7.09),\n",
       " (2.08, 7.09),\n",
       " (2.08, 7.09),\n",
       " (2.08, 7.09),\n",
       " (2.08, 7.09),\n",
       " (2.08, 7.09),\n",
       " (3.01, 8.15),\n",
       " (3.01, 8.15),\n",
       " (3.01, 8.15),\n",
       " (3.01, 8.15),\n",
       " (3.01, 8.15),\n",
       " (3.01, 8.15),\n",
       " (3.01, 8.15),\n",
       " (3.01, 8.15),\n",
       " (3.01, 8.15),\n",
       " (3.01, 8.15),\n",
       " (3.01, 8.15),\n",
       " (3.01, 8.15),\n",
       " (7.09, 7.04),\n",
       " (7.09, 7.04),\n",
       " (7.09, 7.04),\n",
       " (7.09, 7.04),\n",
       " (7.09, 7.04),\n",
       " (7.09, 7.04),\n",
       " (7.09, 7.04),\n",
       " (7.09, 7.04),\n",
       " (7.09, 7.04),\n",
       " (7.09, 7.04),\n",
       " (7.09, 7.04),\n",
       " (7.21, 8.86),\n",
       " (7.21, 8.86),\n",
       " (7.21, 8.86),\n",
       " (7.21, 8.86),\n",
       " (7.21, 8.86),\n",
       " (7.21, 8.86),\n",
       " (7.21, 8.86),\n",
       " (7.21, 8.86),\n",
       " (7.21, 8.86),\n",
       " (7.21, 8.86),\n",
       " (7.21, 8.86),\n",
       " (7.27, 7.28),\n",
       " (7.27, 7.28),\n",
       " (7.27, 7.28),\n",
       " (7.27, 7.28),\n",
       " (7.27, 7.28),\n",
       " (7.27, 7.28),\n",
       " (7.27, 7.28),\n",
       " (7.27, 7.28),\n",
       " (7.27, 7.28),\n",
       " (7.27, 7.28),\n",
       " (7.27, 7.28),\n",
       " (7.27, 7.28),\n",
       " (6.95, 7.35),\n",
       " (6.95, 7.35),\n",
       " (6.95, 7.35),\n",
       " (6.95, 7.35),\n",
       " (6.95, 7.35),\n",
       " (6.95, 7.35),\n",
       " (6.95, 7.35),\n",
       " (6.95, 7.35),\n",
       " (6.95, 7.35),\n",
       " (6.95, 7.35),\n",
       " (6.95, 7.35),\n",
       " (3.35, 3.88),\n",
       " (3.35, 3.88),\n",
       " (3.35, 3.88),\n",
       " (3.35, 3.88),\n",
       " (3.35, 3.88),\n",
       " (3.35, 3.88),\n",
       " (3.35, 3.88),\n",
       " (3.35, 3.88),\n",
       " (3.35, 3.88),\n",
       " (3.35, 3.88),\n",
       " (3.35, 3.88),\n",
       " (3.35, 3.88),\n",
       " (2.27, 1.36),\n",
       " (2.27, 1.36),\n",
       " (2.27, 1.36),\n",
       " (2.27, 1.36),\n",
       " (2.27, 1.36),\n",
       " (2.27, 1.36),\n",
       " (2.27, 1.36),\n",
       " (2.27, 1.36),\n",
       " (2.27, 1.36),\n",
       " (2.27, 1.36),\n",
       " (2.27, 1.36),\n",
       " (2.99, 2.08),\n",
       " (2.99, 2.08),\n",
       " (2.99, 2.08),\n",
       " (2.99, 2.08),\n",
       " (2.99, 2.08),\n",
       " (2.99, 2.08),\n",
       " (2.99, 2.08),\n",
       " (2.99, 2.08),\n",
       " (2.99, 2.08),\n",
       " (2.99, 2.08),\n",
       " (2.99, 2.08),\n",
       " (2.99, 2.08),\n",
       " (8.14, 3.03),\n",
       " (8.14, 3.03),\n",
       " (8.14, 3.03),\n",
       " (8.14, 3.03),\n",
       " (8.14, 3.03),\n",
       " (8.14, 3.03),\n",
       " (8.14, 3.03),\n",
       " (8.14, 3.03),\n",
       " (8.14, 3.03),\n",
       " (8.14, 3.03),\n",
       " (8.14, 3.03),\n",
       " (8.0, 2.28),\n",
       " (8.0, 2.28),\n",
       " (8.0, 2.28),\n",
       " (8.0, 2.28),\n",
       " (8.0, 2.28),\n",
       " (8.0, 2.28),\n",
       " (8.0, 2.28),\n",
       " (8.0, 2.28),\n",
       " (8.0, 2.28),\n",
       " (8.0, 2.28),\n",
       " (8.0, 2.28),\n",
       " (3.85, 3.81),\n",
       " (3.85, 3.81),\n",
       " (3.85, 3.81),\n",
       " (3.85, 3.81),\n",
       " (3.85, 3.81),\n",
       " (3.85, 3.81),\n",
       " (3.85, 3.81),\n",
       " (3.85, 3.81),\n",
       " (3.85, 3.81),\n",
       " (3.85, 3.81),\n",
       " (3.85, 3.81),\n",
       " (3.85, 3.81),\n",
       " (7.09, 2.28),\n",
       " (7.09, 2.28),\n",
       " (7.09, 2.28),\n",
       " (7.09, 2.28),\n",
       " (7.09, 2.28),\n",
       " (7.09, 2.28),\n",
       " (7.09, 2.28),\n",
       " (7.09, 2.28),\n",
       " (7.09, 2.28),\n",
       " (7.09, 2.28),\n",
       " (7.09, 2.28),\n",
       " (8.15, 2.06),\n",
       " (8.15, 2.06),\n",
       " (8.15, 2.06),\n",
       " (8.15, 2.06),\n",
       " (8.15, 2.06),\n",
       " (8.15, 2.06),\n",
       " (8.15, 2.06),\n",
       " (8.15, 2.06),\n",
       " (8.15, 2.06),\n",
       " (8.15, 2.06),\n",
       " (8.15, 2.06),\n",
       " (8.15, 2.06),\n",
       " (6.92, 2.9),\n",
       " (6.92, 2.9),\n",
       " (6.92, 2.9),\n",
       " (6.92, 2.9),\n",
       " (6.92, 2.9),\n",
       " (6.92, 2.9),\n",
       " (6.92, 2.9),\n",
       " (6.92, 2.9),\n",
       " (6.92, 2.9),\n",
       " (6.92, 2.9),\n",
       " (6.92, 2.9),\n",
       " (6.88, 2.31),\n",
       " (6.88, 2.31),\n",
       " (6.88, 2.31),\n",
       " (6.88, 2.31),\n",
       " (6.88, 2.31),\n",
       " (6.88, 2.31),\n",
       " (6.88, 2.31),\n",
       " (6.88, 2.31),\n",
       " (6.88, 2.31),\n",
       " (6.88, 2.31),\n",
       " (6.88, 2.31),\n",
       " (6.88, 2.31),\n",
       " (7.18, 3.33),\n",
       " (7.18, 3.33),\n",
       " (7.18, 3.33),\n",
       " (7.18, 3.33),\n",
       " (7.18, 3.33),\n",
       " (7.18, 3.33),\n",
       " (7.18, 3.33),\n",
       " (7.18, 3.33),\n",
       " (7.18, 3.33),\n",
       " (7.18, 3.33),\n",
       " (7.18, 3.33),\n",
       " (6.18, 3.24),\n",
       " (6.18, 3.24),\n",
       " (6.18, 3.24),\n",
       " (6.18, 3.24),\n",
       " (6.18, 3.24),\n",
       " (6.18, 3.24),\n",
       " (6.18, 3.24),\n",
       " (6.18, 3.24),\n",
       " (6.18, 3.24),\n",
       " (6.18, 3.24),\n",
       " (6.18, 3.24),\n",
       " (6.18, 3.24),\n",
       " (7.12, 5.1),\n",
       " (7.12, 5.1),\n",
       " (7.12, 5.1),\n",
       " (7.12, 5.1),\n",
       " (7.12, 5.1),\n",
       " (7.12, 5.1),\n",
       " (7.12, 5.1),\n",
       " (7.12, 5.1),\n",
       " (7.12, 5.1),\n",
       " (7.12, 5.1),\n",
       " (7.12, 5.1),\n",
       " (5.03, 9.0),\n",
       " (5.03, 9.0),\n",
       " (5.03, 9.0),\n",
       " (5.03, 9.0),\n",
       " (5.03, 9.0),\n",
       " (5.03, 9.0),\n",
       " (5.03, 9.0),\n",
       " (5.03, 9.0),\n",
       " (5.03, 9.0),\n",
       " (5.03, 9.0),\n",
       " (5.03, 9.0),\n",
       " (7.1, 8.01),\n",
       " (7.1, 8.01),\n",
       " (7.1, 8.01),\n",
       " (7.1, 8.01),\n",
       " (7.1, 8.01),\n",
       " (7.1, 8.01),\n",
       " (7.1, 8.01),\n",
       " (7.1, 8.01),\n",
       " (7.1, 8.01),\n",
       " (7.1, 8.01),\n",
       " (7.1, 8.01),\n",
       " (7.1, 8.01),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (1.0, 6.05),\n",
       " (1.0, 6.05),\n",
       " (1.0, 6.05),\n",
       " (1.0, 6.05),\n",
       " (1.0, 6.05),\n",
       " (1.0, 6.05),\n",
       " (1.0, 6.05),\n",
       " (1.0, 6.05),\n",
       " (1.0, 6.05),\n",
       " (1.0, 6.05),\n",
       " (1.0, 6.05),\n",
       " (1.0, 6.05),\n",
       " (3.0, 5.04),\n",
       " (3.0, 5.04),\n",
       " (3.0, 5.04),\n",
       " (3.0, 5.04),\n",
       " (3.0, 5.04),\n",
       " (3.0, 5.04),\n",
       " (3.0, 5.04),\n",
       " (3.0, 5.04),\n",
       " (3.0, 5.04),\n",
       " (3.0, 5.04),\n",
       " (3.0, 5.04),\n",
       " (4.94, 5.0),\n",
       " (4.94, 5.0),\n",
       " (4.94, 5.0),\n",
       " (4.94, 5.0),\n",
       " (4.94, 5.0),\n",
       " (4.94, 5.0),\n",
       " (4.94, 5.0),\n",
       " (4.94, 5.0),\n",
       " (4.94, 5.0),\n",
       " (4.94, 5.0),\n",
       " (4.94, 5.0),\n",
       " (4.94, 5.0),\n",
       " (1.99, 4.96),\n",
       " (1.99, 4.96),\n",
       " (1.99, 4.96),\n",
       " (1.99, 4.96),\n",
       " (1.99, 4.96),\n",
       " (1.99, 4.96),\n",
       " (1.99, 4.96),\n",
       " (1.99, 4.96),\n",
       " (1.99, 4.96),\n",
       " (1.99, 4.96),\n",
       " (1.99, 4.96),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (7.0, 9.0),\n",
       " (7.0, 9.0),\n",
       " (7.0, 9.0),\n",
       " (7.0, 9.0),\n",
       " (7.0, 9.0),\n",
       " (7.0, 9.0),\n",
       " (7.0, 9.0),\n",
       " (7.0, 9.0),\n",
       " (7.0, 9.0),\n",
       " (7.0, 9.0),\n",
       " (7.0, 9.0),\n",
       " (7.0, 9.0),\n",
       " (1.0, 4.99),\n",
       " (1.0, 4.99),\n",
       " (1.0, 4.99),\n",
       " (1.0, 4.99),\n",
       " (1.0, 4.99),\n",
       " (1.0, 4.99),\n",
       " (1.0, 4.99),\n",
       " (1.0, 4.99),\n",
       " (1.0, 4.99),\n",
       " (1.0, 4.99),\n",
       " (1.0, 4.99),\n",
       " (1.0, 7.08),\n",
       " (1.0, 7.08),\n",
       " (1.0, 7.08),\n",
       " (1.0, 7.08),\n",
       " (1.0, 7.08),\n",
       " (1.0, 7.08),\n",
       " (1.0, 7.08),\n",
       " (1.0, 7.08),\n",
       " (1.0, 7.08),\n",
       " (1.0, 7.08),\n",
       " (1.0, 7.08),\n",
       " (1.0, 7.08),\n",
       " (6.06, 8.01),\n",
       " (6.06, 8.01),\n",
       " (6.06, 8.01),\n",
       " (6.06, 8.01),\n",
       " (6.06, 8.01),\n",
       " (6.06, 8.01),\n",
       " (6.06, 8.01),\n",
       " (6.06, 8.01),\n",
       " (6.06, 8.01),\n",
       " (6.06, 8.01),\n",
       " (6.06, 8.01),\n",
       " (4.99, 9.0),\n",
       " (4.99, 9.0),\n",
       " (4.99, 9.0),\n",
       " (4.99, 9.0),\n",
       " (4.99, 9.0),\n",
       " (4.99, 9.0),\n",
       " (4.99, 9.0),\n",
       " (4.99, 9.0),\n",
       " (4.99, 9.0),\n",
       " (4.99, 9.0),\n",
       " (4.99, 9.0),\n",
       " (4.99, 9.0),\n",
       " (9.0, 8.94),\n",
       " (9.0, 8.94),\n",
       " (9.0, 8.94),\n",
       " (9.0, 8.94),\n",
       " (9.0, 8.94),\n",
       " (9.0, 8.94),\n",
       " (9.0, 8.94),\n",
       " (9.0, 8.94),\n",
       " (9.0, 8.94),\n",
       " (9.0, 8.94),\n",
       " (9.0, 8.94),\n",
       " (8.01, 9.0),\n",
       " (8.01, 9.0),\n",
       " (8.01, 9.0),\n",
       " (8.01, 9.0),\n",
       " (8.01, 9.0),\n",
       " (8.01, 9.0),\n",
       " (8.01, 9.0),\n",
       " (8.01, 9.0),\n",
       " (8.01, 9.0),\n",
       " (8.01, 9.0),\n",
       " (8.01, 9.0),\n",
       " (8.01, 9.0),\n",
       " (5.05, 6.0),\n",
       " (5.05, 6.0),\n",
       " (5.05, 6.0),\n",
       " (5.05, 6.0),\n",
       " (5.05, 6.0),\n",
       " (5.05, 6.0),\n",
       " (5.05, 6.0),\n",
       " (5.05, 6.0),\n",
       " (5.05, 6.0),\n",
       " (5.05, 6.0),\n",
       " (5.05, 6.0),\n",
       " (3.0, 9.0),\n",
       " (3.0, 9.0),\n",
       " (3.0, 9.0),\n",
       " (3.0, 9.0),\n",
       " (3.0, 9.0),\n",
       " (3.0, 9.0),\n",
       " (3.0, 9.0),\n",
       " (3.0, 9.0),\n",
       " (3.0, 9.0),\n",
       " (3.0, 9.0),\n",
       " (3.0, 9.0),\n",
       " (6.15, 9.0),\n",
       " (6.15, 9.0),\n",
       " (6.15, 9.0),\n",
       " (6.15, 9.0),\n",
       " (6.15, 9.0),\n",
       " (6.15, 9.0),\n",
       " (6.15, 9.0),\n",
       " (6.15, 9.0),\n",
       " (6.15, 9.0),\n",
       " (6.15, 9.0),\n",
       " (6.15, 9.0),\n",
       " (6.15, 9.0),\n",
       " (8.32, 9.0),\n",
       " (8.32, 9.0),\n",
       " (8.32, 9.0),\n",
       " (8.32, 9.0),\n",
       " (8.32, 9.0),\n",
       " (8.32, 9.0),\n",
       " (8.32, 9.0),\n",
       " (8.32, 9.0),\n",
       " (8.32, 9.0),\n",
       " (8.32, 9.0),\n",
       " (8.32, 9.0),\n",
       " (9.0, 8.97),\n",
       " (9.0, 8.97),\n",
       " (9.0, 8.97),\n",
       " (9.0, 8.97),\n",
       " (9.0, 8.97),\n",
       " (9.0, 8.97),\n",
       " (9.0, 8.97),\n",
       " (9.0, 8.97),\n",
       " (9.0, 8.97),\n",
       " (9.0, 8.97),\n",
       " (9.0, 8.97),\n",
       " (9.0, 8.97),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 5.0),\n",
       " (1.0, 5.0),\n",
       " (1.0, 5.0),\n",
       " (1.0, 5.0),\n",
       " (1.0, 5.0),\n",
       " (1.0, 5.0),\n",
       " (1.0, 5.0),\n",
       " (1.0, 5.0),\n",
       " (1.0, 5.0),\n",
       " (1.0, 5.0),\n",
       " (1.0, 5.0),\n",
       " (1.0, 5.0),\n",
       " (2.97, 1.0),\n",
       " (2.97, 1.0),\n",
       " (2.97, 1.0),\n",
       " (2.97, 1.0),\n",
       " (2.97, 1.0),\n",
       " (2.97, 1.0),\n",
       " (2.97, 1.0),\n",
       " (2.97, 1.0),\n",
       " (2.97, 1.0),\n",
       " (2.97, 1.0),\n",
       " (2.97, 1.0),\n",
       " (9.0, 7.08),\n",
       " (9.0, 7.08),\n",
       " (9.0, 7.08),\n",
       " (9.0, 7.08),\n",
       " (9.0, 7.08),\n",
       " (9.0, 7.08),\n",
       " (9.0, 7.08),\n",
       " (9.0, 7.08),\n",
       " (9.0, 7.08),\n",
       " (9.0, 7.08),\n",
       " (9.0, 7.08),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (9.0, 9.0),\n",
       " (8.04, 9.0),\n",
       " (8.04, 9.0),\n",
       " (8.04, 9.0),\n",
       " (8.04, 9.0),\n",
       " (8.04, 9.0),\n",
       " (8.04, 9.0),\n",
       " (8.04, 9.0),\n",
       " (8.04, 9.0),\n",
       " (8.04, 9.0),\n",
       " (8.04, 9.0),\n",
       " (8.04, 9.0),\n",
       " (5.01, 4.96),\n",
       " (5.01, 4.96),\n",
       " (5.01, 4.96),\n",
       " (5.01, 4.96),\n",
       " (5.01, 4.96),\n",
       " (5.01, 4.96),\n",
       " (5.01, 4.96),\n",
       " (5.01, 4.96),\n",
       " (5.01, 4.96),\n",
       " (5.01, 4.96),\n",
       " (5.01, 4.96),\n",
       " (5.01, 4.96),\n",
       " (4.97, 4.06),\n",
       " (4.97, 4.06),\n",
       " (4.97, 4.06),\n",
       " (4.97, 4.06),\n",
       " (4.97, 4.06),\n",
       " (4.97, 4.06),\n",
       " (4.97, 4.06),\n",
       " (4.97, 4.06),\n",
       " (4.97, 4.06),\n",
       " (4.97, 4.06),\n",
       " (4.97, 4.06),\n",
       " (7.04, 2.99),\n",
       " (7.04, 2.99),\n",
       " (7.04, 2.99),\n",
       " (7.04, 2.99),\n",
       " (7.04, 2.99),\n",
       " (7.04, 2.99),\n",
       " (7.04, 2.99),\n",
       " (7.04, 2.99),\n",
       " (7.04, 2.99),\n",
       " (7.04, 2.99),\n",
       " (7.04, 2.99),\n",
       " (7.04, 2.99),\n",
       " (7.05, 2.01),\n",
       " (7.05, 2.01),\n",
       " (7.05, 2.01),\n",
       " (7.05, 2.01),\n",
       " (7.05, 2.01),\n",
       " (7.05, 2.01),\n",
       " (7.05, 2.01),\n",
       " (7.05, 2.01),\n",
       " (7.05, 2.01),\n",
       " (7.05, 2.01),\n",
       " (7.05, 2.01),\n",
       " (7.05, 4.06),\n",
       " (7.05, 4.06),\n",
       " (7.05, 4.06),\n",
       " (7.05, 4.06),\n",
       " (7.05, 4.06),\n",
       " (7.05, 4.06),\n",
       " (7.05, 4.06),\n",
       " (7.05, 4.06),\n",
       " (7.05, 4.06),\n",
       " (7.05, 4.06),\n",
       " (7.05, 4.06),\n",
       " (7.05, 4.06),\n",
       " (9.0, 1.0),\n",
       " (9.0, 1.0),\n",
       " (9.0, 1.0),\n",
       " (9.0, 1.0),\n",
       " (9.0, 1.0),\n",
       " (9.0, 1.0),\n",
       " (9.0, 1.0),\n",
       " (9.0, 1.0),\n",
       " (9.0, 1.0),\n",
       " (9.0, 1.0),\n",
       " (9.0, 1.0),\n",
       " (8.03, 5.0),\n",
       " (8.03, 5.0),\n",
       " (8.03, 5.0),\n",
       " (8.03, 5.0),\n",
       " (8.03, 5.0),\n",
       " (8.03, 5.0),\n",
       " (8.03, 5.0),\n",
       " (8.03, 5.0),\n",
       " (8.03, 5.0),\n",
       " (8.03, 5.0),\n",
       " (8.03, 5.0),\n",
       " (1.0, 4.87),\n",
       " (1.0, 4.87),\n",
       " (1.0, 4.87),\n",
       " (1.0, 4.87),\n",
       " (1.0, 4.87),\n",
       " (1.0, 4.87),\n",
       " (1.0, 4.87),\n",
       " (1.0, 4.87),\n",
       " (1.0, 4.87),\n",
       " (1.0, 4.87),\n",
       " (1.0, 4.87),\n",
       " (1.0, 4.87),\n",
       " (2.0, 1.99),\n",
       " (2.0, 1.99),\n",
       " (2.0, 1.99),\n",
       " (2.0, 1.99),\n",
       " (2.0, 1.99),\n",
       " (2.0, 1.99),\n",
       " (2.0, 1.99),\n",
       " (2.0, 1.99),\n",
       " (2.0, 1.99),\n",
       " (2.0, 1.99),\n",
       " (2.0, 1.99),\n",
       " (1.0, 5.04),\n",
       " (1.0, 5.04),\n",
       " (1.0, 5.04),\n",
       " (1.0, 5.04),\n",
       " (1.0, 5.04),\n",
       " (1.0, 5.04),\n",
       " (1.0, 5.04),\n",
       " (1.0, 5.04),\n",
       " (1.0, 5.04),\n",
       " (1.0, 5.04),\n",
       " (1.0, 5.04),\n",
       " (1.0, 5.04),\n",
       " (8.06, 1.0),\n",
       " (8.06, 1.0),\n",
       " (8.06, 1.0),\n",
       " (8.06, 1.0),\n",
       " (8.06, 1.0),\n",
       " (8.06, 1.0),\n",
       " (8.06, 1.0),\n",
       " (8.06, 1.0),\n",
       " (8.06, 1.0),\n",
       " (8.06, 1.0),\n",
       " (8.06, 1.0),\n",
       " (9.0, 1.0),\n",
       " (9.0, 1.0),\n",
       " (9.0, 1.0),\n",
       " (9.0, 1.0),\n",
       " (9.0, 1.0),\n",
       " (9.0, 1.0),\n",
       " (9.0, 1.0),\n",
       " (9.0, 1.0),\n",
       " (9.0, 1.0),\n",
       " (9.0, 1.0),\n",
       " (9.0, 1.0),\n",
       " (9.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (6.67, 6.05),\n",
       " (6.67, 6.05),\n",
       " (6.67, 6.05),\n",
       " (6.67, 6.05),\n",
       " (6.67, 6.05),\n",
       " (6.67, 6.05),\n",
       " (6.67, 6.05),\n",
       " (6.67, 6.05),\n",
       " (6.67, 6.05),\n",
       " (6.67, 6.05),\n",
       " (6.67, 6.05),\n",
       " (4.67, 5.33),\n",
       " (4.67, 5.33),\n",
       " (4.67, 5.33),\n",
       " (4.67, 5.33),\n",
       " (4.67, 5.33),\n",
       " (4.67, 5.33),\n",
       " (4.67, 5.33),\n",
       " (4.67, 5.33),\n",
       " (4.67, 5.33),\n",
       " (4.67, 5.33),\n",
       " (4.67, 5.33),\n",
       " (4.67, 5.33),\n",
       " (4.44, 7.21),\n",
       " (4.44, 7.21),\n",
       " (4.44, 7.21),\n",
       " (4.44, 7.21),\n",
       " (4.44, 7.21),\n",
       " (4.44, 7.21),\n",
       " (4.44, 7.21),\n",
       " (4.44, 7.21),\n",
       " (4.44, 7.21),\n",
       " (4.44, 7.21),\n",
       " (4.44, 7.21),\n",
       " (2.81, 7.55),\n",
       " (2.81, 7.55),\n",
       " (2.81, 7.55),\n",
       " (2.81, 7.55),\n",
       " (2.81, 7.55),\n",
       " (2.81, 7.55),\n",
       " (2.81, 7.55),\n",
       " (2.81, 7.55),\n",
       " (2.81, 7.55),\n",
       " (2.81, 7.55),\n",
       " (2.81, 7.55),\n",
       " (2.81, 7.55),\n",
       " (3.69, 4.69),\n",
       " (3.69, 4.69),\n",
       " (3.69, 4.69),\n",
       " (3.69, 4.69),\n",
       " (3.69, 4.69),\n",
       " (3.69, 4.69),\n",
       " (3.69, 4.69),\n",
       " (3.69, 4.69),\n",
       " (3.69, 4.69),\n",
       " (3.69, 4.69),\n",
       " (3.69, 4.69),\n",
       " (4.44, 6.92),\n",
       " (4.44, 6.92),\n",
       " (4.44, 6.92),\n",
       " (4.44, 6.92),\n",
       " (4.44, 6.92),\n",
       " (4.44, 6.92),\n",
       " (4.44, 6.92),\n",
       " (4.44, 6.92),\n",
       " (4.44, 6.92),\n",
       " (4.44, 6.92),\n",
       " (4.44, 6.92),\n",
       " (4.44, 6.92),\n",
       " (1.97, 6.79),\n",
       " (1.97, 6.79),\n",
       " (1.97, 6.79),\n",
       " (1.97, 6.79),\n",
       " (1.97, 6.79),\n",
       " (1.97, 6.79),\n",
       " (1.97, 6.79),\n",
       " (1.97, 6.79),\n",
       " (1.97, 6.79),\n",
       " (1.97, 6.79),\n",
       " (1.97, 6.79),\n",
       " (3.01, 5.45),\n",
       " (3.01, 5.45),\n",
       " (3.01, 5.45),\n",
       " (3.01, 5.45),\n",
       " (3.01, 5.45),\n",
       " (3.01, 5.45),\n",
       " (3.01, 5.45),\n",
       " (3.01, 5.45),\n",
       " (3.01, 5.45),\n",
       " (3.01, 5.45),\n",
       " (3.01, 5.45),\n",
       " (3.01, 5.45),\n",
       " (4.33, 7.14),\n",
       " (4.33, 7.14),\n",
       " (4.33, 7.14),\n",
       " (4.33, 7.14),\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mx = MinMaxScaler(feature_range=(0, 1))\n",
    "Y_scaled_training=mx.fit_transform(Y_scaled_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train\n",
    "X_training = X[0:468480:32]\n",
    "Y_training = ZZ[0:468480:32]\n",
    "\n",
    "# test\n",
    "X_testing = M[0:78080:32]\n",
    "Y_testing = LL[0:78080:32]\n",
    "\n",
    "X_valid=A[0:78080:32]\n",
    "Y_valid=BB[0:78080:32]\n",
    "\n",
    "# convert to dataframe\n",
    "X_scaled_training = pd.DataFrame (data = X_training).values\n",
    "Y_scaled_training = pd.DataFrame (data = Y_training).values\n",
    "\n",
    "X_scaled_testing = pd.DataFrame (data = X_testing).values\n",
    "Y_scaled_testing = pd.DataFrame (data = Y_testing).values\n",
    "\n",
    "X_scaled_valid = pd.DataFrame (data = X_valid).values\n",
    "Y_scaled_valid = pd.DataFrame (data = Y_valid).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.6 , 7.71],\n",
       "       [7.6 , 7.71],\n",
       "       [7.6 , 7.71],\n",
       "       ...,\n",
       "       [4.03, 7.15],\n",
       "       [4.03, 7.15],\n",
       "       [4.03, 7.15]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_scaled_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=70,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=4, min_samples_split=10,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regressor = RandomForestRegressor(bootstrap=True,\n",
    " max_depth=70,\n",
    " max_features='auto',\n",
    " min_samples_leaf=4,\n",
    " min_samples_split=10,\n",
    " n_estimators= 400)\n",
    "regressor.fit(X_scaled_training, Y_scaled_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_scaled_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.42382333, 4.81850639],\n",
       "       [5.55233602, 5.50316974],\n",
       "       [5.73548833, 5.73216643],\n",
       "       ...,\n",
       "       [5.62983762, 4.73525782],\n",
       "       [5.68514617, 4.71034844],\n",
       "       [5.23546973, 5.37064666]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Random Forest Trained on Original Data\n",
      "Average absolute error: 1.25 degrees.\n",
      "Accuracy: 60.42 %.\n"
     ]
    }
   ],
   "source": [
    "errors = abs(y_pred - Y_scaled_testing)\n",
    "\n",
    "print('Metrics for Random Forest Trained on Original Data')\n",
    "print('Average absolute error:', round(np.mean(errors), 2), 'degrees.')\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / Y_scaled_testing)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def soft_acc(y_true, y_pred):\n",
    "    return K.mean(K.equal(K.round(y_true), K.round(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list=[]\n",
    "for i in range(1,71):\n",
    "    feature_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable:                   25 Importance: 0.03\n",
      "Variable:                   40 Importance: 0.03\n",
      "Variable:                    5 Importance: 0.02\n",
      "Variable:                   15 Importance: 0.02\n",
      "Variable:                   20 Importance: 0.02\n",
      "Variable:                   27 Importance: 0.02\n",
      "Variable:                   28 Importance: 0.02\n",
      "Variable:                   30 Importance: 0.02\n",
      "Variable:                   32 Importance: 0.02\n",
      "Variable:                   35 Importance: 0.02\n",
      "Variable:                   45 Importance: 0.02\n",
      "Variable:                   50 Importance: 0.02\n",
      "Variable:                   55 Importance: 0.02\n",
      "Variable:                   60 Importance: 0.02\n",
      "Variable:                   65 Importance: 0.02\n",
      "Variable:                   70 Importance: 0.02\n",
      "Variable:                    1 Importance: 0.01\n",
      "Variable:                    2 Importance: 0.01\n",
      "Variable:                    3 Importance: 0.01\n",
      "Variable:                    4 Importance: 0.01\n",
      "Variable:                    6 Importance: 0.01\n",
      "Variable:                    7 Importance: 0.01\n",
      "Variable:                    8 Importance: 0.01\n",
      "Variable:                    9 Importance: 0.01\n",
      "Variable:                   10 Importance: 0.01\n",
      "Variable:                   11 Importance: 0.01\n",
      "Variable:                   12 Importance: 0.01\n",
      "Variable:                   13 Importance: 0.01\n",
      "Variable:                   14 Importance: 0.01\n",
      "Variable:                   16 Importance: 0.01\n",
      "Variable:                   17 Importance: 0.01\n",
      "Variable:                   18 Importance: 0.01\n",
      "Variable:                   19 Importance: 0.01\n",
      "Variable:                   21 Importance: 0.01\n",
      "Variable:                   22 Importance: 0.01\n",
      "Variable:                   23 Importance: 0.01\n",
      "Variable:                   24 Importance: 0.01\n",
      "Variable:                   26 Importance: 0.01\n",
      "Variable:                   29 Importance: 0.01\n",
      "Variable:                   31 Importance: 0.01\n",
      "Variable:                   33 Importance: 0.01\n",
      "Variable:                   34 Importance: 0.01\n",
      "Variable:                   36 Importance: 0.01\n",
      "Variable:                   37 Importance: 0.01\n",
      "Variable:                   38 Importance: 0.01\n",
      "Variable:                   39 Importance: 0.01\n",
      "Variable:                   41 Importance: 0.01\n",
      "Variable:                   42 Importance: 0.01\n",
      "Variable:                   43 Importance: 0.01\n",
      "Variable:                   44 Importance: 0.01\n",
      "Variable:                   46 Importance: 0.01\n",
      "Variable:                   47 Importance: 0.01\n",
      "Variable:                   48 Importance: 0.01\n",
      "Variable:                   49 Importance: 0.01\n",
      "Variable:                   51 Importance: 0.01\n",
      "Variable:                   52 Importance: 0.01\n",
      "Variable:                   53 Importance: 0.01\n",
      "Variable:                   54 Importance: 0.01\n",
      "Variable:                   56 Importance: 0.01\n",
      "Variable:                   57 Importance: 0.01\n",
      "Variable:                   58 Importance: 0.01\n",
      "Variable:                   59 Importance: 0.01\n",
      "Variable:                   61 Importance: 0.01\n",
      "Variable:                   62 Importance: 0.01\n",
      "Variable:                   63 Importance: 0.01\n",
      "Variable:                   64 Importance: 0.01\n",
      "Variable:                   66 Importance: 0.01\n",
      "Variable:                   67 Importance: 0.01\n",
      "Variable:                   68 Importance: 0.01\n",
      "Variable:                   69 Importance: 0.01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(regressor.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"glorot_uniform\", bias_initializer=\"Zeros\", activation=\"relu\", input_dim=70, units=512)`\n",
      "  \n",
      "D:\\Users\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"glorot_uniform\", bias_initializer=\"Zeros\", activation=\"relu\", units=1024)`\n",
      "  app.launch_new_instance()\n",
      "D:\\Users\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"glorot_uniform\", bias_initializer=\"Zeros\", activation=\"relu\", units=2048)`\n",
      "D:\\Users\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"glorot_uniform\", bias_initializer=\"Zeros\", activation=\"relu\", units=1024)`\n",
      "D:\\Users\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"glorot_uniform\", bias_initializer=\"Zeros\", activation=\"relu\", units=512)`\n",
      "D:\\Users\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"glorot_uniform\", bias_initializer=\"Zeros\", activation=\"linear\", units=2)`\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import adam\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "ann= Sequential()\n",
    "\n",
    "\n",
    "\n",
    "ann.add(Dense(output_dim=512,kernel_initializer='glorot_uniform',bias_initializer=\"Zeros\",activation='relu',input_dim=70))\n",
    "\n",
    "ann.add(Dense(output_dim=1024,kernel_initializer='glorot_uniform',bias_initializer=\"Zeros\",activation='relu'))\n",
    "\n",
    "ann.add(Dense(output_dim=2048,kernel_initializer='glorot_uniform',bias_initializer=\"Zeros\",activation='relu'))\n",
    "\n",
    "ann.add(Dense(output_dim=1024,kernel_initializer='glorot_uniform',bias_initializer=\"Zeros\",activation='relu'))\n",
    "\n",
    "ann.add(Dense(output_dim=512,kernel_initializer='glorot_uniform',bias_initializer=\"Zeros\",activation='relu'))\n",
    "\n",
    "ann.add(Dense(output_dim=2,kernel_initializer='glorot_uniform' ,bias_initializer=\"Zeros\",activation='linear'))\n",
    "\n",
    "opt = adam(lr=0.001)\n",
    "\n",
    "ann.compile(optimizer=opt,loss='mean_squared_error',metrics=[soft_acc])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.6 , 7.71],\n",
       "       [7.6 , 7.71],\n",
       "       [7.6 , 7.71],\n",
       "       ...,\n",
       "       [4.03, 7.15],\n",
       "       [4.03, 7.15],\n",
       "       [4.03, 7.15]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_scaled_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14640 samples, validate on 2440 samples\n",
      "Epoch 1/5000\n",
      "14640/14640 [==============================] - 7s 464us/step - loss: 4.9311 - soft_acc: 0.1563 - val_loss: 4.2361 - val_soft_acc: 0.1648\n",
      "Epoch 2/5000\n",
      " 2310/14640 [===>..........................] - ETA: 0s - loss: 4.3573 - soft_acc: 0.1545"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\anaconda\\lib\\site-packages\\keras\\callbacks.py:434: RuntimeWarning: Can save best model only with val_accuracy available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 77us/step - loss: 4.2712 - soft_acc: 0.1620 - val_loss: 4.2538 - val_soft_acc: 0.1660\n",
      "Epoch 3/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 4.0978 - soft_acc: 0.1649 - val_loss: 4.0557 - val_soft_acc: 0.1678\n",
      "Epoch 4/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.9950 - soft_acc: 0.1700 - val_loss: 3.8680 - val_soft_acc: 0.1750\n",
      "Epoch 5/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.9263 - soft_acc: 0.1667 - val_loss: 3.8090 - val_soft_acc: 0.1725\n",
      "Epoch 6/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.9352 - soft_acc: 0.1724 - val_loss: 3.7878 - val_soft_acc: 0.1801\n",
      "Epoch 7/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 3.8785 - soft_acc: 0.1748 - val_loss: 3.7909 - val_soft_acc: 0.1594\n",
      "Epoch 8/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.8365 - soft_acc: 0.1758 - val_loss: 3.7702 - val_soft_acc: 0.1885\n",
      "Epoch 9/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.8032 - soft_acc: 0.1789 - val_loss: 3.6669 - val_soft_acc: 0.1760\n",
      "Epoch 10/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 3.8290 - soft_acc: 0.1767 - val_loss: 3.7989 - val_soft_acc: 0.1684\n",
      "Epoch 11/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.8031 - soft_acc: 0.1766 - val_loss: 3.8528 - val_soft_acc: 0.1803\n",
      "Epoch 12/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.7374 - soft_acc: 0.1794 - val_loss: 3.7416 - val_soft_acc: 0.1682\n",
      "Epoch 13/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.7180 - soft_acc: 0.1827 - val_loss: 3.6963 - val_soft_acc: 0.1947\n",
      "Epoch 14/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.7200 - soft_acc: 0.1828 - val_loss: 3.6435 - val_soft_acc: 0.2004\n",
      "Epoch 15/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 3.7007 - soft_acc: 0.1835 - val_loss: 3.6646 - val_soft_acc: 0.1791\n",
      "Epoch 16/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.6705 - soft_acc: 0.1836 - val_loss: 3.6551 - val_soft_acc: 0.1982\n",
      "Epoch 17/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.6652 - soft_acc: 0.1848 - val_loss: 3.5314 - val_soft_acc: 0.2012\n",
      "Epoch 18/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.6393 - soft_acc: 0.1858 - val_loss: 3.5957 - val_soft_acc: 0.1828\n",
      "Epoch 19/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.5886 - soft_acc: 0.1902 - val_loss: 3.4667 - val_soft_acc: 0.1861\n",
      "Epoch 20/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.5986 - soft_acc: 0.1867 - val_loss: 3.4958 - val_soft_acc: 0.1895\n",
      "Epoch 21/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.5439 - soft_acc: 0.1912 - val_loss: 3.4778 - val_soft_acc: 0.1955\n",
      "Epoch 22/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.5414 - soft_acc: 0.1922 - val_loss: 3.5839 - val_soft_acc: 0.1781\n",
      "Epoch 23/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.5237 - soft_acc: 0.1900 - val_loss: 3.4469 - val_soft_acc: 0.1914\n",
      "Epoch 24/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.5146 - soft_acc: 0.1932 - val_loss: 3.4277 - val_soft_acc: 0.1969\n",
      "Epoch 25/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.4945 - soft_acc: 0.1941 - val_loss: 3.5907 - val_soft_acc: 0.1889\n",
      "Epoch 26/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.4515 - soft_acc: 0.1954 - val_loss: 3.4265 - val_soft_acc: 0.1934\n",
      "Epoch 27/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.4429 - soft_acc: 0.1966 - val_loss: 3.3667 - val_soft_acc: 0.2025\n",
      "Epoch 28/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.4209 - soft_acc: 0.1975 - val_loss: 3.5200 - val_soft_acc: 0.1898\n",
      "Epoch 29/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.4148 - soft_acc: 0.1974 - val_loss: 3.3709 - val_soft_acc: 0.2020\n",
      "Epoch 30/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.4007 - soft_acc: 0.1993 - val_loss: 3.3489 - val_soft_acc: 0.1980\n",
      "Epoch 31/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.3883 - soft_acc: 0.1982 - val_loss: 3.4429 - val_soft_acc: 0.2127\n",
      "Epoch 32/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.3666 - soft_acc: 0.2031 - val_loss: 3.4343 - val_soft_acc: 0.1939\n",
      "Epoch 33/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.3126 - soft_acc: 0.2095 - val_loss: 3.3227 - val_soft_acc: 0.2037\n",
      "Epoch 34/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.3331 - soft_acc: 0.2036 - val_loss: 3.3164 - val_soft_acc: 0.2084\n",
      "Epoch 35/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.3019 - soft_acc: 0.2092 - val_loss: 3.2866 - val_soft_acc: 0.2137\n",
      "Epoch 36/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.2701 - soft_acc: 0.2130 - val_loss: 3.2343 - val_soft_acc: 0.2102\n",
      "Epoch 37/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.2921 - soft_acc: 0.2084 - val_loss: 3.2042 - val_soft_acc: 0.2199\n",
      "Epoch 38/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.2314 - soft_acc: 0.2128 - val_loss: 3.2960 - val_soft_acc: 0.2043\n",
      "Epoch 39/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.2324 - soft_acc: 0.2114 - val_loss: 3.2847 - val_soft_acc: 0.2109\n",
      "Epoch 40/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.1947 - soft_acc: 0.2153 - val_loss: 3.3670 - val_soft_acc: 0.2020\n",
      "Epoch 41/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.1879 - soft_acc: 0.2158 - val_loss: 3.2823 - val_soft_acc: 0.2162\n",
      "Epoch 42/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.1574 - soft_acc: 0.2164 - val_loss: 3.0948 - val_soft_acc: 0.2111\n",
      "Epoch 43/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.1675 - soft_acc: 0.2198 - val_loss: 3.1901 - val_soft_acc: 0.2275\n",
      "Epoch 44/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.1637 - soft_acc: 0.2198 - val_loss: 3.1131 - val_soft_acc: 0.2234\n",
      "Epoch 45/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.1244 - soft_acc: 0.2234 - val_loss: 3.1608 - val_soft_acc: 0.2250\n",
      "Epoch 46/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.1082 - soft_acc: 0.2243 - val_loss: 3.2049 - val_soft_acc: 0.2141\n",
      "Epoch 47/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.0857 - soft_acc: 0.2245 - val_loss: 3.0458 - val_soft_acc: 0.2227\n",
      "Epoch 48/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.0949 - soft_acc: 0.2205 - val_loss: 3.1580 - val_soft_acc: 0.2170\n",
      "Epoch 49/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.0676 - soft_acc: 0.2297 - val_loss: 3.0238 - val_soft_acc: 0.2283\n",
      "Epoch 50/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.0114 - soft_acc: 0.2268 - val_loss: 3.0638 - val_soft_acc: 0.2432\n",
      "Epoch 51/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.0160 - soft_acc: 0.2324 - val_loss: 3.1387 - val_soft_acc: 0.2371\n",
      "Epoch 52/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.9779 - soft_acc: 0.2370 - val_loss: 3.0295 - val_soft_acc: 0.2322\n",
      "Epoch 53/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.9733 - soft_acc: 0.2364 - val_loss: 2.9544 - val_soft_acc: 0.2365\n",
      "Epoch 54/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.9271 - soft_acc: 0.2375 - val_loss: 2.9672 - val_soft_acc: 0.2414\n",
      "Epoch 55/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.9225 - soft_acc: 0.2404 - val_loss: 3.0217 - val_soft_acc: 0.2398\n",
      "Epoch 56/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.8784 - soft_acc: 0.2416 - val_loss: 3.0097 - val_soft_acc: 0.2449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.8637 - soft_acc: 0.2435 - val_loss: 2.8838 - val_soft_acc: 0.2506\n",
      "Epoch 58/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.8799 - soft_acc: 0.2444 - val_loss: 3.0175 - val_soft_acc: 0.2289\n",
      "Epoch 59/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.8506 - soft_acc: 0.2480 - val_loss: 2.8558 - val_soft_acc: 0.2568\n",
      "Epoch 60/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.8118 - soft_acc: 0.2501 - val_loss: 2.8784 - val_soft_acc: 0.2492\n",
      "Epoch 61/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.7769 - soft_acc: 0.2536 - val_loss: 2.8132 - val_soft_acc: 0.2549\n",
      "Epoch 62/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.7830 - soft_acc: 0.2523 - val_loss: 2.8118 - val_soft_acc: 0.2432\n",
      "Epoch 63/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.7768 - soft_acc: 0.2524 - val_loss: 2.7842 - val_soft_acc: 0.2557\n",
      "Epoch 64/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.7269 - soft_acc: 0.2567 - val_loss: 2.8709 - val_soft_acc: 0.2662\n",
      "Epoch 65/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.6892 - soft_acc: 0.2530 - val_loss: 2.8562 - val_soft_acc: 0.2635\n",
      "Epoch 66/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.6982 - soft_acc: 0.2601 - val_loss: 2.7715 - val_soft_acc: 0.2525\n",
      "Epoch 67/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.6328 - soft_acc: 0.2690 - val_loss: 2.6953 - val_soft_acc: 0.2635\n",
      "Epoch 68/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.6107 - soft_acc: 0.2726 - val_loss: 2.6050 - val_soft_acc: 0.2717\n",
      "Epoch 69/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.5955 - soft_acc: 0.2710 - val_loss: 2.9678 - val_soft_acc: 0.2686\n",
      "Epoch 70/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.5784 - soft_acc: 0.2721 - val_loss: 2.7295 - val_soft_acc: 0.2727\n",
      "Epoch 71/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.5550 - soft_acc: 0.2742 - val_loss: 2.6187 - val_soft_acc: 0.2742\n",
      "Epoch 72/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.5329 - soft_acc: 0.2775 - val_loss: 2.7111 - val_soft_acc: 0.2482\n",
      "Epoch 73/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.4931 - soft_acc: 0.2803 - val_loss: 2.6097 - val_soft_acc: 0.2693\n",
      "Epoch 74/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.4541 - soft_acc: 0.2827 - val_loss: 2.5661 - val_soft_acc: 0.2861\n",
      "Epoch 75/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.4465 - soft_acc: 0.2861 - val_loss: 2.5838 - val_soft_acc: 0.2703\n",
      "Epoch 76/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.4167 - soft_acc: 0.2902 - val_loss: 2.6694 - val_soft_acc: 0.2789\n",
      "Epoch 77/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.4008 - soft_acc: 0.2903 - val_loss: 2.5879 - val_soft_acc: 0.2777\n",
      "Epoch 78/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.3215 - soft_acc: 0.2979 - val_loss: 2.5135 - val_soft_acc: 0.2934\n",
      "Epoch 79/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.3168 - soft_acc: 0.2945 - val_loss: 2.7193 - val_soft_acc: 0.2766\n",
      "Epoch 80/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.3385 - soft_acc: 0.2938 - val_loss: 2.5166 - val_soft_acc: 0.2920\n",
      "Epoch 81/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.2656 - soft_acc: 0.3050 - val_loss: 2.5893 - val_soft_acc: 0.2914\n",
      "Epoch 82/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.2464 - soft_acc: 0.3059 - val_loss: 2.4299 - val_soft_acc: 0.2959\n",
      "Epoch 83/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.2406 - soft_acc: 0.3028 - val_loss: 2.4497 - val_soft_acc: 0.2842\n",
      "Epoch 84/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.2069 - soft_acc: 0.3106 - val_loss: 2.5291 - val_soft_acc: 0.2924\n",
      "Epoch 85/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.1662 - soft_acc: 0.3159 - val_loss: 2.4628 - val_soft_acc: 0.3004\n",
      "Epoch 86/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.1578 - soft_acc: 0.3181 - val_loss: 2.3946 - val_soft_acc: 0.2975\n",
      "Epoch 87/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.1148 - soft_acc: 0.3169 - val_loss: 2.3180 - val_soft_acc: 0.3115\n",
      "Epoch 88/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.1165 - soft_acc: 0.3209 - val_loss: 2.3905 - val_soft_acc: 0.3074\n",
      "Epoch 89/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.0519 - soft_acc: 0.3318 - val_loss: 2.3868 - val_soft_acc: 0.3061\n",
      "Epoch 90/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.0200 - soft_acc: 0.3321 - val_loss: 2.3585 - val_soft_acc: 0.3209\n",
      "Epoch 91/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.0049 - soft_acc: 0.3381 - val_loss: 2.3351 - val_soft_acc: 0.3176\n",
      "Epoch 92/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.9936 - soft_acc: 0.3369 - val_loss: 2.4743 - val_soft_acc: 0.3119\n",
      "Epoch 93/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.9696 - soft_acc: 0.3408 - val_loss: 2.1721 - val_soft_acc: 0.3248\n",
      "Epoch 94/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.8903 - soft_acc: 0.3454 - val_loss: 2.3253 - val_soft_acc: 0.3197\n",
      "Epoch 95/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.9061 - soft_acc: 0.3473 - val_loss: 2.3170 - val_soft_acc: 0.3184\n",
      "Epoch 96/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.9239 - soft_acc: 0.3466 - val_loss: 2.1738 - val_soft_acc: 0.3291\n",
      "Epoch 97/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.8037 - soft_acc: 0.3585 - val_loss: 2.1474 - val_soft_acc: 0.3344\n",
      "Epoch 98/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.8150 - soft_acc: 0.3637 - val_loss: 2.2028 - val_soft_acc: 0.3219\n",
      "Epoch 99/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.7764 - soft_acc: 0.3686 - val_loss: 2.2958 - val_soft_acc: 0.3293\n",
      "Epoch 100/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.9736 - soft_acc: 0.3438 - val_loss: 2.3157 - val_soft_acc: 0.3260\n",
      "Epoch 101/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.8089 - soft_acc: 0.3638 - val_loss: 2.1722 - val_soft_acc: 0.3355\n",
      "Epoch 102/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.7273 - soft_acc: 0.3733 - val_loss: 2.1648 - val_soft_acc: 0.3322\n",
      "Epoch 103/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.6807 - soft_acc: 0.3801 - val_loss: 2.1555 - val_soft_acc: 0.3486\n",
      "Epoch 104/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.6573 - soft_acc: 0.3872 - val_loss: 2.2537 - val_soft_acc: 0.3318\n",
      "Epoch 105/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.6016 - soft_acc: 0.3906 - val_loss: 2.1499 - val_soft_acc: 0.3518\n",
      "Epoch 106/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.6284 - soft_acc: 0.3896 - val_loss: 2.0479 - val_soft_acc: 0.3545\n",
      "Epoch 107/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.5748 - soft_acc: 0.3947 - val_loss: 2.1155 - val_soft_acc: 0.3623\n",
      "Epoch 108/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.5049 - soft_acc: 0.4033 - val_loss: 1.9916 - val_soft_acc: 0.3658\n",
      "Epoch 109/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.4723 - soft_acc: 0.4136 - val_loss: 2.0410 - val_soft_acc: 0.3666\n",
      "Epoch 110/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 1.4696 - soft_acc: 0.4096 - val_loss: 2.0041 - val_soft_acc: 0.3705\n",
      "Epoch 111/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 78us/step - loss: 1.4798 - soft_acc: 0.4110 - val_loss: 1.9239 - val_soft_acc: 0.3834\n",
      "Epoch 112/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.4309 - soft_acc: 0.4223 - val_loss: 1.9727 - val_soft_acc: 0.3750\n",
      "Epoch 113/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.3764 - soft_acc: 0.4321 - val_loss: 2.0223 - val_soft_acc: 0.3689\n",
      "Epoch 114/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.4041 - soft_acc: 0.4261 - val_loss: 1.9610 - val_soft_acc: 0.3855\n",
      "Epoch 115/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.4077 - soft_acc: 0.4250 - val_loss: 2.0129 - val_soft_acc: 0.3787\n",
      "Epoch 116/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.3225 - soft_acc: 0.4444 - val_loss: 1.8268 - val_soft_acc: 0.4141\n",
      "Epoch 117/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.3414 - soft_acc: 0.4317 - val_loss: 1.8526 - val_soft_acc: 0.4027\n",
      "Epoch 118/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.3653 - soft_acc: 0.4334 - val_loss: 1.9005 - val_soft_acc: 0.3871\n",
      "Epoch 119/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.2618 - soft_acc: 0.4517 - val_loss: 1.7504 - val_soft_acc: 0.4168\n",
      "Epoch 120/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.2609 - soft_acc: 0.4527 - val_loss: 2.0114 - val_soft_acc: 0.3740\n",
      "Epoch 121/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.2047 - soft_acc: 0.4636 - val_loss: 1.7991 - val_soft_acc: 0.3963\n",
      "Epoch 122/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.1491 - soft_acc: 0.4722 - val_loss: 1.8005 - val_soft_acc: 0.4121\n",
      "Epoch 123/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.1687 - soft_acc: 0.4699 - val_loss: 1.8437 - val_soft_acc: 0.4156\n",
      "Epoch 124/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.1850 - soft_acc: 0.4689 - val_loss: 1.9157 - val_soft_acc: 0.4172\n",
      "Epoch 125/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.1430 - soft_acc: 0.4696 - val_loss: 1.8474 - val_soft_acc: 0.4092\n",
      "Epoch 126/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.1580 - soft_acc: 0.4686 - val_loss: 1.7562 - val_soft_acc: 0.4090\n",
      "Epoch 127/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.1256 - soft_acc: 0.4787 - val_loss: 1.8749 - val_soft_acc: 0.4088\n",
      "Epoch 128/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.1151 - soft_acc: 0.4806 - val_loss: 1.7692 - val_soft_acc: 0.4199\n",
      "Epoch 129/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 1.0975 - soft_acc: 0.49 - 1s 74us/step - loss: 1.0976 - soft_acc: 0.4910 - val_loss: 1.7102 - val_soft_acc: 0.4330\n",
      "Epoch 130/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.0123 - soft_acc: 0.5113 - val_loss: 1.8834 - val_soft_acc: 0.3930\n",
      "Epoch 131/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.0869 - soft_acc: 0.4859 - val_loss: 1.7379 - val_soft_acc: 0.4377\n",
      "Epoch 132/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.0343 - soft_acc: 0.4978 - val_loss: 1.7785 - val_soft_acc: 0.4387\n",
      "Epoch 133/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.0309 - soft_acc: 0.4963 - val_loss: 1.7576 - val_soft_acc: 0.4295\n",
      "Epoch 134/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.0328 - soft_acc: 0.5005 - val_loss: 1.8391 - val_soft_acc: 0.4314\n",
      "Epoch 135/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9656 - soft_acc: 0.5188 - val_loss: 1.6468 - val_soft_acc: 0.4525\n",
      "Epoch 136/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9312 - soft_acc: 0.5311 - val_loss: 1.6593 - val_soft_acc: 0.4547\n",
      "Epoch 137/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.9399 - soft_acc: 0.5240 - val_loss: 1.7355 - val_soft_acc: 0.4533\n",
      "Epoch 138/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.9185 - soft_acc: 0.5286 - val_loss: 1.7741 - val_soft_acc: 0.4350\n",
      "Epoch 139/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9923 - soft_acc: 0.5154 - val_loss: 1.7985 - val_soft_acc: 0.4311\n",
      "Epoch 140/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.0168 - soft_acc: 0.5077 - val_loss: 1.7369 - val_soft_acc: 0.4391\n",
      "Epoch 141/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8571 - soft_acc: 0.5359 - val_loss: 1.7382 - val_soft_acc: 0.4496\n",
      "Epoch 142/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.8569 - soft_acc: 0.5473 - val_loss: 1.7111 - val_soft_acc: 0.4488\n",
      "Epoch 143/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9499 - soft_acc: 0.5184 - val_loss: 1.6977 - val_soft_acc: 0.4471\n",
      "Epoch 144/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8388 - soft_acc: 0.5478 - val_loss: 1.6228 - val_soft_acc: 0.4662\n",
      "Epoch 145/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8440 - soft_acc: 0.5522 - val_loss: 1.7414 - val_soft_acc: 0.4391\n",
      "Epoch 146/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8881 - soft_acc: 0.5356 - val_loss: 1.7660 - val_soft_acc: 0.4508\n",
      "Epoch 147/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.8744 - soft_acc: 0.5392 - val_loss: 1.6156 - val_soft_acc: 0.4654\n",
      "Epoch 148/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.7717 - soft_acc: 0.5655 - val_loss: 1.5512 - val_soft_acc: 0.4889\n",
      "Epoch 149/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7330 - soft_acc: 0.5794 - val_loss: 1.5322 - val_soft_acc: 0.4891\n",
      "Epoch 150/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7501 - soft_acc: 0.5782 - val_loss: 1.5720 - val_soft_acc: 0.4756\n",
      "Epoch 151/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8211 - soft_acc: 0.5610 - val_loss: 1.5539 - val_soft_acc: 0.4744\n",
      "Epoch 152/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7786 - soft_acc: 0.5698 - val_loss: 1.7324 - val_soft_acc: 0.4527\n",
      "Epoch 153/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.7818 - soft_acc: 0.5671 - val_loss: 1.6446 - val_soft_acc: 0.4676\n",
      "Epoch 154/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7939 - soft_acc: 0.5593 - val_loss: 1.6381 - val_soft_acc: 0.4850\n",
      "Epoch 155/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8069 - soft_acc: 0.5597 - val_loss: 1.6005 - val_soft_acc: 0.4668\n",
      "Epoch 156/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8941 - soft_acc: 0.5370 - val_loss: 1.5678 - val_soft_acc: 0.4852\n",
      "Epoch 157/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6918 - soft_acc: 0.5864 - val_loss: 1.5075 - val_soft_acc: 0.5008\n",
      "Epoch 158/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6069 - soft_acc: 0.6215 - val_loss: 1.6236 - val_soft_acc: 0.5078\n",
      "Epoch 159/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6244 - soft_acc: 0.6239 - val_loss: 1.5222 - val_soft_acc: 0.4959\n",
      "Epoch 160/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6723 - soft_acc: 0.6054 - val_loss: 1.6447 - val_soft_acc: 0.4844\n",
      "Epoch 161/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6941 - soft_acc: 0.5939 - val_loss: 1.6516 - val_soft_acc: 0.4695\n",
      "Epoch 162/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7540 - soft_acc: 0.5757 - val_loss: 1.5474 - val_soft_acc: 0.5004\n",
      "Epoch 163/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7274 - soft_acc: 0.5853 - val_loss: 1.6627 - val_soft_acc: 0.4865\n",
      "Epoch 164/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6302 - soft_acc: 0.6168 - val_loss: 1.4453 - val_soft_acc: 0.5176\n",
      "Epoch 165/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.5899 - soft_acc: 0.6337 - val_loss: 1.4974 - val_soft_acc: 0.5039\n",
      "Epoch 166/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.6085 - soft_acc: 0.6259 - val_loss: 1.5742 - val_soft_acc: 0.4703\n",
      "Epoch 167/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.5877 - soft_acc: 0.6271 - val_loss: 1.5745 - val_soft_acc: 0.5072\n",
      "Epoch 168/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7986 - soft_acc: 0.5718 - val_loss: 1.5541 - val_soft_acc: 0.5066\n",
      "Epoch 169/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.6460 - soft_acc: 0.6122 - val_loss: 1.5912 - val_soft_acc: 0.4910\n",
      "Epoch 170/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.5895 - soft_acc: 0.6363 - val_loss: 1.5540 - val_soft_acc: 0.4924\n",
      "Epoch 171/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.5742 - soft_acc: 0.6319 - val_loss: 1.4731 - val_soft_acc: 0.5084\n",
      "Epoch 172/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.6173 - soft_acc: 0.6252 - val_loss: 1.5058 - val_soft_acc: 0.5186\n",
      "Epoch 173/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.6577 - soft_acc: 0.6153 - val_loss: 1.6359 - val_soft_acc: 0.4707\n",
      "Epoch 174/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.5725 - soft_acc: 0.6277 - val_loss: 1.4252 - val_soft_acc: 0.5186\n",
      "Epoch 175/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.5172 - soft_acc: 0.6616 - val_loss: 1.5485 - val_soft_acc: 0.5129\n",
      "Epoch 176/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.5877 - soft_acc: 0.6334 - val_loss: 1.4751 - val_soft_acc: 0.5082\n",
      "Epoch 177/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5523 - soft_acc: 0.6417 - val_loss: 1.5849 - val_soft_acc: 0.4889\n",
      "Epoch 178/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.5871 - soft_acc: 0.6324 - val_loss: 1.4888 - val_soft_acc: 0.5197\n",
      "Epoch 179/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.6137 - soft_acc: 0.6312 - val_loss: 1.5659 - val_soft_acc: 0.4963\n",
      "Epoch 180/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.5784 - soft_acc: 0.6359 - val_loss: 1.4066 - val_soft_acc: 0.5395\n",
      "Epoch 181/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.4576 - soft_acc: 0.6893 - val_loss: 1.4187 - val_soft_acc: 0.5303\n",
      "Epoch 182/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5758 - soft_acc: 0.6413 - val_loss: 1.6019 - val_soft_acc: 0.4820\n",
      "Epoch 183/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5138 - soft_acc: 0.6560 - val_loss: 1.3129 - val_soft_acc: 0.5389\n",
      "Epoch 184/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.4815 - soft_acc: 0.6803 - val_loss: 1.6359 - val_soft_acc: 0.4941\n",
      "Epoch 185/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8319 - soft_acc: 0.5622 - val_loss: 1.7221 - val_soft_acc: 0.4557\n",
      "Epoch 186/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5957 - soft_acc: 0.6239 - val_loss: 1.4271 - val_soft_acc: 0.5133\n",
      "Epoch 187/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4259 - soft_acc: 0.6903 - val_loss: 1.3612 - val_soft_acc: 0.5482\n",
      "Epoch 188/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4475 - soft_acc: 0.6922 - val_loss: 1.4054 - val_soft_acc: 0.5238\n",
      "Epoch 189/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4989 - soft_acc: 0.6724 - val_loss: 1.6825 - val_soft_acc: 0.4773\n",
      "Epoch 190/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5754 - soft_acc: 0.6393 - val_loss: 1.5014 - val_soft_acc: 0.5162\n",
      "Epoch 191/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5590 - soft_acc: 0.6474 - val_loss: 1.4362 - val_soft_acc: 0.5414\n",
      "Epoch 192/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4879 - soft_acc: 0.6727 - val_loss: 1.3279 - val_soft_acc: 0.5576\n",
      "Epoch 193/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3767 - soft_acc: 0.7223 - val_loss: 1.3195 - val_soft_acc: 0.5500\n",
      "Epoch 194/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3817 - soft_acc: 0.7230 - val_loss: 1.2673 - val_soft_acc: 0.5662\n",
      "Epoch 195/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.4773 - soft_acc: 0.6856 - val_loss: 1.4748 - val_soft_acc: 0.5160\n",
      "Epoch 196/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.5700 - soft_acc: 0.6439 - val_loss: 1.4394 - val_soft_acc: 0.5260\n",
      "Epoch 197/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4429 - soft_acc: 0.6804 - val_loss: 1.3106 - val_soft_acc: 0.5588\n",
      "Epoch 198/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4383 - soft_acc: 0.6905 - val_loss: 1.3718 - val_soft_acc: 0.5461\n",
      "Epoch 199/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4536 - soft_acc: 0.6813 - val_loss: 1.3692 - val_soft_acc: 0.5350\n",
      "Epoch 200/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.5496 - soft_acc: 0.6548 - val_loss: 1.7138 - val_soft_acc: 0.4691\n",
      "Epoch 201/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5761 - soft_acc: 0.6370 - val_loss: 1.4327 - val_soft_acc: 0.5326\n",
      "Epoch 202/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3456 - soft_acc: 0.7284 - val_loss: 1.2999 - val_soft_acc: 0.5645\n",
      "Epoch 203/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.3368 - soft_acc: 0.7496 - val_loss: 1.2889 - val_soft_acc: 0.5613\n",
      "Epoch 204/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3535 - soft_acc: 0.7376 - val_loss: 1.3475 - val_soft_acc: 0.5492\n",
      "Epoch 205/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.3769 - soft_acc: 0.7210 - val_loss: 1.2757 - val_soft_acc: 0.5695\n",
      "Epoch 206/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.4370 - soft_acc: 0.7027 - val_loss: 1.5768 - val_soft_acc: 0.5078\n",
      "Epoch 207/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6932 - soft_acc: 0.6016 - val_loss: 1.6006 - val_soft_acc: 0.4893\n",
      "Epoch 208/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5910 - soft_acc: 0.6290 - val_loss: 1.4865 - val_soft_acc: 0.5223\n",
      "Epoch 209/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3792 - soft_acc: 0.7112 - val_loss: 1.2856 - val_soft_acc: 0.5781\n",
      "Epoch 210/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2976 - soft_acc: 0.7611 - val_loss: 1.2053 - val_soft_acc: 0.5836\n",
      "Epoch 211/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3245 - soft_acc: 0.7609 - val_loss: 1.5880 - val_soft_acc: 0.5324\n",
      "Epoch 212/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4862 - soft_acc: 0.6730 - val_loss: 1.4343 - val_soft_acc: 0.5457\n",
      "Epoch 213/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5403 - soft_acc: 0.6536 - val_loss: 1.3665 - val_soft_acc: 0.5467\n",
      "Epoch 214/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.3436 - soft_acc: 0.7405 - val_loss: 1.3768 - val_soft_acc: 0.5574\n",
      "Epoch 215/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3595 - soft_acc: 0.7350 - val_loss: 1.5315 - val_soft_acc: 0.5295\n",
      "Epoch 216/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.4622 - soft_acc: 0.6856 - val_loss: 1.5426 - val_soft_acc: 0.5266\n",
      "Epoch 217/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4508 - soft_acc: 0.6819 - val_loss: 1.4750 - val_soft_acc: 0.5299\n",
      "Epoch 218/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3126 - soft_acc: 0.7446 - val_loss: 1.2329 - val_soft_acc: 0.5986\n",
      "Epoch 219/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3206 - soft_acc: 0.7565 - val_loss: 1.3776 - val_soft_acc: 0.5658\n",
      "Epoch 220/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.3626 - soft_acc: 0.7349 - val_loss: 1.4214 - val_soft_acc: 0.5531\n",
      "Epoch 221/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.3089 - soft_acc: 0.7515 - val_loss: 1.2948 - val_soft_acc: 0.5631\n",
      "Epoch 222/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.3377 - soft_acc: 0.7450 - val_loss: 1.3502 - val_soft_acc: 0.5582\n",
      "Epoch 223/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.3124 - soft_acc: 0.7496 - val_loss: 1.4459 - val_soft_acc: 0.5486\n",
      "Epoch 224/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.3570 - soft_acc: 0.7287 - val_loss: 1.2847 - val_soft_acc: 0.5836\n",
      "Epoch 225/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3363 - soft_acc: 0.7473 - val_loss: 1.3301 - val_soft_acc: 0.5684\n",
      "Epoch 226/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.4695 - soft_acc: 0.6892 - val_loss: 1.7236 - val_soft_acc: 0.4910\n",
      "Epoch 227/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5714 - soft_acc: 0.6357 - val_loss: 1.5694 - val_soft_acc: 0.5344\n",
      "Epoch 228/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.4314 - soft_acc: 0.6892 - val_loss: 1.3338 - val_soft_acc: 0.5742\n",
      "Epoch 229/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.2424 - soft_acc: 0.7814 - val_loss: 1.2240 - val_soft_acc: 0.6014\n",
      "Epoch 230/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.2572 - soft_acc: 0.7905 - val_loss: 1.2925 - val_soft_acc: 0.5781\n",
      "Epoch 231/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.2532 - soft_acc: 0.7919 - val_loss: 1.4365 - val_soft_acc: 0.5615\n",
      "Epoch 232/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.3474 - soft_acc: 0.7492 - val_loss: 1.5328 - val_soft_acc: 0.5285\n",
      "Epoch 233/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4693 - soft_acc: 0.6792 - val_loss: 1.4149 - val_soft_acc: 0.5502\n",
      "Epoch 234/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3541 - soft_acc: 0.7394 - val_loss: 1.3963 - val_soft_acc: 0.5527\n",
      "Epoch 235/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3022 - soft_acc: 0.7546 - val_loss: 1.1731 - val_soft_acc: 0.6008\n",
      "Epoch 236/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2644 - soft_acc: 0.7834 - val_loss: 1.3321 - val_soft_acc: 0.5920\n",
      "Epoch 237/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3670 - soft_acc: 0.7477 - val_loss: 1.6260 - val_soft_acc: 0.5172\n",
      "Epoch 238/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3711 - soft_acc: 0.7196 - val_loss: 1.3322 - val_soft_acc: 0.5613\n",
      "Epoch 239/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2705 - soft_acc: 0.7731 - val_loss: 1.2314 - val_soft_acc: 0.5926\n",
      "Epoch 240/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2426 - soft_acc: 0.7945 - val_loss: 1.4007 - val_soft_acc: 0.5842\n",
      "Epoch 241/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3345 - soft_acc: 0.7428 - val_loss: 1.4180 - val_soft_acc: 0.5516\n",
      "Epoch 242/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3509 - soft_acc: 0.7332 - val_loss: 1.3842 - val_soft_acc: 0.5596\n",
      "Epoch 243/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.3001 - soft_acc: 0.7564 - val_loss: 1.4049 - val_soft_acc: 0.5773\n",
      "Epoch 244/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4049 - soft_acc: 0.7210 - val_loss: 1.4411 - val_soft_acc: 0.5436\n",
      "Epoch 245/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.4695 - soft_acc: 0.6832 - val_loss: 1.3953 - val_soft_acc: 0.5588\n",
      "Epoch 246/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.2303 - soft_acc: 0.7882 - val_loss: 1.2328 - val_soft_acc: 0.6057\n",
      "Epoch 247/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1448 - soft_acc: 0.8506 - val_loss: 1.1914 - val_soft_acc: 0.6377\n",
      "Epoch 248/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1409 - soft_acc: 0.8645 - val_loss: 1.1977 - val_soft_acc: 0.6309\n",
      "Epoch 249/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1347 - soft_acc: 0.8704 - val_loss: 1.1741 - val_soft_acc: 0.6361\n",
      "Epoch 250/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1858 - soft_acc: 0.8374 - val_loss: 1.2745 - val_soft_acc: 0.5988\n",
      "Epoch 251/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2505 - soft_acc: 0.7930 - val_loss: 1.3508 - val_soft_acc: 0.5588\n",
      "Epoch 252/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8807 - soft_acc: 0.5699 - val_loss: 1.9441 - val_soft_acc: 0.4613\n",
      "Epoch 253/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6031 - soft_acc: 0.6298 - val_loss: 1.6252 - val_soft_acc: 0.4945\n",
      "Epoch 254/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9611 - soft_acc: 0.5488 - val_loss: 1.4173 - val_soft_acc: 0.5373\n",
      "Epoch 255/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3109 - soft_acc: 0.7315 - val_loss: 1.2128 - val_soft_acc: 0.6008\n",
      "Epoch 256/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1739 - soft_acc: 0.8236 - val_loss: 1.2540 - val_soft_acc: 0.6072\n",
      "Epoch 257/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2229 - soft_acc: 0.8158 - val_loss: 1.2655 - val_soft_acc: 0.6115\n",
      "Epoch 258/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1850 - soft_acc: 0.8322 - val_loss: 1.2810 - val_soft_acc: 0.6072\n",
      "Epoch 259/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1648 - soft_acc: 0.8440 - val_loss: 1.2011 - val_soft_acc: 0.6227\n",
      "Epoch 260/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1529 - soft_acc: 0.8520 - val_loss: 1.2326 - val_soft_acc: 0.6209\n",
      "Epoch 261/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1448 - soft_acc: 0.8612 - val_loss: 1.1804 - val_soft_acc: 0.6299\n",
      "Epoch 262/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1163 - soft_acc: 0.8780 - val_loss: 1.2444 - val_soft_acc: 0.6158\n",
      "Epoch 263/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2378 - soft_acc: 0.7990 - val_loss: 1.4194 - val_soft_acc: 0.5746\n",
      "Epoch 264/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5655 - soft_acc: 0.6656 - val_loss: 2.1566 - val_soft_acc: 0.4133\n",
      "Epoch 265/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7097 - soft_acc: 0.6036 - val_loss: 1.5317 - val_soft_acc: 0.5350\n",
      "Epoch 266/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3926 - soft_acc: 0.7017 - val_loss: 1.4183 - val_soft_acc: 0.5584\n",
      "Epoch 267/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3227 - soft_acc: 0.7411 - val_loss: 1.3029 - val_soft_acc: 0.6049\n",
      "Epoch 268/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1828 - soft_acc: 0.8251 - val_loss: 1.1803 - val_soft_acc: 0.6336\n",
      "Epoch 269/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1185 - soft_acc: 0.8728 - val_loss: 1.1042 - val_soft_acc: 0.6486\n",
      "Epoch 270/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1258 - soft_acc: 0.8812 - val_loss: 1.2424 - val_soft_acc: 0.6238\n",
      "Epoch 271/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1315 - soft_acc: 0.8772 - val_loss: 1.1814 - val_soft_acc: 0.6490\n",
      "Epoch 272/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.1117 - soft_acc: 0.8909 - val_loss: 1.2069 - val_soft_acc: 0.6236\n",
      "Epoch 273/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1877 - soft_acc: 0.8459 - val_loss: 1.2970 - val_soft_acc: 0.6012\n",
      "Epoch 274/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5273 - soft_acc: 0.6880 - val_loss: 1.7967 - val_soft_acc: 0.4578\n",
      "Epoch 275/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7209 - soft_acc: 0.5971 - val_loss: 1.5525 - val_soft_acc: 0.5322\n",
      "Epoch 276/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3876 - soft_acc: 0.7201 - val_loss: 1.2274 - val_soft_acc: 0.5961\n",
      "Epoch 277/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1409 - soft_acc: 0.8481 - val_loss: 1.1241 - val_soft_acc: 0.6363\n",
      "Epoch 278/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1002 - soft_acc: 0.8855 - val_loss: 1.1095 - val_soft_acc: 0.6504\n",
      "Epoch 279/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1996 - soft_acc: 0.8365 - val_loss: 1.1816 - val_soft_acc: 0.6299\n",
      "Epoch 280/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1208 - soft_acc: 0.8744 - val_loss: 1.1471 - val_soft_acc: 0.6510\n",
      "Epoch 281/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0950 - soft_acc: 0.8969 - val_loss: 1.1503 - val_soft_acc: 0.6545\n",
      "Epoch 282/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1156 - soft_acc: 0.8860 - val_loss: 1.1737 - val_soft_acc: 0.6244\n",
      "Epoch 283/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3356 - soft_acc: 0.7626 - val_loss: 1.5121 - val_soft_acc: 0.5400\n",
      "Epoch 284/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5230 - soft_acc: 0.6603 - val_loss: 1.4354 - val_soft_acc: 0.5457\n",
      "Epoch 285/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2877 - soft_acc: 0.7581 - val_loss: 1.3075 - val_soft_acc: 0.5939\n",
      "Epoch 286/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1598 - soft_acc: 0.8376 - val_loss: 1.1689 - val_soft_acc: 0.6230\n",
      "Epoch 287/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2172 - soft_acc: 0.8112 - val_loss: 1.2473 - val_soft_acc: 0.6066\n",
      "Epoch 288/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1698 - soft_acc: 0.8347 - val_loss: 1.1985 - val_soft_acc: 0.6369\n",
      "Epoch 289/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.3964 - soft_acc: 0.7490 - val_loss: 1.6930 - val_soft_acc: 0.4980\n",
      "Epoch 290/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5272 - soft_acc: 0.6537 - val_loss: 1.4632 - val_soft_acc: 0.5533\n",
      "Epoch 291/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3001 - soft_acc: 0.7512 - val_loss: 1.3654 - val_soft_acc: 0.5748\n",
      "Epoch 292/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2257 - soft_acc: 0.8020 - val_loss: 1.2272 - val_soft_acc: 0.6252\n",
      "Epoch 293/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0971 - soft_acc: 0.8827 - val_loss: 1.1097 - val_soft_acc: 0.6600\n",
      "Epoch 294/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1009 - soft_acc: 0.8963 - val_loss: 1.1307 - val_soft_acc: 0.6484\n",
      "Epoch 295/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1573 - soft_acc: 0.8586 - val_loss: 1.2057 - val_soft_acc: 0.6326\n",
      "Epoch 296/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1563 - soft_acc: 0.8484 - val_loss: 1.2485 - val_soft_acc: 0.6209\n",
      "Epoch 297/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2277 - soft_acc: 0.8012 - val_loss: 1.2199 - val_soft_acc: 0.6008\n",
      "Epoch 298/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1752 - soft_acc: 0.8262 - val_loss: 1.3875 - val_soft_acc: 0.5750\n",
      "Epoch 299/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2942 - soft_acc: 0.7664 - val_loss: 1.4467 - val_soft_acc: 0.5580\n",
      "Epoch 300/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3649 - soft_acc: 0.7264 - val_loss: 1.2596 - val_soft_acc: 0.6023\n",
      "Epoch 301/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1603 - soft_acc: 0.8277 - val_loss: 1.1326 - val_soft_acc: 0.6422\n",
      "Epoch 302/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1209 - soft_acc: 0.8688 - val_loss: 1.1571 - val_soft_acc: 0.6449\n",
      "Epoch 303/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3507 - soft_acc: 0.7527 - val_loss: 1.4309 - val_soft_acc: 0.5623\n",
      "Epoch 304/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3575 - soft_acc: 0.7356 - val_loss: 1.2723 - val_soft_acc: 0.5953\n",
      "Epoch 305/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1667 - soft_acc: 0.8317 - val_loss: 1.2873 - val_soft_acc: 0.6244\n",
      "Epoch 306/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1099 - soft_acc: 0.8752 - val_loss: 1.1711 - val_soft_acc: 0.6566\n",
      "Epoch 307/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0916 - soft_acc: 0.8959 - val_loss: 1.0859 - val_soft_acc: 0.6650\n",
      "Epoch 308/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1373 - soft_acc: 0.8710 - val_loss: 1.2637 - val_soft_acc: 0.6186\n",
      "Epoch 309/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2024 - soft_acc: 0.8325 - val_loss: 1.3190 - val_soft_acc: 0.6129\n",
      "Epoch 310/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3205 - soft_acc: 0.7525 - val_loss: 1.3426 - val_soft_acc: 0.5848\n",
      "Epoch 311/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2888 - soft_acc: 0.7620 - val_loss: 1.5025 - val_soft_acc: 0.5594\n",
      "Epoch 312/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3200 - soft_acc: 0.7646 - val_loss: 1.5190 - val_soft_acc: 0.5510\n",
      "Epoch 313/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2024 - soft_acc: 0.8084 - val_loss: 1.1453 - val_soft_acc: 0.6432\n",
      "Epoch 314/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1010 - soft_acc: 0.8908 - val_loss: 1.1243 - val_soft_acc: 0.6639\n",
      "Epoch 315/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0551 - soft_acc: 0.9234 - val_loss: 1.0964 - val_soft_acc: 0.6758\n",
      "Epoch 316/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0532 - soft_acc: 0.9332 - val_loss: 1.0861 - val_soft_acc: 0.6814\n",
      "Epoch 317/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0578 - soft_acc: 0.9301 - val_loss: 1.1113 - val_soft_acc: 0.6666\n",
      "Epoch 318/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0919 - soft_acc: 0.9095 - val_loss: 1.1669 - val_soft_acc: 0.6355\n",
      "Epoch 319/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1804 - soft_acc: 0.8400 - val_loss: 1.3041 - val_soft_acc: 0.5959\n",
      "Epoch 320/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7423 - soft_acc: 0.6202 - val_loss: 1.7146 - val_soft_acc: 0.4914\n",
      "Epoch 321/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4503 - soft_acc: 0.6776 - val_loss: 1.3762 - val_soft_acc: 0.5787\n",
      "Epoch 322/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2125 - soft_acc: 0.8004 - val_loss: 1.2509 - val_soft_acc: 0.6324\n",
      "Epoch 323/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1866 - soft_acc: 0.8419 - val_loss: 1.1486 - val_soft_acc: 0.6432\n",
      "Epoch 324/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0885 - soft_acc: 0.8978 - val_loss: 1.0898 - val_soft_acc: 0.6645\n",
      "Epoch 325/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1053 - soft_acc: 0.8974 - val_loss: 1.2019 - val_soft_acc: 0.6402\n",
      "Epoch 326/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0889 - soft_acc: 0.9040 - val_loss: 1.1512 - val_soft_acc: 0.6578\n",
      "Epoch 327/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1239 - soft_acc: 0.8797 - val_loss: 1.1732 - val_soft_acc: 0.6377\n",
      "Epoch 328/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0877 - soft_acc: 0.8979 - val_loss: 1.1278 - val_soft_acc: 0.6619\n",
      "Epoch 329/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0681 - soft_acc: 0.9187 - val_loss: 1.2431 - val_soft_acc: 0.6180\n",
      "Epoch 330/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5698 - soft_acc: 0.6616 - val_loss: 1.7807 - val_soft_acc: 0.4898\n",
      "Epoch 331/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4488 - soft_acc: 0.6877 - val_loss: 1.3276 - val_soft_acc: 0.5770\n",
      "Epoch 332/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.2130 - soft_acc: 0.8039 - val_loss: 1.1609 - val_soft_acc: 0.6326\n",
      "Epoch 333/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1666 - soft_acc: 0.8443 - val_loss: 1.1300 - val_soft_acc: 0.6451\n",
      "Epoch 334/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1396 - soft_acc: 0.8665 - val_loss: 1.1450 - val_soft_acc: 0.6576\n",
      "Epoch 335/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0962 - soft_acc: 0.8932 - val_loss: 1.0725 - val_soft_acc: 0.6680\n",
      "Epoch 336/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0624 - soft_acc: 0.9175 - val_loss: 1.0805 - val_soft_acc: 0.6807\n",
      "Epoch 337/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0457 - soft_acc: 0.9382 - val_loss: 1.1294 - val_soft_acc: 0.6768\n",
      "Epoch 338/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0667 - soft_acc: 0.9295 - val_loss: 1.0957 - val_soft_acc: 0.6846\n",
      "Epoch 339/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0450 - soft_acc: 0.9414 - val_loss: 1.0519 - val_soft_acc: 0.6803\n",
      "Epoch 340/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0810 - soft_acc: 0.9192 - val_loss: 1.1276 - val_soft_acc: 0.6393\n",
      "Epoch 341/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5269 - soft_acc: 0.7099 - val_loss: 1.7540 - val_soft_acc: 0.4818\n",
      "Epoch 342/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.7819 - soft_acc: 0.5851 - val_loss: 1.8231 - val_soft_acc: 0.4547\n",
      "Epoch 343/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.5716 - soft_acc: 0.6533 - val_loss: 1.1821 - val_soft_acc: 0.6092\n",
      "Epoch 344/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1095 - soft_acc: 0.8536 - val_loss: 1.0793 - val_soft_acc: 0.6639\n",
      "Epoch 345/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0516 - soft_acc: 0.9230 - val_loss: 1.0389 - val_soft_acc: 0.6797\n",
      "Epoch 346/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0317 - soft_acc: 0.9468 - val_loss: 1.0527 - val_soft_acc: 0.6873\n",
      "Epoch 347/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0273 - soft_acc: 0.9540 - val_loss: 1.0297 - val_soft_acc: 0.6883\n",
      "Epoch 348/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0292 - soft_acc: 0.9555 - val_loss: 1.0123 - val_soft_acc: 0.6889\n",
      "Epoch 349/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0289 - soft_acc: 0.9562 - val_loss: 1.0425 - val_soft_acc: 0.6846\n",
      "Epoch 350/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1715 - soft_acc: 0.8775 - val_loss: 1.3954 - val_soft_acc: 0.5973\n",
      "Epoch 351/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5137 - soft_acc: 0.6810 - val_loss: 1.7183 - val_soft_acc: 0.5254\n",
      "Epoch 352/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3836 - soft_acc: 0.7159 - val_loss: 1.4130 - val_soft_acc: 0.5760\n",
      "Epoch 353/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2122 - soft_acc: 0.8107 - val_loss: 1.1606 - val_soft_acc: 0.6436\n",
      "Epoch 354/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1417 - soft_acc: 0.8566 - val_loss: 1.0796 - val_soft_acc: 0.6617\n",
      "Epoch 355/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0665 - soft_acc: 0.9159 - val_loss: 1.1011 - val_soft_acc: 0.6842\n",
      "Epoch 356/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0528 - soft_acc: 0.9333 - val_loss: 1.1639 - val_soft_acc: 0.6594\n",
      "Epoch 357/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0677 - soft_acc: 0.9244 - val_loss: 1.0753 - val_soft_acc: 0.6805\n",
      "Epoch 358/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0566 - soft_acc: 0.9304 - val_loss: 1.1124 - val_soft_acc: 0.6719\n",
      "Epoch 359/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0740 - soft_acc: 0.9162 - val_loss: 1.1189 - val_soft_acc: 0.6619\n",
      "Epoch 360/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1429 - soft_acc: 0.8731 - val_loss: 1.4396 - val_soft_acc: 0.5811\n",
      "Epoch 361/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6568 - soft_acc: 0.6468 - val_loss: 1.8119 - val_soft_acc: 0.4775\n",
      "Epoch 362/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5101 - soft_acc: 0.6747 - val_loss: 1.3843 - val_soft_acc: 0.5811\n",
      "Epoch 363/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1751 - soft_acc: 0.8147 - val_loss: 1.1619 - val_soft_acc: 0.6412\n",
      "Epoch 364/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0960 - soft_acc: 0.8900 - val_loss: 1.1034 - val_soft_acc: 0.6617\n",
      "Epoch 365/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0515 - soft_acc: 0.9292 - val_loss: 1.0599 - val_soft_acc: 0.6898\n",
      "Epoch 366/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0424 - soft_acc: 0.9429 - val_loss: 1.0539 - val_soft_acc: 0.6777\n",
      "Epoch 367/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0344 - soft_acc: 0.9493 - val_loss: 1.0424 - val_soft_acc: 0.6967\n",
      "Epoch 368/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0227 - soft_acc: 0.9595 - val_loss: 1.0845 - val_soft_acc: 0.6807\n",
      "Epoch 369/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0222 - soft_acc: 0.9610 - val_loss: 1.0734 - val_soft_acc: 0.6951\n",
      "Epoch 370/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3077 - soft_acc: 0.7886 - val_loss: 1.6304 - val_soft_acc: 0.5293\n",
      "Epoch 371/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6125 - soft_acc: 0.6398 - val_loss: 1.5800 - val_soft_acc: 0.5086\n",
      "Epoch 372/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2085 - soft_acc: 0.7945 - val_loss: 1.1921 - val_soft_acc: 0.6318\n",
      "Epoch 373/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1056 - soft_acc: 0.8874 - val_loss: 1.1695 - val_soft_acc: 0.6545\n",
      "Epoch 374/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0426 - soft_acc: 0.9324 - val_loss: 1.0872 - val_soft_acc: 0.6855\n",
      "Epoch 375/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0819 - soft_acc: 0.9208 - val_loss: 1.3364 - val_soft_acc: 0.6053\n",
      "Epoch 376/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1874 - soft_acc: 0.8353 - val_loss: 1.1858 - val_soft_acc: 0.6420\n",
      "Epoch 377/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1107 - soft_acc: 0.8885 - val_loss: 1.0863 - val_soft_acc: 0.6730\n",
      "Epoch 378/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0556 - soft_acc: 0.9286 - val_loss: 1.0794 - val_soft_acc: 0.6770\n",
      "Epoch 379/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0726 - soft_acc: 0.9205 - val_loss: 1.0850 - val_soft_acc: 0.6748\n",
      "Epoch 380/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0507 - soft_acc: 0.9326 - val_loss: 1.0925 - val_soft_acc: 0.6787\n",
      "Epoch 381/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0380 - soft_acc: 0.9464 - val_loss: 1.1041 - val_soft_acc: 0.6773\n",
      "Epoch 382/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.2054 - soft_acc: 0.8627 - val_loss: 2.2916 - val_soft_acc: 0.4299\n",
      "Epoch 383/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.8264 - soft_acc: 0.5791 - val_loss: 1.5897 - val_soft_acc: 0.5359\n",
      "Epoch 384/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.3231 - soft_acc: 0.7365 - val_loss: 1.2755 - val_soft_acc: 0.6098\n",
      "Epoch 385/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1631 - soft_acc: 0.8359 - val_loss: 1.2351 - val_soft_acc: 0.6377\n",
      "Epoch 386/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0796 - soft_acc: 0.9017 - val_loss: 1.0410 - val_soft_acc: 0.6869\n",
      "Epoch 387/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0420 - soft_acc: 0.9386 - val_loss: 1.0437 - val_soft_acc: 0.6885\n",
      "Epoch 388/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0426 - soft_acc: 0.9426 - val_loss: 1.0395 - val_soft_acc: 0.6877\n",
      "Epoch 389/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0451 - soft_acc: 0.9467 - val_loss: 1.0231 - val_soft_acc: 0.6984\n",
      "Epoch 390/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0276 - soft_acc: 0.9583 - val_loss: 1.0722 - val_soft_acc: 0.6805\n",
      "Epoch 391/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0641 - soft_acc: 0.9326 - val_loss: 1.1879 - val_soft_acc: 0.6480\n",
      "Epoch 392/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0405 - soft_acc: 0.9383 - val_loss: 1.1489 - val_soft_acc: 0.6654\n",
      "Epoch 393/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.5993 - soft_acc: 0.6688 - val_loss: 1.7921 - val_soft_acc: 0.4908\n",
      "Epoch 394/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4178 - soft_acc: 0.6972 - val_loss: 1.3402 - val_soft_acc: 0.6084\n",
      "Epoch 395/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.2077 - soft_acc: 0.7988 - val_loss: 1.1384 - val_soft_acc: 0.6441\n",
      "Epoch 396/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0662 - soft_acc: 0.9044 - val_loss: 1.0826 - val_soft_acc: 0.6750\n",
      "Epoch 397/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0316 - soft_acc: 0.9448 - val_loss: 1.0684 - val_soft_acc: 0.6859\n",
      "Epoch 398/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0213 - soft_acc: 0.9618 - val_loss: 1.0559 - val_soft_acc: 0.6855\n",
      "Epoch 00398: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c784037f28>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "ann.fit(X_scaled_training,Y_scaled_training,validation_data=(X_scaled_testing, Y_scaled_testing),batch_size=70,epochs=5000 ,callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = ann.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "ann.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "\n",
    " \n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz=loaded_model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Deep neural network Trained on Original Data\n",
      "Average absolute error: 0.26 degrees.\n",
      "Training Accuracy: 93.22 %.\n"
     ]
    }
   ],
   "source": [
    "errors = abs(ynew - Y_scaled_training)\n",
    "\n",
    "print('Metrics for Deep neural network Trained on Original Data')\n",
    "print('Average absolute error:', round(np.mean(errors), 2), 'degrees.')\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / Y_scaled_training)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Training Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Deep neural network Trained on Original Data\n",
      "Average absolute error: 0.67 degrees.\n",
      "Testing Accuracy: 81.26 %.\n"
     ]
    }
   ],
   "source": [
    "errors = abs(ynew - Y_scaled_testing)\n",
    "\n",
    "print('Metrics for Deep neural network Trained on Original Data')\n",
    "print('Average absolute error:', round(np.mean(errors), 2), 'degrees.')\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / Y_scaled_testing)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Testing Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.6 , 7.71],\n",
       "       [7.6 , 7.71],\n",
       "       [7.31, 8.1 ],\n",
       "       ...,\n",
       "       [7.17, 3.99],\n",
       "       [7.17, 3.99],\n",
       "       [4.03, 7.15]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_scaled_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import adam\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "def create_model():\n",
    "\n",
    "\n",
    "    ann= Sequential()\n",
    "\n",
    "\n",
    "\n",
    "    ann.add(Dense(output_dim=512,kernel_initializer='glorot_uniform',bias_initializer=\"Zeros\",activation='relu',input_dim=70))\n",
    "\n",
    "    ann.add(Dense(output_dim=1024,kernel_initializer='glorot_uniform',bias_initializer=\"Zeros\",activation='relu'))\n",
    "\n",
    "    ann.add(Dense(output_dim=2048,kernel_initializer='glorot_uniform',bias_initializer=\"Zeros\",activation='relu'))\n",
    "\n",
    "    ann.add(Dense(output_dim=1024,kernel_initializer='glorot_uniform',bias_initializer=\"Zeros\",activation='relu'))\n",
    "\n",
    "    ann.add(Dense(output_dim=512,kernel_initializer='glorot_uniform',bias_initializer=\"Zeros\",activation='relu'))\n",
    "\n",
    "    ann.add(Dense(output_dim=2,kernel_initializer='glorot_uniform' ,bias_initializer=\"Zeros\",activation='linear'))\n",
    "\n",
    "    opt = adam(lr=0.001)\n",
    "\n",
    "    ann.compile(optimizer=opt,loss='mean_squared_error',metrics=[soft_acc])\n",
    "\n",
    "    return ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluate a single mlp model\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "\t# encode targets\n",
    "\n",
    "\t# define model\n",
    "\tmodel=create_model()\n",
    "\t# fit model\n",
    "\tes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "\tmodel.fit(trainX, trainy,validation_data=(X_scaled_valid, Y_scaled_valid), batch_size=70,epochs=5000 ,callbacks=[es])\n",
    "\t# evaluate the model\n",
    "\t_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "\treturn model, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"glorot_uniform\", bias_initializer=\"Zeros\", activation=\"relu\", input_dim=70, units=512)`\n",
      "D:\\Users\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"glorot_uniform\", bias_initializer=\"Zeros\", activation=\"relu\", units=1024)`\n",
      "D:\\Users\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"glorot_uniform\", bias_initializer=\"Zeros\", activation=\"relu\", units=2048)`\n",
      "D:\\Users\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"glorot_uniform\", bias_initializer=\"Zeros\", activation=\"relu\", units=1024)`\n",
      "D:\\Users\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"glorot_uniform\", bias_initializer=\"Zeros\", activation=\"relu\", units=512)`\n",
      "D:\\Users\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"glorot_uniform\", bias_initializer=\"Zeros\", activation=\"linear\", units=2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14640 samples, validate on 2440 samples\n",
      "Epoch 1/5000\n",
      "14640/14640 [==============================] - 1s 99us/step - loss: 4.9157 - soft_acc: 0.1511 - val_loss: 4.2079 - val_soft_acc: 0.1420\n",
      "Epoch 2/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 4.2235 - soft_acc: 0.1593 - val_loss: 4.0806 - val_soft_acc: 0.1699\n",
      "Epoch 3/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 4.1215 - soft_acc: 0.1614 - val_loss: 4.7133 - val_soft_acc: 0.1574\n",
      "Epoch 4/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 4.0578 - soft_acc: 0.1672 - val_loss: 3.8757 - val_soft_acc: 0.1701\n",
      "Epoch 5/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 3.9514 - soft_acc: 0.1747 - val_loss: 3.9421 - val_soft_acc: 0.1684\n",
      "Epoch 6/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 3.9182 - soft_acc: 0.1726 - val_loss: 3.7232 - val_soft_acc: 0.1809\n",
      "Epoch 7/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 3.8861 - soft_acc: 0.1769 - val_loss: 3.7054 - val_soft_acc: 0.1695\n",
      "Epoch 8/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 3.8279 - soft_acc: 0.1733 - val_loss: 3.7030 - val_soft_acc: 0.1836\n",
      "Epoch 9/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.8180 - soft_acc: 0.1789 - val_loss: 3.7476 - val_soft_acc: 0.1752\n",
      "Epoch 10/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.8234 - soft_acc: 0.1798 - val_loss: 3.7137 - val_soft_acc: 0.1943\n",
      "Epoch 11/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.7959 - soft_acc: 0.1769 - val_loss: 3.7118 - val_soft_acc: 0.1990\n",
      "Epoch 12/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.7439 - soft_acc: 0.1778 - val_loss: 3.7969 - val_soft_acc: 0.1967\n",
      "Epoch 13/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.7349 - soft_acc: 0.1798 - val_loss: 3.6294 - val_soft_acc: 0.1807\n",
      "Epoch 14/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.7117 - soft_acc: 0.1797 - val_loss: 3.6489 - val_soft_acc: 0.1955\n",
      "Epoch 15/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.6878 - soft_acc: 0.1852 - val_loss: 3.5238 - val_soft_acc: 0.1889\n",
      "Epoch 16/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.6398 - soft_acc: 0.1879 - val_loss: 3.5390 - val_soft_acc: 0.1859\n",
      "Epoch 17/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.6291 - soft_acc: 0.1861 - val_loss: 3.4869 - val_soft_acc: 0.1855\n",
      "Epoch 18/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.6494 - soft_acc: 0.1876 - val_loss: 3.4840 - val_soft_acc: 0.1951\n",
      "Epoch 19/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.5974 - soft_acc: 0.1863 - val_loss: 3.4772 - val_soft_acc: 0.1930\n",
      "Epoch 20/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.5817 - soft_acc: 0.1875 - val_loss: 3.4458 - val_soft_acc: 0.1926\n",
      "Epoch 21/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.5483 - soft_acc: 0.1922 - val_loss: 3.4452 - val_soft_acc: 0.1711\n",
      "Epoch 22/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.5396 - soft_acc: 0.1919 - val_loss: 3.4401 - val_soft_acc: 0.1855\n",
      "Epoch 23/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 3.5018 - soft_acc: 0.1931 - val_loss: 3.4238 - val_soft_acc: 0.1973\n",
      "Epoch 24/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.4886 - soft_acc: 0.1954 - val_loss: 3.3956 - val_soft_acc: 0.1984\n",
      "Epoch 25/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.5078 - soft_acc: 0.1926 - val_loss: 3.4672 - val_soft_acc: 0.2059\n",
      "Epoch 26/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.4796 - soft_acc: 0.1935 - val_loss: 3.3400 - val_soft_acc: 0.1961\n",
      "Epoch 27/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.4447 - soft_acc: 0.1970 - val_loss: 3.3875 - val_soft_acc: 0.1926\n",
      "Epoch 28/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.4123 - soft_acc: 0.1998 - val_loss: 3.4319 - val_soft_acc: 0.1893\n",
      "Epoch 29/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.4083 - soft_acc: 0.2020 - val_loss: 3.3645 - val_soft_acc: 0.1955\n",
      "Epoch 30/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.3664 - soft_acc: 0.2048 - val_loss: 3.3435 - val_soft_acc: 0.1904\n",
      "Epoch 31/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.3579 - soft_acc: 0.2045 - val_loss: 3.3960 - val_soft_acc: 0.1949\n",
      "Epoch 32/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.3467 - soft_acc: 0.2071 - val_loss: 3.2815 - val_soft_acc: 0.2082\n",
      "Epoch 33/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.3282 - soft_acc: 0.2055 - val_loss: 3.2997 - val_soft_acc: 0.2092\n",
      "Epoch 34/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.2911 - soft_acc: 0.2082 - val_loss: 3.2409 - val_soft_acc: 0.2135\n",
      "Epoch 35/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.3088 - soft_acc: 0.2071 - val_loss: 3.3257 - val_soft_acc: 0.2066\n",
      "Epoch 36/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.2519 - soft_acc: 0.2084 - val_loss: 3.3113 - val_soft_acc: 0.1994\n",
      "Epoch 37/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.2627 - soft_acc: 0.2090 - val_loss: 3.2664 - val_soft_acc: 0.2207\n",
      "Epoch 38/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.2283 - soft_acc: 0.2124 - val_loss: 3.1559 - val_soft_acc: 0.2256\n",
      "Epoch 39/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.2283 - soft_acc: 0.2132 - val_loss: 3.1816 - val_soft_acc: 0.2090\n",
      "Epoch 40/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.1854 - soft_acc: 0.2136 - val_loss: 3.1730 - val_soft_acc: 0.2102\n",
      "Epoch 41/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.1799 - soft_acc: 0.2173 - val_loss: 3.1749 - val_soft_acc: 0.2012\n",
      "Epoch 42/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.1686 - soft_acc: 0.2212 - val_loss: 3.1102 - val_soft_acc: 0.2291\n",
      "Epoch 43/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 3.1374 - soft_acc: 0.21 - 1s 74us/step - loss: 3.1370 - soft_acc: 0.2189 - val_loss: 3.1426 - val_soft_acc: 0.2199\n",
      "Epoch 44/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.1272 - soft_acc: 0.2196 - val_loss: 3.0990 - val_soft_acc: 0.2252\n",
      "Epoch 45/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.0771 - soft_acc: 0.2264 - val_loss: 3.1152 - val_soft_acc: 0.2281\n",
      "Epoch 46/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.0879 - soft_acc: 0.2233 - val_loss: 3.0693 - val_soft_acc: 0.2367\n",
      "Epoch 47/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.0610 - soft_acc: 0.2288 - val_loss: 3.0188 - val_soft_acc: 0.2404\n",
      "Epoch 48/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.0468 - soft_acc: 0.2302 - val_loss: 3.0153 - val_soft_acc: 0.2264\n",
      "Epoch 49/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.0306 - soft_acc: 0.2289 - val_loss: 3.0303 - val_soft_acc: 0.2266\n",
      "Epoch 50/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.0323 - soft_acc: 0.2274 - val_loss: 3.0480 - val_soft_acc: 0.2375\n",
      "Epoch 51/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.9800 - soft_acc: 0.2340 - val_loss: 3.0260 - val_soft_acc: 0.2359\n",
      "Epoch 52/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.0295 - soft_acc: 0.2284 - val_loss: 3.0304 - val_soft_acc: 0.2455\n",
      "Epoch 53/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.9516 - soft_acc: 0.2358 - val_loss: 2.9952 - val_soft_acc: 0.2289\n",
      "Epoch 54/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.9218 - soft_acc: 0.2398 - val_loss: 2.9904 - val_soft_acc: 0.2365\n",
      "Epoch 55/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.8956 - soft_acc: 0.2452 - val_loss: 2.9850 - val_soft_acc: 0.2256\n",
      "Epoch 56/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.9001 - soft_acc: 0.2439 - val_loss: 2.9655 - val_soft_acc: 0.2424\n",
      "Epoch 57/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.8862 - soft_acc: 0.2401 - val_loss: 2.9065 - val_soft_acc: 0.2357\n",
      "Epoch 58/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.8680 - soft_acc: 0.2483 - val_loss: 2.8774 - val_soft_acc: 0.2439\n",
      "Epoch 59/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.7818 - soft_acc: 0.2489 - val_loss: 2.8590 - val_soft_acc: 0.2504\n",
      "Epoch 60/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.8106 - soft_acc: 0.2475 - val_loss: 2.7843 - val_soft_acc: 0.2590\n",
      "Epoch 61/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.7959 - soft_acc: 0.2502 - val_loss: 2.7951 - val_soft_acc: 0.2520\n",
      "Epoch 62/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.7449 - soft_acc: 0.2561 - val_loss: 2.7849 - val_soft_acc: 0.2605\n",
      "Epoch 63/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.7193 - soft_acc: 0.2596 - val_loss: 2.8301 - val_soft_acc: 0.2531\n",
      "Epoch 64/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.6784 - soft_acc: 0.2623 - val_loss: 2.8059 - val_soft_acc: 0.2707\n",
      "Epoch 65/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.6774 - soft_acc: 0.2647 - val_loss: 2.7816 - val_soft_acc: 0.2539\n",
      "Epoch 66/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.6662 - soft_acc: 0.2595 - val_loss: 2.7639 - val_soft_acc: 0.2672\n",
      "Epoch 67/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.6208 - soft_acc: 0.2693 - val_loss: 2.7852 - val_soft_acc: 0.2609\n",
      "Epoch 68/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.6367 - soft_acc: 0.2681 - val_loss: 2.7839 - val_soft_acc: 0.2416\n",
      "Epoch 69/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.5800 - soft_acc: 0.2714 - val_loss: 2.6909 - val_soft_acc: 0.2662\n",
      "Epoch 70/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.5505 - soft_acc: 0.2760 - val_loss: 2.7268 - val_soft_acc: 0.2541\n",
      "Epoch 71/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.5272 - soft_acc: 0.2748 - val_loss: 2.6330 - val_soft_acc: 0.2699\n",
      "Epoch 72/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.4651 - soft_acc: 0.2840 - val_loss: 2.6133 - val_soft_acc: 0.2850\n",
      "Epoch 73/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.4681 - soft_acc: 0.2834 - val_loss: 2.6285 - val_soft_acc: 0.2846\n",
      "Epoch 74/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.4558 - soft_acc: 0.2856 - val_loss: 2.6187 - val_soft_acc: 0.2746\n",
      "Epoch 75/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.4326 - soft_acc: 0.2819 - val_loss: 2.5864 - val_soft_acc: 0.2930\n",
      "Epoch 76/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.3888 - soft_acc: 0.2944 - val_loss: 2.8042 - val_soft_acc: 0.2701\n",
      "Epoch 77/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.3442 - soft_acc: 0.2954 - val_loss: 2.5139 - val_soft_acc: 0.2947\n",
      "Epoch 78/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.3513 - soft_acc: 0.2977 - val_loss: 2.6473 - val_soft_acc: 0.2939\n",
      "Epoch 79/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.3123 - soft_acc: 0.2996 - val_loss: 2.4863 - val_soft_acc: 0.2916\n",
      "Epoch 80/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.2694 - soft_acc: 0.3028 - val_loss: 2.4470 - val_soft_acc: 0.2943\n",
      "Epoch 81/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.2354 - soft_acc: 0.3077 - val_loss: 2.4980 - val_soft_acc: 0.2953\n",
      "Epoch 82/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.1935 - soft_acc: 0.3164 - val_loss: 2.4616 - val_soft_acc: 0.3080\n",
      "Epoch 83/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.1866 - soft_acc: 0.3119 - val_loss: 2.5342 - val_soft_acc: 0.3049\n",
      "Epoch 84/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.1526 - soft_acc: 0.3175 - val_loss: 2.4256 - val_soft_acc: 0.3111\n",
      "Epoch 85/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.1351 - soft_acc: 0.3164 - val_loss: 2.4036 - val_soft_acc: 0.3082\n",
      "Epoch 86/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.0685 - soft_acc: 0.3312 - val_loss: 2.3396 - val_soft_acc: 0.3189\n",
      "Epoch 87/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.0260 - soft_acc: 0.3297 - val_loss: 2.2745 - val_soft_acc: 0.3094\n",
      "Epoch 88/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.0135 - soft_acc: 0.3353 - val_loss: 2.2558 - val_soft_acc: 0.3238\n",
      "Epoch 89/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.9934 - soft_acc: 0.3355 - val_loss: 2.3644 - val_soft_acc: 0.3168\n",
      "Epoch 90/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.9446 - soft_acc: 0.3407 - val_loss: 2.3664 - val_soft_acc: 0.3047\n",
      "Epoch 91/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.9144 - soft_acc: 0.3444 - val_loss: 2.2639 - val_soft_acc: 0.3277\n",
      "Epoch 92/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.9325 - soft_acc: 0.3456 - val_loss: 2.2262 - val_soft_acc: 0.3275\n",
      "Epoch 93/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.8791 - soft_acc: 0.3503 - val_loss: 2.3445 - val_soft_acc: 0.3246\n",
      "Epoch 94/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.8837 - soft_acc: 0.3530 - val_loss: 2.1818 - val_soft_acc: 0.3367\n",
      "Epoch 95/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.7687 - soft_acc: 0.3670 - val_loss: 2.2959 - val_soft_acc: 0.3344\n",
      "Epoch 96/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.7416 - soft_acc: 0.3696 - val_loss: 2.0953 - val_soft_acc: 0.3387\n",
      "Epoch 97/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.7283 - soft_acc: 0.3716 - val_loss: 2.1706 - val_soft_acc: 0.3525\n",
      "Epoch 98/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.7424 - soft_acc: 0.3707 - val_loss: 2.0676 - val_soft_acc: 0.3613\n",
      "Epoch 99/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.7069 - soft_acc: 0.3725 - val_loss: 2.1457 - val_soft_acc: 0.3572\n",
      "Epoch 100/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.6520 - soft_acc: 0.3833 - val_loss: 2.1992 - val_soft_acc: 0.3545\n",
      "Epoch 101/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.6417 - soft_acc: 0.3856 - val_loss: 2.2755 - val_soft_acc: 0.3426\n",
      "Epoch 102/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.6301 - soft_acc: 0.3889 - val_loss: 1.9778 - val_soft_acc: 0.3672\n",
      "Epoch 103/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.5134 - soft_acc: 0.4100 - val_loss: 2.1598 - val_soft_acc: 0.3416\n",
      "Epoch 104/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.5459 - soft_acc: 0.4022 - val_loss: 1.9767 - val_soft_acc: 0.3629\n",
      "Epoch 105/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.4971 - soft_acc: 0.4059 - val_loss: 2.0479 - val_soft_acc: 0.3701\n",
      "Epoch 106/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.5235 - soft_acc: 0.3978 - val_loss: 2.1020 - val_soft_acc: 0.3598\n",
      "Epoch 107/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.4697 - soft_acc: 0.4159 - val_loss: 2.0355 - val_soft_acc: 0.3648\n",
      "Epoch 108/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.4546 - soft_acc: 0.4132 - val_loss: 1.9708 - val_soft_acc: 0.3734\n",
      "Epoch 109/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.4825 - soft_acc: 0.4089 - val_loss: 1.9566 - val_soft_acc: 0.3822\n",
      "Epoch 110/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.3949 - soft_acc: 0.4235 - val_loss: 1.9722 - val_soft_acc: 0.3875\n",
      "Epoch 111/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.3862 - soft_acc: 0.4277 - val_loss: 1.8586 - val_soft_acc: 0.3988\n",
      "Epoch 112/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.3812 - soft_acc: 0.4336 - val_loss: 2.0646 - val_soft_acc: 0.3785\n",
      "Epoch 113/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.3573 - soft_acc: 0.4366 - val_loss: 1.8887 - val_soft_acc: 0.4008\n",
      "Epoch 114/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.2829 - soft_acc: 0.4430 - val_loss: 1.9378 - val_soft_acc: 0.3699\n",
      "Epoch 115/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.3311 - soft_acc: 0.4379 - val_loss: 1.8594 - val_soft_acc: 0.3947\n",
      "Epoch 116/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.2535 - soft_acc: 0.4494 - val_loss: 1.9234 - val_soft_acc: 0.3984\n",
      "Epoch 117/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.1872 - soft_acc: 0.4619 - val_loss: 1.9968 - val_soft_acc: 0.4018\n",
      "Epoch 118/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.2685 - soft_acc: 0.4529 - val_loss: 1.8467 - val_soft_acc: 0.3900\n",
      "Epoch 119/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.2269 - soft_acc: 0.4521 - val_loss: 1.8818 - val_soft_acc: 0.4086\n",
      "Epoch 120/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.2002 - soft_acc: 0.4615 - val_loss: 1.9683 - val_soft_acc: 0.3951\n",
      "Epoch 121/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 1.1216 - soft_acc: 0.4720 - val_loss: 1.7803 - val_soft_acc: 0.4162\n",
      "Epoch 122/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.0623 - soft_acc: 0.4888 - val_loss: 1.7065 - val_soft_acc: 0.4303\n",
      "Epoch 123/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.0999 - soft_acc: 0.4818 - val_loss: 1.7230 - val_soft_acc: 0.4225\n",
      "Epoch 124/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.1089 - soft_acc: 0.4843 - val_loss: 1.7911 - val_soft_acc: 0.4086\n",
      "Epoch 125/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.1353 - soft_acc: 0.4741 - val_loss: 1.8686 - val_soft_acc: 0.4100\n",
      "Epoch 126/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.0868 - soft_acc: 0.4858 - val_loss: 1.7032 - val_soft_acc: 0.4240\n",
      "Epoch 127/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.0363 - soft_acc: 0.5004 - val_loss: 1.9080 - val_soft_acc: 0.4045\n",
      "Epoch 128/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.0566 - soft_acc: 0.4961 - val_loss: 1.7502 - val_soft_acc: 0.4299\n",
      "Epoch 129/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.0241 - soft_acc: 0.5003 - val_loss: 1.6939 - val_soft_acc: 0.4377\n",
      "Epoch 130/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.0236 - soft_acc: 0.5020 - val_loss: 1.8189 - val_soft_acc: 0.4377\n",
      "Epoch 131/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.9683 - soft_acc: 0.5159 - val_loss: 1.7065 - val_soft_acc: 0.4375\n",
      "Epoch 132/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9480 - soft_acc: 0.5176 - val_loss: 1.7280 - val_soft_acc: 0.4484\n",
      "Epoch 133/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9652 - soft_acc: 0.5192 - val_loss: 1.6852 - val_soft_acc: 0.4395\n",
      "Epoch 134/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9005 - soft_acc: 0.5268 - val_loss: 1.7500 - val_soft_acc: 0.4461\n",
      "Epoch 135/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9075 - soft_acc: 0.5285 - val_loss: 1.7973 - val_soft_acc: 0.4377\n",
      "Epoch 136/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8839 - soft_acc: 0.5360 - val_loss: 1.8686 - val_soft_acc: 0.4193\n",
      "Epoch 137/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9201 - soft_acc: 0.5332 - val_loss: 1.8150 - val_soft_acc: 0.4262\n",
      "Epoch 138/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9223 - soft_acc: 0.5288 - val_loss: 1.7522 - val_soft_acc: 0.4352\n",
      "Epoch 139/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8648 - soft_acc: 0.5491 - val_loss: 1.7130 - val_soft_acc: 0.4506\n",
      "Epoch 140/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8119 - soft_acc: 0.5590 - val_loss: 1.6831 - val_soft_acc: 0.4439\n",
      "Epoch 141/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8198 - soft_acc: 0.5551 - val_loss: 1.6929 - val_soft_acc: 0.4449\n",
      "Epoch 142/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8478 - soft_acc: 0.5485 - val_loss: 1.5955 - val_soft_acc: 0.4658\n",
      "Epoch 143/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7717 - soft_acc: 0.5691 - val_loss: 1.5946 - val_soft_acc: 0.4598\n",
      "Epoch 144/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.0549 - soft_acc: 0.4961 - val_loss: 1.7829 - val_soft_acc: 0.4451\n",
      "Epoch 145/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.8898 - soft_acc: 0.5332 - val_loss: 1.6545 - val_soft_acc: 0.4633\n",
      "Epoch 146/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8408 - soft_acc: 0.5480 - val_loss: 1.6321 - val_soft_acc: 0.4588\n",
      "Epoch 147/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.7856 - soft_acc: 0.5623 - val_loss: 1.7040 - val_soft_acc: 0.4582\n",
      "Epoch 148/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.7043 - soft_acc: 0.5898 - val_loss: 1.7087 - val_soft_acc: 0.4654\n",
      "Epoch 149/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7552 - soft_acc: 0.5777 - val_loss: 1.6109 - val_soft_acc: 0.4803\n",
      "Epoch 150/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8173 - soft_acc: 0.5583 - val_loss: 1.6761 - val_soft_acc: 0.4344\n",
      "Epoch 151/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.7245 - soft_acc: 0.5748 - val_loss: 1.6415 - val_soft_acc: 0.4680\n",
      "Epoch 152/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8377 - soft_acc: 0.5571 - val_loss: 1.7661 - val_soft_acc: 0.4664\n",
      "Epoch 153/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7517 - soft_acc: 0.5712 - val_loss: 1.6665 - val_soft_acc: 0.4660\n",
      "Epoch 154/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7664 - soft_acc: 0.5682 - val_loss: 1.5853 - val_soft_acc: 0.4803\n",
      "Epoch 155/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8415 - soft_acc: 0.5552 - val_loss: 1.5588 - val_soft_acc: 0.4947\n",
      "Epoch 156/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6899 - soft_acc: 0.5942 - val_loss: 1.5231 - val_soft_acc: 0.4898\n",
      "Epoch 157/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6660 - soft_acc: 0.6058 - val_loss: 1.4686 - val_soft_acc: 0.4811\n",
      "Epoch 158/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6223 - soft_acc: 0.6174 - val_loss: 1.6536 - val_soft_acc: 0.4613\n",
      "Epoch 159/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7573 - soft_acc: 0.5709 - val_loss: 1.6572 - val_soft_acc: 0.4730\n",
      "Epoch 160/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6987 - soft_acc: 0.5908 - val_loss: 1.4882 - val_soft_acc: 0.5066\n",
      "Epoch 161/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.5865 - soft_acc: 0.6284 - val_loss: 1.5253 - val_soft_acc: 0.5018\n",
      "Epoch 162/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5568 - soft_acc: 0.6407 - val_loss: 1.7108 - val_soft_acc: 0.4695\n",
      "Epoch 163/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.7564 - soft_acc: 0.5821 - val_loss: 1.6491 - val_soft_acc: 0.4633\n",
      "Epoch 164/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.7131 - soft_acc: 0.5872 - val_loss: 1.5863 - val_soft_acc: 0.4889\n",
      "Epoch 165/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5717 - soft_acc: 0.6279 - val_loss: 1.5645 - val_soft_acc: 0.4992\n",
      "Epoch 166/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5781 - soft_acc: 0.6350 - val_loss: 1.5402 - val_soft_acc: 0.5029\n",
      "Epoch 167/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.5790 - soft_acc: 0.6361 - val_loss: 1.5071 - val_soft_acc: 0.4980\n",
      "Epoch 168/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.5564 - soft_acc: 0.6362 - val_loss: 1.5044 - val_soft_acc: 0.5143\n",
      "Epoch 169/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.4833 - soft_acc: 0.6765 - val_loss: 1.5964 - val_soft_acc: 0.4949\n",
      "Epoch 170/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.5690 - soft_acc: 0.6391 - val_loss: 1.6252 - val_soft_acc: 0.4734\n",
      "Epoch 171/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.6275 - soft_acc: 0.6140 - val_loss: 1.5897 - val_soft_acc: 0.5012\n",
      "Epoch 172/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5301 - soft_acc: 0.6450 - val_loss: 1.4988 - val_soft_acc: 0.5043\n",
      "Epoch 173/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5788 - soft_acc: 0.6258 - val_loss: 1.6457 - val_soft_acc: 0.4686\n",
      "Epoch 174/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7108 - soft_acc: 0.5904 - val_loss: 1.6039 - val_soft_acc: 0.4977\n",
      "Epoch 175/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5360 - soft_acc: 0.6448 - val_loss: 1.5447 - val_soft_acc: 0.5084\n",
      "Epoch 176/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5565 - soft_acc: 0.6420 - val_loss: 1.4762 - val_soft_acc: 0.5162\n",
      "Epoch 177/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6569 - soft_acc: 0.6105 - val_loss: 1.6452 - val_soft_acc: 0.4855\n",
      "Epoch 178/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5373 - soft_acc: 0.6416 - val_loss: 1.4606 - val_soft_acc: 0.5139\n",
      "Epoch 179/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5527 - soft_acc: 0.6450 - val_loss: 1.5390 - val_soft_acc: 0.5000\n",
      "Epoch 180/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5802 - soft_acc: 0.6346 - val_loss: 1.5067 - val_soft_acc: 0.5109\n",
      "Epoch 181/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4805 - soft_acc: 0.6746 - val_loss: 1.4502 - val_soft_acc: 0.5293\n",
      "Epoch 182/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7340 - soft_acc: 0.5897 - val_loss: 1.6611 - val_soft_acc: 0.4955\n",
      "Epoch 183/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5318 - soft_acc: 0.6482 - val_loss: 1.4219 - val_soft_acc: 0.5186\n",
      "Epoch 184/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4901 - soft_acc: 0.6640 - val_loss: 1.4229 - val_soft_acc: 0.5090\n",
      "Epoch 185/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4117 - soft_acc: 0.7006 - val_loss: 1.4296 - val_soft_acc: 0.5486\n",
      "Epoch 186/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5129 - soft_acc: 0.6606 - val_loss: 1.4477 - val_soft_acc: 0.5084\n",
      "Epoch 187/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4899 - soft_acc: 0.6682 - val_loss: 1.4826 - val_soft_acc: 0.5285\n",
      "Epoch 188/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5155 - soft_acc: 0.6624 - val_loss: 1.5445 - val_soft_acc: 0.4916\n",
      "Epoch 189/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6550 - soft_acc: 0.6138 - val_loss: 1.6168 - val_soft_acc: 0.5154\n",
      "Epoch 190/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4444 - soft_acc: 0.6783 - val_loss: 1.3951 - val_soft_acc: 0.5502\n",
      "Epoch 191/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3710 - soft_acc: 0.7233 - val_loss: 1.3386 - val_soft_acc: 0.5613\n",
      "Epoch 192/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4100 - soft_acc: 0.7129 - val_loss: 1.3965 - val_soft_acc: 0.5475\n",
      "Epoch 193/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4426 - soft_acc: 0.6995 - val_loss: 1.4979 - val_soft_acc: 0.5205\n",
      "Epoch 194/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4598 - soft_acc: 0.6764 - val_loss: 1.4788 - val_soft_acc: 0.5299\n",
      "Epoch 195/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3737 - soft_acc: 0.7155 - val_loss: 1.4797 - val_soft_acc: 0.5432\n",
      "Epoch 196/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3690 - soft_acc: 0.7324 - val_loss: 1.4310 - val_soft_acc: 0.5170\n",
      "Epoch 197/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4437 - soft_acc: 0.6917 - val_loss: 1.6325 - val_soft_acc: 0.5111\n",
      "Epoch 198/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4996 - soft_acc: 0.6714 - val_loss: 1.4641 - val_soft_acc: 0.5197\n",
      "Epoch 199/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4545 - soft_acc: 0.6795 - val_loss: 1.5835 - val_soft_acc: 0.4904\n",
      "Epoch 200/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4551 - soft_acc: 0.6839 - val_loss: 1.3969 - val_soft_acc: 0.5234\n",
      "Epoch 201/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4030 - soft_acc: 0.7060 - val_loss: 1.5493 - val_soft_acc: 0.4957\n",
      "Epoch 202/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7275 - soft_acc: 0.5980 - val_loss: 1.7337 - val_soft_acc: 0.4498\n",
      "Epoch 203/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5310 - soft_acc: 0.6411 - val_loss: 1.3689 - val_soft_acc: 0.5375\n",
      "Epoch 204/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3036 - soft_acc: 0.7474 - val_loss: 1.3426 - val_soft_acc: 0.5758\n",
      "Epoch 205/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3098 - soft_acc: 0.7548 - val_loss: 1.2765 - val_soft_acc: 0.5713\n",
      "Epoch 206/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3243 - soft_acc: 0.7595 - val_loss: 1.4336 - val_soft_acc: 0.5383\n",
      "Epoch 207/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6469 - soft_acc: 0.6298 - val_loss: 1.5702 - val_soft_acc: 0.5061\n",
      "Epoch 208/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4062 - soft_acc: 0.6945 - val_loss: 1.3166 - val_soft_acc: 0.5625\n",
      "Epoch 209/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3538 - soft_acc: 0.7332 - val_loss: 1.3345 - val_soft_acc: 0.5596\n",
      "Epoch 210/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3295 - soft_acc: 0.7466 - val_loss: 1.4726 - val_soft_acc: 0.5350\n",
      "Epoch 211/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3203 - soft_acc: 0.7479 - val_loss: 1.3941 - val_soft_acc: 0.5447\n",
      "Epoch 212/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3841 - soft_acc: 0.7212 - val_loss: 1.4248 - val_soft_acc: 0.5484\n",
      "Epoch 213/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4166 - soft_acc: 0.7020 - val_loss: 1.5111 - val_soft_acc: 0.5457\n",
      "Epoch 214/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3597 - soft_acc: 0.7246 - val_loss: 1.4790 - val_soft_acc: 0.5549\n",
      "Epoch 215/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3922 - soft_acc: 0.7195 - val_loss: 1.4773 - val_soft_acc: 0.5422\n",
      "Epoch 216/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3847 - soft_acc: 0.7173 - val_loss: 1.3748 - val_soft_acc: 0.5684\n",
      "Epoch 217/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5032 - soft_acc: 0.6829 - val_loss: 1.6134 - val_soft_acc: 0.5111\n",
      "Epoch 218/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4896 - soft_acc: 0.6648 - val_loss: 1.4346 - val_soft_acc: 0.5453\n",
      "Epoch 219/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3917 - soft_acc: 0.7097 - val_loss: 1.5412 - val_soft_acc: 0.5320\n",
      "Epoch 220/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.3769 - soft_acc: 0.7135 - val_loss: 1.3997 - val_soft_acc: 0.5518\n",
      "Epoch 221/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3409 - soft_acc: 0.7434 - val_loss: 1.5778 - val_soft_acc: 0.5285\n",
      "Epoch 222/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6968 - soft_acc: 0.6158 - val_loss: 1.4259 - val_soft_acc: 0.5377\n",
      "Epoch 223/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3055 - soft_acc: 0.7458 - val_loss: 1.2701 - val_soft_acc: 0.5895\n",
      "Epoch 224/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.2470 - soft_acc: 0.7978 - val_loss: 1.2521 - val_soft_acc: 0.5949\n",
      "Epoch 225/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2078 - soft_acc: 0.8156 - val_loss: 1.2762 - val_soft_acc: 0.6074\n",
      "Epoch 226/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2109 - soft_acc: 0.8184 - val_loss: 1.2940 - val_soft_acc: 0.5932\n",
      "Epoch 227/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2221 - soft_acc: 0.8094 - val_loss: 1.2831 - val_soft_acc: 0.6039\n",
      "Epoch 228/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.2531 - soft_acc: 0.7902 - val_loss: 1.3078 - val_soft_acc: 0.5814\n",
      "Epoch 229/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4754 - soft_acc: 0.6890 - val_loss: 1.8396 - val_soft_acc: 0.4779\n",
      "Epoch 230/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7011 - soft_acc: 0.5981 - val_loss: 1.4735 - val_soft_acc: 0.5441\n",
      "Epoch 231/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3308 - soft_acc: 0.7242 - val_loss: 1.3496 - val_soft_acc: 0.5760\n",
      "Epoch 232/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2408 - soft_acc: 0.7870 - val_loss: 1.2224 - val_soft_acc: 0.6014\n",
      "Epoch 233/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1846 - soft_acc: 0.8327 - val_loss: 1.2150 - val_soft_acc: 0.6102\n",
      "Epoch 234/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1998 - soft_acc: 0.8158 - val_loss: 1.2976 - val_soft_acc: 0.5826\n",
      "Epoch 235/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2671 - soft_acc: 0.7828 - val_loss: 1.4560 - val_soft_acc: 0.5723\n",
      "Epoch 236/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3130 - soft_acc: 0.7589 - val_loss: 1.4706 - val_soft_acc: 0.5648\n",
      "Epoch 237/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5513 - soft_acc: 0.6629 - val_loss: 1.5749 - val_soft_acc: 0.5203\n",
      "Epoch 238/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4021 - soft_acc: 0.7019 - val_loss: 1.5278 - val_soft_acc: 0.5289\n",
      "Epoch 239/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2851 - soft_acc: 0.7586 - val_loss: 1.3002 - val_soft_acc: 0.5840\n",
      "Epoch 240/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4410 - soft_acc: 0.7118 - val_loss: 1.5464 - val_soft_acc: 0.5416\n",
      "Epoch 241/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2882 - soft_acc: 0.7639 - val_loss: 1.2590 - val_soft_acc: 0.6107\n",
      "Epoch 242/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2349 - soft_acc: 0.8005 - val_loss: 1.1831 - val_soft_acc: 0.6127\n",
      "Epoch 243/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1511 - soft_acc: 0.8490 - val_loss: 1.2427 - val_soft_acc: 0.6139\n",
      "Epoch 244/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1784 - soft_acc: 0.8381 - val_loss: 1.2683 - val_soft_acc: 0.6008\n",
      "Epoch 245/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3378 - soft_acc: 0.7466 - val_loss: 1.4267 - val_soft_acc: 0.5484\n",
      "Epoch 246/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6126 - soft_acc: 0.6358 - val_loss: 1.6048 - val_soft_acc: 0.5098\n",
      "Epoch 247/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4892 - soft_acc: 0.6731 - val_loss: 1.3190 - val_soft_acc: 0.5566\n",
      "Epoch 248/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2874 - soft_acc: 0.7575 - val_loss: 1.2507 - val_soft_acc: 0.5953\n",
      "Epoch 249/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1793 - soft_acc: 0.8303 - val_loss: 1.1436 - val_soft_acc: 0.6273\n",
      "Epoch 250/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1480 - soft_acc: 0.8574 - val_loss: 1.1572 - val_soft_acc: 0.6266\n",
      "Epoch 251/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1194 - soft_acc: 0.8751 - val_loss: 1.2111 - val_soft_acc: 0.6145\n",
      "Epoch 252/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1760 - soft_acc: 0.8440 - val_loss: 1.4169 - val_soft_acc: 0.5705\n",
      "Epoch 253/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3921 - soft_acc: 0.7224 - val_loss: 1.6305 - val_soft_acc: 0.5002\n",
      "Epoch 254/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7111 - soft_acc: 0.6039 - val_loss: 1.4198 - val_soft_acc: 0.5395\n",
      "Epoch 255/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3528 - soft_acc: 0.7284 - val_loss: 1.5223 - val_soft_acc: 0.5254\n",
      "Epoch 256/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4199 - soft_acc: 0.6989 - val_loss: 1.3700 - val_soft_acc: 0.5773\n",
      "Epoch 257/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2042 - soft_acc: 0.8085 - val_loss: 1.2587 - val_soft_acc: 0.6115\n",
      "Epoch 258/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1319 - soft_acc: 0.8609 - val_loss: 1.2238 - val_soft_acc: 0.6322\n",
      "Epoch 259/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1282 - soft_acc: 0.8778 - val_loss: 1.1205 - val_soft_acc: 0.6416\n",
      "Epoch 260/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1554 - soft_acc: 0.8566 - val_loss: 1.2257 - val_soft_acc: 0.6209\n",
      "Epoch 261/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1715 - soft_acc: 0.8538 - val_loss: 1.2834 - val_soft_acc: 0.5871\n",
      "Epoch 262/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.3808 - soft_acc: 0.72 - 1s 75us/step - loss: 0.3902 - soft_acc: 0.7203 - val_loss: 1.6993 - val_soft_acc: 0.5076\n",
      "Epoch 263/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.6082 - soft_acc: 0.6260 - val_loss: 1.5036 - val_soft_acc: 0.5416\n",
      "Epoch 264/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4148 - soft_acc: 0.7031 - val_loss: 1.3880 - val_soft_acc: 0.5586\n",
      "Epoch 265/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3523 - soft_acc: 0.7370 - val_loss: 1.3429 - val_soft_acc: 0.5779\n",
      "Epoch 266/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1791 - soft_acc: 0.8245 - val_loss: 1.2295 - val_soft_acc: 0.6311\n",
      "Epoch 267/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1602 - soft_acc: 0.8524 - val_loss: 1.1593 - val_soft_acc: 0.6391\n",
      "Epoch 268/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1093 - soft_acc: 0.8789 - val_loss: 1.1295 - val_soft_acc: 0.6574\n",
      "Epoch 269/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1263 - soft_acc: 0.8840 - val_loss: 1.3070 - val_soft_acc: 0.6066\n",
      "Epoch 270/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2890 - soft_acc: 0.7711 - val_loss: 1.6231 - val_soft_acc: 0.5496\n",
      "Epoch 271/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.4878 - soft_acc: 0.6942 - val_loss: 1.4894 - val_soft_acc: 0.5342\n",
      "Epoch 272/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.3666 - soft_acc: 0.7301 - val_loss: 1.3318 - val_soft_acc: 0.5811\n",
      "Epoch 273/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.2254 - soft_acc: 0.7894 - val_loss: 1.2090 - val_soft_acc: 0.6145\n",
      "Epoch 274/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1822 - soft_acc: 0.8287 - val_loss: 1.1778 - val_soft_acc: 0.6357\n",
      "Epoch 275/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1212 - soft_acc: 0.8762 - val_loss: 1.1489 - val_soft_acc: 0.6379\n",
      "Epoch 276/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0972 - soft_acc: 0.8908 - val_loss: 1.1252 - val_soft_acc: 0.6537\n",
      "Epoch 277/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0811 - soft_acc: 0.9060 - val_loss: 1.0776 - val_soft_acc: 0.6699\n",
      "Epoch 278/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1283 - soft_acc: 0.8778 - val_loss: 1.4117 - val_soft_acc: 0.5865\n",
      "Epoch 279/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.9596 - soft_acc: 0.5588 - val_loss: 1.7909 - val_soft_acc: 0.4887\n",
      "Epoch 280/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.3970 - soft_acc: 0.6999 - val_loss: 1.4001 - val_soft_acc: 0.5631\n",
      "Epoch 281/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.2010 - soft_acc: 0.8032 - val_loss: 1.2241 - val_soft_acc: 0.6102\n",
      "Epoch 282/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.1539 - soft_acc: 0.83 - 1s 73us/step - loss: 0.1536 - soft_acc: 0.8399 - val_loss: 1.2099 - val_soft_acc: 0.6178\n",
      "Epoch 283/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1102 - soft_acc: 0.8765 - val_loss: 1.1371 - val_soft_acc: 0.6451\n",
      "Epoch 284/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0955 - soft_acc: 0.9006 - val_loss: 1.1470 - val_soft_acc: 0.6482\n",
      "Epoch 285/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0839 - soft_acc: 0.9100 - val_loss: 1.0848 - val_soft_acc: 0.6625\n",
      "Epoch 286/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1085 - soft_acc: 0.8893 - val_loss: 1.1150 - val_soft_acc: 0.6590\n",
      "Epoch 287/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1387 - soft_acc: 0.8745 - val_loss: 1.2142 - val_soft_acc: 0.6219\n",
      "Epoch 288/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3058 - soft_acc: 0.7829 - val_loss: 1.8160 - val_soft_acc: 0.4664\n",
      "Epoch 289/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8581 - soft_acc: 0.5660 - val_loss: 1.6975 - val_soft_acc: 0.4936\n",
      "Epoch 290/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.3364 - soft_acc: 0.7223 - val_loss: 1.3081 - val_soft_acc: 0.6131\n",
      "Epoch 291/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1449 - soft_acc: 0.8429 - val_loss: 1.1994 - val_soft_acc: 0.6352\n",
      "Epoch 292/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1189 - soft_acc: 0.8739 - val_loss: 1.1495 - val_soft_acc: 0.6578\n",
      "Epoch 293/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0916 - soft_acc: 0.8930 - val_loss: 1.1493 - val_soft_acc: 0.6486\n",
      "Epoch 294/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1007 - soft_acc: 0.8958 - val_loss: 1.1839 - val_soft_acc: 0.6410\n",
      "Epoch 295/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1180 - soft_acc: 0.8841 - val_loss: 1.1654 - val_soft_acc: 0.6328\n",
      "Epoch 296/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1628 - soft_acc: 0.8567 - val_loss: 1.3409 - val_soft_acc: 0.5875\n",
      "Epoch 297/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6832 - soft_acc: 0.6336 - val_loss: 1.7573 - val_soft_acc: 0.4793\n",
      "Epoch 298/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4773 - soft_acc: 0.6720 - val_loss: 1.3209 - val_soft_acc: 0.5836\n",
      "Epoch 299/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1888 - soft_acc: 0.8197 - val_loss: 1.1491 - val_soft_acc: 0.6324\n",
      "Epoch 300/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0964 - soft_acc: 0.8853 - val_loss: 1.0701 - val_soft_acc: 0.6625\n",
      "Epoch 301/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0696 - soft_acc: 0.9162 - val_loss: 1.0767 - val_soft_acc: 0.6545\n",
      "Epoch 302/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0523 - soft_acc: 0.9311 - val_loss: 1.0590 - val_soft_acc: 0.6697\n",
      "Epoch 303/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0563 - soft_acc: 0.9302 - val_loss: 1.0970 - val_soft_acc: 0.6652\n",
      "Epoch 304/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0653 - soft_acc: 0.9246 - val_loss: 1.1424 - val_soft_acc: 0.6441\n",
      "Epoch 305/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.2604 - soft_acc: 0.8063 - val_loss: 1.6472 - val_soft_acc: 0.5057\n",
      "Epoch 306/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.3067 - soft_acc: 0.4767 - val_loss: 1.6184 - val_soft_acc: 0.5002\n",
      "Epoch 307/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.3732 - soft_acc: 0.6997 - val_loss: 1.3254 - val_soft_acc: 0.5713\n",
      "Epoch 308/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1439 - soft_acc: 0.8342 - val_loss: 1.1756 - val_soft_acc: 0.6283\n",
      "Epoch 309/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0800 - soft_acc: 0.8953 - val_loss: 1.1627 - val_soft_acc: 0.6408\n",
      "Epoch 310/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0578 - soft_acc: 0.9211 - val_loss: 1.1413 - val_soft_acc: 0.6670\n",
      "Epoch 311/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0499 - soft_acc: 0.9311 - val_loss: 1.1177 - val_soft_acc: 0.6605\n",
      "Epoch 312/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1194 - soft_acc: 0.8886 - val_loss: 1.3042 - val_soft_acc: 0.6092\n",
      "Epoch 313/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2985 - soft_acc: 0.7715 - val_loss: 1.5852 - val_soft_acc: 0.5584\n",
      "Epoch 314/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.3090 - soft_acc: 0.7576 - val_loss: 1.2437 - val_soft_acc: 0.6000\n",
      "Epoch 315/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1515 - soft_acc: 0.8415 - val_loss: 1.1533 - val_soft_acc: 0.6441\n",
      "Epoch 316/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0988 - soft_acc: 0.8837 - val_loss: 1.2364 - val_soft_acc: 0.6283\n",
      "Epoch 317/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1769 - soft_acc: 0.8514 - val_loss: 1.2342 - val_soft_acc: 0.6285\n",
      "Epoch 318/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1200 - soft_acc: 0.8691 - val_loss: 1.1631 - val_soft_acc: 0.6299\n",
      "Epoch 319/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.2533 - soft_acc: 0.8074 - val_loss: 1.6645 - val_soft_acc: 0.5176\n",
      "Epoch 320/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4710 - soft_acc: 0.6776 - val_loss: 1.5293 - val_soft_acc: 0.5605\n",
      "Epoch 321/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4089 - soft_acc: 0.7137 - val_loss: 1.3034 - val_soft_acc: 0.6031\n",
      "Epoch 322/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1804 - soft_acc: 0.8267 - val_loss: 1.1941 - val_soft_acc: 0.6223\n",
      "Epoch 323/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1804 - soft_acc: 0.8357 - val_loss: 1.2003 - val_soft_acc: 0.6377\n",
      "Epoch 324/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1104 - soft_acc: 0.8830 - val_loss: 1.0635 - val_soft_acc: 0.6611\n",
      "Epoch 325/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0604 - soft_acc: 0.9209 - val_loss: 1.0609 - val_soft_acc: 0.6633\n",
      "Epoch 326/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0569 - soft_acc: 0.9326 - val_loss: 1.0762 - val_soft_acc: 0.6664\n",
      "Epoch 327/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0616 - soft_acc: 0.9258 - val_loss: 1.1456 - val_soft_acc: 0.6545\n",
      "Epoch 328/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1426 - soft_acc: 0.8714 - val_loss: 1.5428 - val_soft_acc: 0.5475\n",
      "Epoch 329/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.5459 - soft_acc: 0.6570 - val_loss: 1.4996 - val_soft_acc: 0.5486\n",
      "Epoch 330/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.2969 - soft_acc: 0.7478 - val_loss: 1.3276 - val_soft_acc: 0.5984\n",
      "Epoch 331/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1189 - soft_acc: 0.8555 - val_loss: 1.1459 - val_soft_acc: 0.6553\n",
      "Epoch 332/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1159 - soft_acc: 0.8728 - val_loss: 1.1929 - val_soft_acc: 0.6281\n",
      "Epoch 333/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1124 - soft_acc: 0.8853 - val_loss: 1.2553 - val_soft_acc: 0.6307\n",
      "Epoch 334/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1161 - soft_acc: 0.8796 - val_loss: 1.1079 - val_soft_acc: 0.6621\n",
      "Epoch 335/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.2056 - soft_acc: 0.8469 - val_loss: 1.5957 - val_soft_acc: 0.5494\n",
      "Epoch 336/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6158 - soft_acc: 0.6476 - val_loss: 1.7236 - val_soft_acc: 0.5254\n",
      "Epoch 337/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.3479 - soft_acc: 0.7316 - val_loss: 1.2137 - val_soft_acc: 0.6137\n",
      "Epoch 338/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1225 - soft_acc: 0.8598 - val_loss: 1.0813 - val_soft_acc: 0.6650\n",
      "Epoch 339/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0471 - soft_acc: 0.9257 - val_loss: 1.0454 - val_soft_acc: 0.6811\n",
      "Epoch 340/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0252 - soft_acc: 0.9542 - val_loss: 1.0379 - val_soft_acc: 0.6881\n",
      "Epoch 341/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0187 - soft_acc: 0.9629 - val_loss: 1.0253 - val_soft_acc: 0.6920\n",
      "Epoch 342/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0146 - soft_acc: 0.9674 - val_loss: 1.0185 - val_soft_acc: 0.6961\n",
      "Epoch 343/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0157 - soft_acc: 0.9706 - val_loss: 1.0434 - val_soft_acc: 0.6930\n",
      "Epoch 344/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0647 - soft_acc: 0.9307 - val_loss: 1.1158 - val_soft_acc: 0.6506\n",
      "Epoch 345/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.4585 - soft_acc: 0.7236 - val_loss: 1.8283 - val_soft_acc: 0.4660\n",
      "Epoch 346/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.7132 - soft_acc: 0.6051 - val_loss: 1.5007 - val_soft_acc: 0.5408\n",
      "Epoch 347/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3017 - soft_acc: 0.7463 - val_loss: 1.2819 - val_soft_acc: 0.5967\n",
      "Epoch 348/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1637 - soft_acc: 0.8380 - val_loss: 1.1185 - val_soft_acc: 0.6457\n",
      "Epoch 349/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0687 - soft_acc: 0.9085 - val_loss: 1.0445 - val_soft_acc: 0.6748\n",
      "Epoch 350/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0339 - soft_acc: 0.9420 - val_loss: 1.0105 - val_soft_acc: 0.6828\n",
      "Epoch 351/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0203 - soft_acc: 0.9602 - val_loss: 1.0328 - val_soft_acc: 0.6910\n",
      "Epoch 352/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0133 - soft_acc: 0.9697 - val_loss: 1.0109 - val_soft_acc: 0.6920\n",
      "Epoch 353/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0108 - soft_acc: 0.9733 - val_loss: 1.0170 - val_soft_acc: 0.6914\n",
      "Epoch 354/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0148 - soft_acc: 0.9709 - val_loss: 1.0375 - val_soft_acc: 0.6945\n",
      "Epoch 355/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1810 - soft_acc: 0.8788 - val_loss: 1.6096 - val_soft_acc: 0.5020\n",
      "Epoch 356/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.9104 - soft_acc: 0.5624 - val_loss: 1.5875 - val_soft_acc: 0.5111\n",
      "Epoch 357/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3483 - soft_acc: 0.7194 - val_loss: 1.3402 - val_soft_acc: 0.5727\n",
      "Epoch 358/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1801 - soft_acc: 0.8218 - val_loss: 1.1517 - val_soft_acc: 0.6402\n",
      "Epoch 359/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0903 - soft_acc: 0.8951 - val_loss: 1.1441 - val_soft_acc: 0.6490\n",
      "Epoch 360/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0522 - soft_acc: 0.9239 - val_loss: 1.0844 - val_soft_acc: 0.6754\n",
      "Epoch 361/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0248 - soft_acc: 0.9539 - val_loss: 1.0624 - val_soft_acc: 0.6906\n",
      "Epoch 362/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0293 - soft_acc: 0.9565 - val_loss: 1.0622 - val_soft_acc: 0.6889\n",
      "Epoch 363/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1035 - soft_acc: 0.9139 - val_loss: 1.4200 - val_soft_acc: 0.5863\n",
      "Epoch 364/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5951 - soft_acc: 0.6630 - val_loss: 1.7825 - val_soft_acc: 0.4977\n",
      "Epoch 365/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.3629 - soft_acc: 0.7287 - val_loss: 1.2720 - val_soft_acc: 0.6246\n",
      "Epoch 366/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1262 - soft_acc: 0.8595 - val_loss: 1.1684 - val_soft_acc: 0.6504\n",
      "Epoch 367/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0927 - soft_acc: 0.8991 - val_loss: 1.1491 - val_soft_acc: 0.6422\n",
      "Epoch 368/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0742 - soft_acc: 0.9134 - val_loss: 1.1026 - val_soft_acc: 0.6701\n",
      "Epoch 369/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0694 - soft_acc: 0.9194 - val_loss: 1.0716 - val_soft_acc: 0.6838\n",
      "Epoch 370/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0366 - soft_acc: 0.9473 - val_loss: 1.0588 - val_soft_acc: 0.6863\n",
      "Epoch 371/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0203 - soft_acc: 0.9609 - val_loss: 1.0251 - val_soft_acc: 0.6973\n",
      "Epoch 372/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0159 - soft_acc: 0.9680 - val_loss: 1.0349 - val_soft_acc: 0.6939\n",
      "Epoch 373/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0840 - soft_acc: 0.9219 - val_loss: 1.2242 - val_soft_acc: 0.6387\n",
      "Epoch 374/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.7660 - soft_acc: 0.6254 - val_loss: 1.8042 - val_soft_acc: 0.4777\n",
      "Epoch 375/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4939 - soft_acc: 0.6778 - val_loss: 1.3265 - val_soft_acc: 0.5855\n",
      "Epoch 376/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1790 - soft_acc: 0.8233 - val_loss: 1.1237 - val_soft_acc: 0.6498\n",
      "Epoch 377/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0712 - soft_acc: 0.9039 - val_loss: 1.1164 - val_soft_acc: 0.6643\n",
      "Epoch 378/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0616 - soft_acc: 0.9208 - val_loss: 1.1264 - val_soft_acc: 0.6582\n",
      "Epoch 379/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0787 - soft_acc: 0.9118 - val_loss: 1.1964 - val_soft_acc: 0.6412\n",
      "Epoch 380/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1373 - soft_acc: 0.8768 - val_loss: 1.2455 - val_soft_acc: 0.6307\n",
      "Epoch 381/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.3925 - soft_acc: 0.7238 - val_loss: 1.3226 - val_soft_acc: 0.6041\n",
      "Epoch 382/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1598 - soft_acc: 0.8305 - val_loss: 1.2174 - val_soft_acc: 0.6301\n",
      "Epoch 383/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1250 - soft_acc: 0.8765 - val_loss: 1.2619 - val_soft_acc: 0.6260\n",
      "Epoch 384/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1229 - soft_acc: 0.8691 - val_loss: 1.3286 - val_soft_acc: 0.6041\n",
      "Epoch 385/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.2955 - soft_acc: 0.7772 - val_loss: 1.3015 - val_soft_acc: 0.5951\n",
      "Epoch 386/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1719 - soft_acc: 0.8323 - val_loss: 1.1381 - val_soft_acc: 0.6576\n",
      "Epoch 387/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0684 - soft_acc: 0.9093 - val_loss: 1.1098 - val_soft_acc: 0.6787\n",
      "Epoch 388/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0333 - soft_acc: 0.9434 - val_loss: 1.0679 - val_soft_acc: 0.6988\n",
      "Epoch 389/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0176 - soft_acc: 0.9636 - val_loss: 1.0599 - val_soft_acc: 0.6975\n",
      "Epoch 390/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0228 - soft_acc: 0.9634 - val_loss: 1.0871 - val_soft_acc: 0.6900\n",
      "Epoch 391/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0153 - soft_acc: 0.9680 - val_loss: 1.0669 - val_soft_acc: 0.7008\n",
      "Epoch 392/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0113 - soft_acc: 0.9738 - val_loss: 1.0527 - val_soft_acc: 0.6955\n",
      "Epoch 393/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0097 - soft_acc: 0.9757 - val_loss: 1.0673 - val_soft_acc: 0.6980\n",
      "Epoch 394/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.8115 - soft_acc: 0.6241 - val_loss: 1.7753 - val_soft_acc: 0.4689\n",
      "Epoch 395/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5456 - soft_acc: 0.6551 - val_loss: 1.4125 - val_soft_acc: 0.5660\n",
      "Epoch 396/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2144 - soft_acc: 0.8014 - val_loss: 1.1185 - val_soft_acc: 0.6410\n",
      "Epoch 397/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.0554 - soft_acc: 0.9131 - val_loss: 1.0519 - val_soft_acc: 0.6768\n",
      "Epoch 398/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.0272 - soft_acc: 0.9503 - val_loss: 1.0215 - val_soft_acc: 0.6857\n",
      "Epoch 399/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0150 - soft_acc: 0.9653 - val_loss: 1.0213 - val_soft_acc: 0.6873\n",
      "Epoch 400/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0093 - soft_acc: 0.9742 - val_loss: 1.0160 - val_soft_acc: 0.6926\n",
      "Epoch 00400: early stopping\n",
      ">0.693\n",
      "Train on 14640 samples, validate on 2440 samples\n",
      "Epoch 1/5000\n",
      "14640/14640 [==============================] - 1s 102us/step - loss: 4.8135 - soft_acc: 0.1542 - val_loss: 4.1948 - val_soft_acc: 0.1615\n",
      "Epoch 2/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 4.2135 - soft_acc: 0.1616 - val_loss: 4.0360 - val_soft_acc: 0.1717\n",
      "Epoch 3/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 4.1118 - soft_acc: 0.1634 - val_loss: 4.0315 - val_soft_acc: 0.1775\n",
      "Epoch 4/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 4.0544 - soft_acc: 0.1670 - val_loss: 3.8526 - val_soft_acc: 0.1643\n",
      "Epoch 5/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.9748 - soft_acc: 0.1698 - val_loss: 4.3229 - val_soft_acc: 0.1520\n",
      "Epoch 6/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.9340 - soft_acc: 0.1753 - val_loss: 3.9432 - val_soft_acc: 0.1797\n",
      "Epoch 7/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.8605 - soft_acc: 0.1737 - val_loss: 3.8312 - val_soft_acc: 0.1920\n",
      "Epoch 8/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 3.8203 - soft_acc: 0.1783 - val_loss: 3.7059 - val_soft_acc: 0.1869\n",
      "Epoch 9/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 3.7914 - soft_acc: 0.1789 - val_loss: 3.8455 - val_soft_acc: 0.1756\n",
      "Epoch 10/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.8169 - soft_acc: 0.1805 - val_loss: 3.8044 - val_soft_acc: 0.1748\n",
      "Epoch 11/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 3.8030 - soft_acc: 0.1801 - val_loss: 3.6600 - val_soft_acc: 0.1877\n",
      "Epoch 12/5000\n",
      "14640/14640 [==============================] - 1s 80us/step - loss: 3.7525 - soft_acc: 0.1833 - val_loss: 3.6598 - val_soft_acc: 0.1809\n",
      "Epoch 13/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.7321 - soft_acc: 0.1849 - val_loss: 3.6628 - val_soft_acc: 0.2008\n",
      "Epoch 14/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.7158 - soft_acc: 0.1818 - val_loss: 3.5859 - val_soft_acc: 0.1842\n",
      "Epoch 15/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.6877 - soft_acc: 0.1839 - val_loss: 3.6020 - val_soft_acc: 0.1879\n",
      "Epoch 16/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 3.7075 - soft_acc: 0.1830 - val_loss: 3.6037 - val_soft_acc: 0.1803\n",
      "Epoch 17/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 3.6183 - soft_acc: 0.1856 - val_loss: 3.5752 - val_soft_acc: 0.1994\n",
      "Epoch 18/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 3.6146 - soft_acc: 0.1857 - val_loss: 3.5069 - val_soft_acc: 0.1975\n",
      "Epoch 19/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 3.5737 - soft_acc: 0.1896 - val_loss: 3.5933 - val_soft_acc: 0.1844\n",
      "Epoch 20/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.5675 - soft_acc: 0.1908 - val_loss: 3.7082 - val_soft_acc: 0.2031\n",
      "Epoch 21/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 3.5598 - soft_acc: 0.1912 - val_loss: 3.5390 - val_soft_acc: 0.2010\n",
      "Epoch 22/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.5219 - soft_acc: 0.1941 - val_loss: 3.4415 - val_soft_acc: 0.1973\n",
      "Epoch 23/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.5571 - soft_acc: 0.1884 - val_loss: 3.4559 - val_soft_acc: 0.1830\n",
      "Epoch 24/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.4928 - soft_acc: 0.1933 - val_loss: 3.4639 - val_soft_acc: 0.1930\n",
      "Epoch 25/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 3.4489 - soft_acc: 0.1955 - val_loss: 3.4397 - val_soft_acc: 0.2037\n",
      "Epoch 26/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.4427 - soft_acc: 0.1975 - val_loss: 3.4250 - val_soft_acc: 0.1918\n",
      "Epoch 27/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.4111 - soft_acc: 0.2001 - val_loss: 3.3751 - val_soft_acc: 0.1879\n",
      "Epoch 28/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.4105 - soft_acc: 0.1986 - val_loss: 3.3663 - val_soft_acc: 0.2016\n",
      "Epoch 29/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.3613 - soft_acc: 0.2061 - val_loss: 3.3617 - val_soft_acc: 0.2148\n",
      "Epoch 30/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.3756 - soft_acc: 0.2019 - val_loss: 3.3352 - val_soft_acc: 0.2129\n",
      "Epoch 31/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 3.3472 - soft_acc: 0.2055 - val_loss: 3.3120 - val_soft_acc: 0.2000\n",
      "Epoch 32/5000\n",
      "14640/14640 [==============================] - 1s 72us/step - loss: 3.3106 - soft_acc: 0.2063 - val_loss: 3.2219 - val_soft_acc: 0.2090\n",
      "Epoch 33/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 73us/step - loss: 3.3171 - soft_acc: 0.2089 - val_loss: 3.1978 - val_soft_acc: 0.2246\n",
      "Epoch 34/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.2658 - soft_acc: 0.2110 - val_loss: 3.2463 - val_soft_acc: 0.2076\n",
      "Epoch 35/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 3.2528 - soft_acc: 0.2166 - val_loss: 3.2724 - val_soft_acc: 0.1984\n",
      "Epoch 36/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.2603 - soft_acc: 0.2124 - val_loss: 3.2336 - val_soft_acc: 0.2268\n",
      "Epoch 37/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.2369 - soft_acc: 0.2142 - val_loss: 3.1704 - val_soft_acc: 0.2191\n",
      "Epoch 38/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.2072 - soft_acc: 0.2155 - val_loss: 3.1511 - val_soft_acc: 0.2215\n",
      "Epoch 39/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.1955 - soft_acc: 0.2144 - val_loss: 3.1503 - val_soft_acc: 0.2320\n",
      "Epoch 40/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.1675 - soft_acc: 0.2211 - val_loss: 3.0613 - val_soft_acc: 0.2285\n",
      "Epoch 41/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.1371 - soft_acc: 0.2222 - val_loss: 3.0901 - val_soft_acc: 0.2236\n",
      "Epoch 42/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.1395 - soft_acc: 0.2198 - val_loss: 3.1149 - val_soft_acc: 0.2258\n",
      "Epoch 43/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.1123 - soft_acc: 0.2260 - val_loss: 3.1895 - val_soft_acc: 0.2135\n",
      "Epoch 44/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.0957 - soft_acc: 0.2272 - val_loss: 3.0736 - val_soft_acc: 0.2277\n",
      "Epoch 45/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.0765 - soft_acc: 0.2258 - val_loss: 3.0284 - val_soft_acc: 0.2307\n",
      "Epoch 46/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.0667 - soft_acc: 0.2269 - val_loss: 3.0278 - val_soft_acc: 0.2307\n",
      "Epoch 47/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.0539 - soft_acc: 0.2296 - val_loss: 3.1587 - val_soft_acc: 0.2162\n",
      "Epoch 48/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.0123 - soft_acc: 0.2309 - val_loss: 3.1094 - val_soft_acc: 0.2250\n",
      "Epoch 49/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.0436 - soft_acc: 0.2270 - val_loss: 3.0128 - val_soft_acc: 0.2346\n",
      "Epoch 50/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.9882 - soft_acc: 0.2321 - val_loss: 3.0107 - val_soft_acc: 0.2346\n",
      "Epoch 51/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.9625 - soft_acc: 0.2365 - val_loss: 3.0517 - val_soft_acc: 0.2227\n",
      "Epoch 52/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.9425 - soft_acc: 0.2356 - val_loss: 2.9819 - val_soft_acc: 0.2369\n",
      "Epoch 53/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.9443 - soft_acc: 0.2380 - val_loss: 3.0024 - val_soft_acc: 0.2230\n",
      "Epoch 54/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.9082 - soft_acc: 0.2372 - val_loss: 2.8949 - val_soft_acc: 0.2436\n",
      "Epoch 55/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.8680 - soft_acc: 0.2458 - val_loss: 2.9064 - val_soft_acc: 0.2391\n",
      "Epoch 56/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.8525 - soft_acc: 0.2475 - val_loss: 2.9074 - val_soft_acc: 0.2527\n",
      "Epoch 57/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.8408 - soft_acc: 0.2466 - val_loss: 2.8497 - val_soft_acc: 0.2496\n",
      "Epoch 58/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.8085 - soft_acc: 0.2487 - val_loss: 2.8242 - val_soft_acc: 0.2549\n",
      "Epoch 59/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.8343 - soft_acc: 0.2465 - val_loss: 2.7989 - val_soft_acc: 0.2561\n",
      "Epoch 60/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.7631 - soft_acc: 0.2540 - val_loss: 2.9225 - val_soft_acc: 0.2428\n",
      "Epoch 61/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.7340 - soft_acc: 0.2544 - val_loss: 2.9181 - val_soft_acc: 0.2617\n",
      "Epoch 62/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.7715 - soft_acc: 0.2525 - val_loss: 2.7713 - val_soft_acc: 0.2648\n",
      "Epoch 63/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.6919 - soft_acc: 0.2613 - val_loss: 2.7787 - val_soft_acc: 0.2523\n",
      "Epoch 64/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 2.6562 - soft_acc: 0.26 - 1s 73us/step - loss: 2.6589 - soft_acc: 0.2620 - val_loss: 2.7521 - val_soft_acc: 0.2666\n",
      "Epoch 65/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.6411 - soft_acc: 0.2690 - val_loss: 2.7575 - val_soft_acc: 0.2748\n",
      "Epoch 66/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.6341 - soft_acc: 0.2666 - val_loss: 2.6834 - val_soft_acc: 0.2668\n",
      "Epoch 67/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.5979 - soft_acc: 0.2695 - val_loss: 2.5911 - val_soft_acc: 0.2695\n",
      "Epoch 68/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.5768 - soft_acc: 0.2692 - val_loss: 2.6735 - val_soft_acc: 0.2566\n",
      "Epoch 69/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.5480 - soft_acc: 0.2767 - val_loss: 2.6422 - val_soft_acc: 0.2803\n",
      "Epoch 70/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.5237 - soft_acc: 0.2792 - val_loss: 2.6680 - val_soft_acc: 0.2713\n",
      "Epoch 71/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.4715 - soft_acc: 0.2789 - val_loss: 2.7075 - val_soft_acc: 0.2660\n",
      "Epoch 72/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.4907 - soft_acc: 0.2801 - val_loss: 2.6830 - val_soft_acc: 0.2707\n",
      "Epoch 73/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.4580 - soft_acc: 0.2822 - val_loss: 2.6215 - val_soft_acc: 0.2855\n",
      "Epoch 74/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.4108 - soft_acc: 0.2925 - val_loss: 2.5896 - val_soft_acc: 0.2828\n",
      "Epoch 75/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.4047 - soft_acc: 0.2864 - val_loss: 2.6208 - val_soft_acc: 0.2756\n",
      "Epoch 76/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.3910 - soft_acc: 0.2895 - val_loss: 2.5509 - val_soft_acc: 0.2797\n",
      "Epoch 77/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.3214 - soft_acc: 0.2978 - val_loss: 2.4433 - val_soft_acc: 0.2984\n",
      "Epoch 78/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.3075 - soft_acc: 0.3021 - val_loss: 2.7034 - val_soft_acc: 0.2598\n",
      "Epoch 79/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.3087 - soft_acc: 0.3039 - val_loss: 2.6097 - val_soft_acc: 0.2801\n",
      "Epoch 80/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.2345 - soft_acc: 0.3109 - val_loss: 2.4429 - val_soft_acc: 0.2971\n",
      "Epoch 81/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.2267 - soft_acc: 0.3090 - val_loss: 2.5376 - val_soft_acc: 0.2867\n",
      "Epoch 82/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.1966 - soft_acc: 0.3141 - val_loss: 2.4314 - val_soft_acc: 0.2922\n",
      "Epoch 83/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.1281 - soft_acc: 0.3204 - val_loss: 2.3879 - val_soft_acc: 0.3123\n",
      "Epoch 84/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.1286 - soft_acc: 0.3232 - val_loss: 2.3109 - val_soft_acc: 0.3113\n",
      "Epoch 85/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.1253 - soft_acc: 0.3192 - val_loss: 2.3843 - val_soft_acc: 0.3000\n",
      "Epoch 86/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.0831 - soft_acc: 0.3258 - val_loss: 2.3348 - val_soft_acc: 0.3148\n",
      "Epoch 87/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 73us/step - loss: 2.0418 - soft_acc: 0.3317 - val_loss: 2.3627 - val_soft_acc: 0.3096\n",
      "Epoch 88/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.0419 - soft_acc: 0.3317 - val_loss: 2.2867 - val_soft_acc: 0.3088\n",
      "Epoch 89/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.0160 - soft_acc: 0.3351 - val_loss: 2.3155 - val_soft_acc: 0.3049\n",
      "Epoch 90/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.9567 - soft_acc: 0.3429 - val_loss: 2.3997 - val_soft_acc: 0.3199\n",
      "Epoch 91/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.9097 - soft_acc: 0.3475 - val_loss: 2.2633 - val_soft_acc: 0.3273\n",
      "Epoch 92/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.8790 - soft_acc: 0.3525 - val_loss: 2.2695 - val_soft_acc: 0.3445\n",
      "Epoch 93/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.8743 - soft_acc: 0.3525 - val_loss: 2.2547 - val_soft_acc: 0.3299\n",
      "Epoch 94/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.8101 - soft_acc: 0.3599 - val_loss: 2.0926 - val_soft_acc: 0.3494\n",
      "Epoch 95/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.7429 - soft_acc: 0.3674 - val_loss: 2.1370 - val_soft_acc: 0.3461\n",
      "Epoch 96/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.7517 - soft_acc: 0.3675 - val_loss: 2.1339 - val_soft_acc: 0.3387\n",
      "Epoch 97/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.7737 - soft_acc: 0.3672 - val_loss: 2.0712 - val_soft_acc: 0.3428\n",
      "Epoch 98/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.7175 - soft_acc: 0.3767 - val_loss: 2.0773 - val_soft_acc: 0.3467\n",
      "Epoch 99/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.7127 - soft_acc: 0.3751 - val_loss: 2.0663 - val_soft_acc: 0.3467\n",
      "Epoch 100/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.6617 - soft_acc: 0.3814 - val_loss: 1.9996 - val_soft_acc: 0.3535\n",
      "Epoch 101/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.6097 - soft_acc: 0.3893 - val_loss: 2.0276 - val_soft_acc: 0.3678\n",
      "Epoch 102/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.5777 - soft_acc: 0.4005 - val_loss: 2.1319 - val_soft_acc: 0.3506\n",
      "Epoch 103/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.5823 - soft_acc: 0.3946 - val_loss: 2.0127 - val_soft_acc: 0.3664\n",
      "Epoch 104/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.5899 - soft_acc: 0.3931 - val_loss: 1.9628 - val_soft_acc: 0.3602\n",
      "Epoch 105/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.5189 - soft_acc: 0.4045 - val_loss: 1.9688 - val_soft_acc: 0.3643\n",
      "Epoch 106/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.4885 - soft_acc: 0.4117 - val_loss: 1.9868 - val_soft_acc: 0.3586\n",
      "Epoch 107/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.4792 - soft_acc: 0.4119 - val_loss: 2.0403 - val_soft_acc: 0.3633\n",
      "Epoch 108/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.5395 - soft_acc: 0.4032 - val_loss: 1.9604 - val_soft_acc: 0.3750\n",
      "Epoch 109/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.4711 - soft_acc: 0.4137 - val_loss: 1.8950 - val_soft_acc: 0.3738\n",
      "Epoch 110/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.3687 - soft_acc: 0.4341 - val_loss: 1.9782 - val_soft_acc: 0.3764\n",
      "Epoch 111/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.3569 - soft_acc: 0.4324 - val_loss: 1.9271 - val_soft_acc: 0.3830\n",
      "Epoch 112/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 1.3189 - soft_acc: 0.4397 - val_loss: 1.9212 - val_soft_acc: 0.3840\n",
      "Epoch 113/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.3210 - soft_acc: 0.4386 - val_loss: 1.9188 - val_soft_acc: 0.3918\n",
      "Epoch 114/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.2695 - soft_acc: 0.4485 - val_loss: 1.8519 - val_soft_acc: 0.3799\n",
      "Epoch 115/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.2177 - soft_acc: 0.4587 - val_loss: 1.9395 - val_soft_acc: 0.3861\n",
      "Epoch 116/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.2781 - soft_acc: 0.4494 - val_loss: 1.8193 - val_soft_acc: 0.4131\n",
      "Epoch 117/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.2290 - soft_acc: 0.4585 - val_loss: 1.8471 - val_soft_acc: 0.4000\n",
      "Epoch 118/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.1851 - soft_acc: 0.4706 - val_loss: 1.9101 - val_soft_acc: 0.4064\n",
      "Epoch 119/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.2556 - soft_acc: 0.4550 - val_loss: 1.8201 - val_soft_acc: 0.4078\n",
      "Epoch 120/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.1620 - soft_acc: 0.4740 - val_loss: 1.8288 - val_soft_acc: 0.4174\n",
      "Epoch 121/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.1504 - soft_acc: 0.4758 - val_loss: 1.8351 - val_soft_acc: 0.4094\n",
      "Epoch 122/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.1694 - soft_acc: 0.4657 - val_loss: 1.7714 - val_soft_acc: 0.4256\n",
      "Epoch 123/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.1188 - soft_acc: 0.4819 - val_loss: 1.7616 - val_soft_acc: 0.4277\n",
      "Epoch 124/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.1050 - soft_acc: 0.4860 - val_loss: 1.8531 - val_soft_acc: 0.4197\n",
      "Epoch 125/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.0585 - soft_acc: 0.4965 - val_loss: 1.7109 - val_soft_acc: 0.4365\n",
      "Epoch 126/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.1274 - soft_acc: 0.4857 - val_loss: 1.8660 - val_soft_acc: 0.3916\n",
      "Epoch 127/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.0652 - soft_acc: 0.4975 - val_loss: 1.8524 - val_soft_acc: 0.4252\n",
      "Epoch 128/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.0005 - soft_acc: 0.5106 - val_loss: 1.6991 - val_soft_acc: 0.4355\n",
      "Epoch 129/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.0136 - soft_acc: 0.5066 - val_loss: 1.7226 - val_soft_acc: 0.4402\n",
      "Epoch 130/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9499 - soft_acc: 0.5232 - val_loss: 1.7538 - val_soft_acc: 0.4328\n",
      "Epoch 131/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.9951 - soft_acc: 0.5096 - val_loss: 1.7410 - val_soft_acc: 0.4494\n",
      "Epoch 132/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9517 - soft_acc: 0.5229 - val_loss: 1.6857 - val_soft_acc: 0.4469\n",
      "Epoch 133/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.9169 - soft_acc: 0.5365 - val_loss: 1.8312 - val_soft_acc: 0.4264\n",
      "Epoch 134/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9905 - soft_acc: 0.5142 - val_loss: 1.7639 - val_soft_acc: 0.4461\n",
      "Epoch 135/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9186 - soft_acc: 0.5328 - val_loss: 1.8241 - val_soft_acc: 0.4459\n",
      "Epoch 136/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9347 - soft_acc: 0.5244 - val_loss: 1.7515 - val_soft_acc: 0.4381\n",
      "Epoch 137/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8805 - soft_acc: 0.5448 - val_loss: 1.7535 - val_soft_acc: 0.4451\n",
      "Epoch 138/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8605 - soft_acc: 0.5494 - val_loss: 1.6658 - val_soft_acc: 0.4502\n",
      "Epoch 139/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8623 - soft_acc: 0.5456 - val_loss: 1.7001 - val_soft_acc: 0.4475\n",
      "Epoch 140/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8538 - soft_acc: 0.5512 - val_loss: 1.6946 - val_soft_acc: 0.4545\n",
      "Epoch 141/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9100 - soft_acc: 0.5325 - val_loss: 1.7241 - val_soft_acc: 0.4539\n",
      "Epoch 142/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.7901 - soft_acc: 0.5626 - val_loss: 1.6676 - val_soft_acc: 0.4600\n",
      "Epoch 143/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.7958 - soft_acc: 0.5675 - val_loss: 1.7265 - val_soft_acc: 0.4574\n",
      "Epoch 144/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.7588 - soft_acc: 0.5745 - val_loss: 1.5200 - val_soft_acc: 0.4932\n",
      "Epoch 145/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7914 - soft_acc: 0.5686 - val_loss: 1.5948 - val_soft_acc: 0.4648\n",
      "Epoch 146/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7775 - soft_acc: 0.5659 - val_loss: 1.5847 - val_soft_acc: 0.4713\n",
      "Epoch 147/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7529 - soft_acc: 0.5780 - val_loss: 1.6380 - val_soft_acc: 0.4658\n",
      "Epoch 148/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.7821 - soft_acc: 0.5656 - val_loss: 1.5812 - val_soft_acc: 0.4820\n",
      "Epoch 149/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.7121 - soft_acc: 0.5910 - val_loss: 1.7125 - val_soft_acc: 0.4568\n",
      "Epoch 150/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9261 - soft_acc: 0.5266 - val_loss: 1.6311 - val_soft_acc: 0.4625\n",
      "Epoch 151/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.7658 - soft_acc: 0.5716 - val_loss: 1.5654 - val_soft_acc: 0.4744\n",
      "Epoch 152/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.6948 - soft_acc: 0.5895 - val_loss: 1.6777 - val_soft_acc: 0.4590\n",
      "Epoch 153/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.6491 - soft_acc: 0.6082 - val_loss: 1.7151 - val_soft_acc: 0.4723\n",
      "Epoch 154/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.6484 - soft_acc: 0.6091 - val_loss: 1.6288 - val_soft_acc: 0.4656\n",
      "Epoch 155/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.7444 - soft_acc: 0.5808 - val_loss: 1.8175 - val_soft_acc: 0.4643\n",
      "Epoch 156/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.6508 - soft_acc: 0.6012 - val_loss: 1.5603 - val_soft_acc: 0.4914\n",
      "Epoch 157/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.6308 - soft_acc: 0.6224 - val_loss: 1.5874 - val_soft_acc: 0.4844\n",
      "Epoch 158/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.6715 - soft_acc: 0.6055 - val_loss: 1.6163 - val_soft_acc: 0.5004\n",
      "Epoch 159/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6423 - soft_acc: 0.6131 - val_loss: 1.7136 - val_soft_acc: 0.4814\n",
      "Epoch 160/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6617 - soft_acc: 0.6083 - val_loss: 1.5340 - val_soft_acc: 0.4963\n",
      "Epoch 161/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5901 - soft_acc: 0.6279 - val_loss: 1.6361 - val_soft_acc: 0.4908\n",
      "Epoch 162/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6217 - soft_acc: 0.6206 - val_loss: 1.4709 - val_soft_acc: 0.5033\n",
      "Epoch 163/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6565 - soft_acc: 0.6116 - val_loss: 1.5787 - val_soft_acc: 0.4916\n",
      "Epoch 164/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6136 - soft_acc: 0.6211 - val_loss: 1.5947 - val_soft_acc: 0.4984\n",
      "Epoch 165/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6526 - soft_acc: 0.6114 - val_loss: 1.6373 - val_soft_acc: 0.4904\n",
      "Epoch 166/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6258 - soft_acc: 0.6229 - val_loss: 1.5546 - val_soft_acc: 0.5131\n",
      "Epoch 167/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5902 - soft_acc: 0.6334 - val_loss: 1.6752 - val_soft_acc: 0.5008\n",
      "Epoch 168/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7153 - soft_acc: 0.5927 - val_loss: 1.5426 - val_soft_acc: 0.5045\n",
      "Epoch 169/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5809 - soft_acc: 0.6381 - val_loss: 1.6346 - val_soft_acc: 0.5045\n",
      "Epoch 170/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5207 - soft_acc: 0.6618 - val_loss: 1.5543 - val_soft_acc: 0.5193\n",
      "Epoch 171/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5366 - soft_acc: 0.6576 - val_loss: 1.5935 - val_soft_acc: 0.4986\n",
      "Epoch 172/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9336 - soft_acc: 0.5436 - val_loss: 1.6360 - val_soft_acc: 0.4846\n",
      "Epoch 173/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5525 - soft_acc: 0.6425 - val_loss: 1.4844 - val_soft_acc: 0.5266\n",
      "Epoch 174/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4317 - soft_acc: 0.6919 - val_loss: 1.4724 - val_soft_acc: 0.5471\n",
      "Epoch 175/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4193 - soft_acc: 0.7151 - val_loss: 1.5279 - val_soft_acc: 0.5316\n",
      "Epoch 176/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6649 - soft_acc: 0.6195 - val_loss: 1.7134 - val_soft_acc: 0.4789\n",
      "Epoch 177/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.7174 - soft_acc: 0.5868 - val_loss: 1.6404 - val_soft_acc: 0.4934\n",
      "Epoch 178/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5776 - soft_acc: 0.6397 - val_loss: 1.4484 - val_soft_acc: 0.5330\n",
      "Epoch 179/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4337 - soft_acc: 0.6977 - val_loss: 1.4811 - val_soft_acc: 0.5383\n",
      "Epoch 180/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.4973 - soft_acc: 0.6775 - val_loss: 1.3853 - val_soft_acc: 0.5449\n",
      "Epoch 181/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.4971 - soft_acc: 0.6840 - val_loss: 1.5836 - val_soft_acc: 0.5039\n",
      "Epoch 182/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4602 - soft_acc: 0.6886 - val_loss: 1.4221 - val_soft_acc: 0.5348\n",
      "Epoch 183/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5036 - soft_acc: 0.6791 - val_loss: 1.4636 - val_soft_acc: 0.5316\n",
      "Epoch 184/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5905 - soft_acc: 0.6313 - val_loss: 1.6812 - val_soft_acc: 0.4951\n",
      "Epoch 185/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5350 - soft_acc: 0.6572 - val_loss: 1.4811 - val_soft_acc: 0.5176\n",
      "Epoch 186/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4376 - soft_acc: 0.6989 - val_loss: 1.4049 - val_soft_acc: 0.5324\n",
      "Epoch 187/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5803 - soft_acc: 0.6622 - val_loss: 1.5778 - val_soft_acc: 0.4959\n",
      "Epoch 188/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4958 - soft_acc: 0.6742 - val_loss: 1.5310 - val_soft_acc: 0.5361\n",
      "Epoch 189/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5207 - soft_acc: 0.6627 - val_loss: 1.4946 - val_soft_acc: 0.5367\n",
      "Epoch 190/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3889 - soft_acc: 0.7136 - val_loss: 1.3332 - val_soft_acc: 0.5572\n",
      "Epoch 191/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3484 - soft_acc: 0.7466 - val_loss: 1.3366 - val_soft_acc: 0.5717\n",
      "Epoch 192/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3421 - soft_acc: 0.7506 - val_loss: 1.3350 - val_soft_acc: 0.5535\n",
      "Epoch 193/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3635 - soft_acc: 0.7305 - val_loss: 1.4090 - val_soft_acc: 0.5633\n",
      "Epoch 194/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7832 - soft_acc: 0.5931 - val_loss: 1.6827 - val_soft_acc: 0.4582\n",
      "Epoch 195/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.6054 - soft_acc: 0.6251 - val_loss: 1.5103 - val_soft_acc: 0.5105\n",
      "Epoch 196/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.4711 - soft_acc: 0.6720 - val_loss: 1.5230 - val_soft_acc: 0.5238\n",
      "Epoch 197/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.4205 - soft_acc: 0.7071 - val_loss: 1.4944 - val_soft_acc: 0.5400\n",
      "Epoch 198/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3814 - soft_acc: 0.7235 - val_loss: 1.4805 - val_soft_acc: 0.5434\n",
      "Epoch 199/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.3621 - soft_acc: 0.7423 - val_loss: 1.3373 - val_soft_acc: 0.5789\n",
      "Epoch 200/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.3352 - soft_acc: 0.7588 - val_loss: 1.2910 - val_soft_acc: 0.5871\n",
      "Epoch 201/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4085 - soft_acc: 0.7279 - val_loss: 1.5103 - val_soft_acc: 0.5400\n",
      "Epoch 202/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4775 - soft_acc: 0.6829 - val_loss: 1.5254 - val_soft_acc: 0.5420\n",
      "Epoch 203/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.4493 - soft_acc: 0.6861 - val_loss: 1.5504 - val_soft_acc: 0.5262\n",
      "Epoch 204/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.7275 - soft_acc: 0.5929 - val_loss: 1.4698 - val_soft_acc: 0.5344\n",
      "Epoch 205/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3865 - soft_acc: 0.7224 - val_loss: 1.3835 - val_soft_acc: 0.5725\n",
      "Epoch 206/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.3282 - soft_acc: 0.7584 - val_loss: 1.3065 - val_soft_acc: 0.5836\n",
      "Epoch 207/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2809 - soft_acc: 0.7790 - val_loss: 1.2647 - val_soft_acc: 0.5932\n",
      "Epoch 208/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3791 - soft_acc: 0.7402 - val_loss: 1.6399 - val_soft_acc: 0.5068\n",
      "Epoch 209/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.5196 - soft_acc: 0.6725 - val_loss: 1.4465 - val_soft_acc: 0.5535\n",
      "Epoch 210/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3547 - soft_acc: 0.7364 - val_loss: 1.4377 - val_soft_acc: 0.5580\n",
      "Epoch 211/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3488 - soft_acc: 0.7428 - val_loss: 1.4137 - val_soft_acc: 0.5766\n",
      "Epoch 212/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3367 - soft_acc: 0.7529 - val_loss: 1.4511 - val_soft_acc: 0.5629\n",
      "Epoch 213/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.5288 - soft_acc: 0.6624 - val_loss: 1.4913 - val_soft_acc: 0.5260\n",
      "Epoch 214/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4045 - soft_acc: 0.7172 - val_loss: 1.4344 - val_soft_acc: 0.5365\n",
      "Epoch 215/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3023 - soft_acc: 0.7557 - val_loss: 1.2334 - val_soft_acc: 0.6152\n",
      "Epoch 216/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2516 - soft_acc: 0.7959 - val_loss: 1.2755 - val_soft_acc: 0.5947\n",
      "Epoch 217/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3006 - soft_acc: 0.7709 - val_loss: 1.4188 - val_soft_acc: 0.5861\n",
      "Epoch 218/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3867 - soft_acc: 0.7352 - val_loss: 1.5633 - val_soft_acc: 0.5408\n",
      "Epoch 219/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.4655 - soft_acc: 0.6902 - val_loss: 1.5871 - val_soft_acc: 0.5174\n",
      "Epoch 220/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.4413 - soft_acc: 0.6863 - val_loss: 1.4174 - val_soft_acc: 0.5602\n",
      "Epoch 221/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3127 - soft_acc: 0.7541 - val_loss: 1.5440 - val_soft_acc: 0.5523\n",
      "Epoch 222/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3959 - soft_acc: 0.7187 - val_loss: 1.3780 - val_soft_acc: 0.5744\n",
      "Epoch 223/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2838 - soft_acc: 0.7699 - val_loss: 1.2517 - val_soft_acc: 0.5900\n",
      "Epoch 224/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2774 - soft_acc: 0.7825 - val_loss: 1.4338 - val_soft_acc: 0.5703\n",
      "Epoch 225/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3962 - soft_acc: 0.7218 - val_loss: 1.5814 - val_soft_acc: 0.5301\n",
      "Epoch 226/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4390 - soft_acc: 0.7019 - val_loss: 1.4586 - val_soft_acc: 0.5547\n",
      "Epoch 227/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3532 - soft_acc: 0.7324 - val_loss: 1.2849 - val_soft_acc: 0.6049\n",
      "Epoch 228/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1990 - soft_acc: 0.8193 - val_loss: 1.2867 - val_soft_acc: 0.6248\n",
      "Epoch 229/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2353 - soft_acc: 0.8156 - val_loss: 1.3105 - val_soft_acc: 0.5820\n",
      "Epoch 230/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3256 - soft_acc: 0.7580 - val_loss: 1.5714 - val_soft_acc: 0.5385\n",
      "Epoch 231/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9607 - soft_acc: 0.5458 - val_loss: 1.5472 - val_soft_acc: 0.5252\n",
      "Epoch 232/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3878 - soft_acc: 0.7075 - val_loss: 1.4539 - val_soft_acc: 0.5691\n",
      "Epoch 233/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2382 - soft_acc: 0.7862 - val_loss: 1.2912 - val_soft_acc: 0.6201\n",
      "Epoch 234/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1836 - soft_acc: 0.8360 - val_loss: 1.2901 - val_soft_acc: 0.5986\n",
      "Epoch 235/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2235 - soft_acc: 0.8223 - val_loss: 1.5727 - val_soft_acc: 0.5596\n",
      "Epoch 236/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2873 - soft_acc: 0.7761 - val_loss: 1.4155 - val_soft_acc: 0.5707\n",
      "Epoch 237/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3107 - soft_acc: 0.7706 - val_loss: 1.4548 - val_soft_acc: 0.5699\n",
      "Epoch 238/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.5469 - soft_acc: 0.6791 - val_loss: 1.6678 - val_soft_acc: 0.5178\n",
      "Epoch 239/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3718 - soft_acc: 0.7193 - val_loss: 1.4075 - val_soft_acc: 0.5840\n",
      "Epoch 240/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2555 - soft_acc: 0.7862 - val_loss: 1.3290 - val_soft_acc: 0.5922\n",
      "Epoch 241/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2511 - soft_acc: 0.8010 - val_loss: 1.2416 - val_soft_acc: 0.6152\n",
      "Epoch 242/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2164 - soft_acc: 0.8296 - val_loss: 1.3047 - val_soft_acc: 0.6051\n",
      "Epoch 243/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4335 - soft_acc: 0.7234 - val_loss: 1.6836 - val_soft_acc: 0.5168 \n",
      "Epoch 244/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3231 - soft_acc: 0.7374 - val_loss: 1.4647 - val_soft_acc: 0.5561\n",
      "Epoch 245/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3377 - soft_acc: 0.7407 - val_loss: 1.4298 - val_soft_acc: 0.5672\n",
      "Epoch 246/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3831 - soft_acc: 0.7165 - val_loss: 1.4494 - val_soft_acc: 0.5650\n",
      "Epoch 247/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2939 - soft_acc: 0.7585 - val_loss: 1.5240 - val_soft_acc: 0.5668\n",
      "Epoch 248/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2254 - soft_acc: 0.8045 - val_loss: 1.2288 - val_soft_acc: 0.6373\n",
      "Epoch 249/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.2021 - soft_acc: 0.8327 - val_loss: 1.4143 - val_soft_acc: 0.5883\n",
      "Epoch 250/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1990 - soft_acc: 0.8160 - val_loss: 1.3381 - val_soft_acc: 0.6100\n",
      "Epoch 251/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1958 - soft_acc: 0.8336 - val_loss: 1.4075 - val_soft_acc: 0.5922\n",
      "Epoch 252/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5915 - soft_acc: 0.6647 - val_loss: 1.5845 - val_soft_acc: 0.5191\n",
      "Epoch 253/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.3760 - soft_acc: 0.7140 - val_loss: 1.6879 - val_soft_acc: 0.5262\n",
      "Epoch 254/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.2819 - soft_acc: 0.7670 - val_loss: 1.2277 - val_soft_acc: 0.6262\n",
      "Epoch 255/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1863 - soft_acc: 0.8389 - val_loss: 1.3256 - val_soft_acc: 0.6023\n",
      "Epoch 256/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2719 - soft_acc: 0.7938 - val_loss: 1.4691 - val_soft_acc: 0.5770\n",
      "Epoch 257/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2926 - soft_acc: 0.7768 - val_loss: 1.4147 - val_soft_acc: 0.5703\n",
      "Epoch 258/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2789 - soft_acc: 0.7800 - val_loss: 1.3028 - val_soft_acc: 0.6016\n",
      "Epoch 259/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3643 - soft_acc: 0.7500 - val_loss: 1.3788 - val_soft_acc: 0.5887\n",
      "Epoch 260/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4540 - soft_acc: 0.6925 - val_loss: 1.5849 - val_soft_acc: 0.5307\n",
      "Epoch 261/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3703 - soft_acc: 0.7260 - val_loss: 1.4045 - val_soft_acc: 0.5576\n",
      "Epoch 262/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2422 - soft_acc: 0.7991 - val_loss: 1.2636 - val_soft_acc: 0.6115\n",
      "Epoch 263/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2062 - soft_acc: 0.8217 - val_loss: 1.2440 - val_soft_acc: 0.6340\n",
      "Epoch 264/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1420 - soft_acc: 0.8641 - val_loss: 1.2308 - val_soft_acc: 0.6422\n",
      "Epoch 265/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1589 - soft_acc: 0.8620 - val_loss: 1.2976 - val_soft_acc: 0.6217\n",
      "Epoch 266/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1346 - soft_acc: 0.8728 - val_loss: 1.1841 - val_soft_acc: 0.6566\n",
      "Epoch 267/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1424 - soft_acc: 0.8787 - val_loss: 1.2574 - val_soft_acc: 0.6176\n",
      "Epoch 268/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2963 - soft_acc: 0.7851 - val_loss: 1.6209 - val_soft_acc: 0.5359\n",
      "Epoch 269/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7565 - soft_acc: 0.6053 - val_loss: 1.5781 - val_soft_acc: 0.5346\n",
      "Epoch 270/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3245 - soft_acc: 0.7270 - val_loss: 1.3909 - val_soft_acc: 0.6027\n",
      "Epoch 271/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1929 - soft_acc: 0.8177 - val_loss: 1.2147 - val_soft_acc: 0.6383\n",
      "Epoch 272/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1359 - soft_acc: 0.8676 - val_loss: 1.2126 - val_soft_acc: 0.6457\n",
      "Epoch 273/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1599 - soft_acc: 0.8652 - val_loss: 1.2372 - val_soft_acc: 0.6203\n",
      "Epoch 274/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1934 - soft_acc: 0.8334 - val_loss: 1.2607 - val_soft_acc: 0.6039\n",
      "Epoch 275/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2025 - soft_acc: 0.8291 - val_loss: 1.5579 - val_soft_acc: 0.5469\n",
      "Epoch 276/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4097 - soft_acc: 0.7257 - val_loss: 1.6167 - val_soft_acc: 0.5270\n",
      "Epoch 277/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4554 - soft_acc: 0.6988 - val_loss: 1.3991 - val_soft_acc: 0.5703\n",
      "Epoch 278/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4097 - soft_acc: 0.7146 - val_loss: 1.3333 - val_soft_acc: 0.5967\n",
      "Epoch 279/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2240 - soft_acc: 0.8021 - val_loss: 1.2562 - val_soft_acc: 0.6256\n",
      "Epoch 280/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1154 - soft_acc: 0.8758 - val_loss: 1.1792 - val_soft_acc: 0.6547\n",
      "Epoch 281/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0772 - soft_acc: 0.9080 - val_loss: 1.1450 - val_soft_acc: 0.6723\n",
      "Epoch 282/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0684 - soft_acc: 0.9200 - val_loss: 1.1694 - val_soft_acc: 0.6629\n",
      "Epoch 283/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0691 - soft_acc: 0.9240 - val_loss: 1.2109 - val_soft_acc: 0.6457\n",
      "Epoch 284/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1266 - soft_acc: 0.8810 - val_loss: 1.2006 - val_soft_acc: 0.6414\n",
      "Epoch 285/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3810 - soft_acc: 0.7603 - val_loss: 1.8050 - val_soft_acc: 0.4803\n",
      "Epoch 286/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7131 - soft_acc: 0.6059 - val_loss: 1.5407 - val_soft_acc: 0.5270\n",
      "Epoch 287/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4237 - soft_acc: 0.7039 - val_loss: 1.2668 - val_soft_acc: 0.6064\n",
      "Epoch 288/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1400 - soft_acc: 0.8496 - val_loss: 1.2305 - val_soft_acc: 0.6262\n",
      "Epoch 289/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1407 - soft_acc: 0.8616 - val_loss: 1.1498 - val_soft_acc: 0.6621\n",
      "Epoch 290/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0844 - soft_acc: 0.9007 - val_loss: 1.1422 - val_soft_acc: 0.6637\n",
      "Epoch 291/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0615 - soft_acc: 0.9250 - val_loss: 1.1632 - val_soft_acc: 0.6496\n",
      "Epoch 292/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0896 - soft_acc: 0.9126 - val_loss: 1.1703 - val_soft_acc: 0.6633\n",
      "Epoch 293/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1081 - soft_acc: 0.8933 - val_loss: 1.2865 - val_soft_acc: 0.6398\n",
      "Epoch 294/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5998 - soft_acc: 0.6627 - val_loss: 1.6019 - val_soft_acc: 0.5244\n",
      "Epoch 295/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4252 - soft_acc: 0.7022 - val_loss: 1.6629 - val_soft_acc: 0.5143\n",
      "Epoch 296/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4069 - soft_acc: 0.7188 - val_loss: 1.2407 - val_soft_acc: 0.6117\n",
      "Epoch 297/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1508 - soft_acc: 0.8409 - val_loss: 1.2220 - val_soft_acc: 0.6389\n",
      "Epoch 298/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1201 - soft_acc: 0.8749 - val_loss: 1.1511 - val_soft_acc: 0.6504\n",
      "Epoch 299/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1406 - soft_acc: 0.8658 - val_loss: 1.1797 - val_soft_acc: 0.6484\n",
      "Epoch 300/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1713 - soft_acc: 0.8522 - val_loss: 1.2148 - val_soft_acc: 0.6305\n",
      "Epoch 301/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1919 - soft_acc: 0.8356 - val_loss: 1.2162 - val_soft_acc: 0.6389\n",
      "Epoch 302/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1039 - soft_acc: 0.8841 - val_loss: 1.1400 - val_soft_acc: 0.6615\n",
      "Epoch 303/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1532 - soft_acc: 0.8700 - val_loss: 1.2144 - val_soft_acc: 0.6363\n",
      "Epoch 304/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1915 - soft_acc: 0.8402 - val_loss: 1.5808 - val_soft_acc: 0.5629\n",
      "Epoch 305/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6850 - soft_acc: 0.6403 - val_loss: 1.5474 - val_soft_acc: 0.5381\n",
      "Epoch 306/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3877 - soft_acc: 0.7217 - val_loss: 1.5214 - val_soft_acc: 0.5539\n",
      "Epoch 307/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5173 - soft_acc: 0.6883 - val_loss: 1.2672 - val_soft_acc: 0.6064\n",
      "Epoch 308/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1915 - soft_acc: 0.8200 - val_loss: 1.1991 - val_soft_acc: 0.6410\n",
      "Epoch 309/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1037 - soft_acc: 0.8825 - val_loss: 1.0870 - val_soft_acc: 0.6676\n",
      "Epoch 310/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0600 - soft_acc: 0.9208 - val_loss: 1.0736 - val_soft_acc: 0.6828\n",
      "Epoch 311/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0398 - soft_acc: 0.9418 - val_loss: 1.0638 - val_soft_acc: 0.6814\n",
      "Epoch 312/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0499 - soft_acc: 0.9410 - val_loss: 1.1048 - val_soft_acc: 0.6725\n",
      "Epoch 313/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0612 - soft_acc: 0.9311 - val_loss: 1.1287 - val_soft_acc: 0.6605\n",
      "Epoch 314/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1926 - soft_acc: 0.8562 - val_loss: 1.4163 - val_soft_acc: 0.5777\n",
      "Epoch 315/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.5536 - soft_acc: 0.6740 - val_loss: 1.5120 - val_soft_acc: 0.5406\n",
      "Epoch 316/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.4453 - soft_acc: 0.7035 - val_loss: 1.5006 - val_soft_acc: 0.5760\n",
      "Epoch 317/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2096 - soft_acc: 0.8047 - val_loss: 1.2065 - val_soft_acc: 0.6523\n",
      "Epoch 318/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0932 - soft_acc: 0.8932 - val_loss: 1.2231 - val_soft_acc: 0.6352\n",
      "Epoch 319/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0826 - soft_acc: 0.9127 - val_loss: 1.1635 - val_soft_acc: 0.6598\n",
      "Epoch 320/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0716 - soft_acc: 0.9200 - val_loss: 1.1223 - val_soft_acc: 0.6754\n",
      "Epoch 321/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0512 - soft_acc: 0.9369 - val_loss: 1.1603 - val_soft_acc: 0.6689\n",
      "Epoch 322/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3158 - soft_acc: 0.7953 - val_loss: 1.5071 - val_soft_acc: 0.5436\n",
      "Epoch 323/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4358 - soft_acc: 0.7103 - val_loss: 1.4164 - val_soft_acc: 0.5740\n",
      "Epoch 324/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2947 - soft_acc: 0.7664 - val_loss: 1.2344 - val_soft_acc: 0.6408\n",
      "Epoch 325/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1199 - soft_acc: 0.8638 - val_loss: 1.1853 - val_soft_acc: 0.6607\n",
      "Epoch 326/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0560 - soft_acc: 0.9205 - val_loss: 1.0860 - val_soft_acc: 0.6818\n",
      "Epoch 327/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0372 - soft_acc: 0.9460 - val_loss: 1.1197 - val_soft_acc: 0.6836\n",
      "Epoch 328/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0518 - soft_acc: 0.9414 - val_loss: 1.2221 - val_soft_acc: 0.6561\n",
      "Epoch 329/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1707 - soft_acc: 0.8649 - val_loss: 1.4842 - val_soft_acc: 0.5725\n",
      "Epoch 330/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6538 - soft_acc: 0.6361 - val_loss: 2.0287 - val_soft_acc: 0.4656\n",
      "Epoch 331/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4544 - soft_acc: 0.6967 - val_loss: 1.3662 - val_soft_acc: 0.5943\n",
      "Epoch 332/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1862 - soft_acc: 0.8189 - val_loss: 1.2407 - val_soft_acc: 0.6094\n",
      "Epoch 333/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1298 - soft_acc: 0.8707 - val_loss: 1.1854 - val_soft_acc: 0.6572\n",
      "Epoch 334/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0995 - soft_acc: 0.9024 - val_loss: 1.2212 - val_soft_acc: 0.6400\n",
      "Epoch 335/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1387 - soft_acc: 0.8797 - val_loss: 1.2529 - val_soft_acc: 0.6531\n",
      "Epoch 336/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0993 - soft_acc: 0.8987 - val_loss: 1.1578 - val_soft_acc: 0.6723\n",
      "Epoch 337/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0490 - soft_acc: 0.9338 - val_loss: 1.1098 - val_soft_acc: 0.6842\n",
      "Epoch 338/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0350 - soft_acc: 0.9496 - val_loss: 1.1127 - val_soft_acc: 0.6949\n",
      "Epoch 339/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0242 - soft_acc: 0.9596 - val_loss: 1.0799 - val_soft_acc: 0.6957\n",
      "Epoch 340/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0195 - soft_acc: 0.9639 - val_loss: 1.0564 - val_soft_acc: 0.6951\n",
      "Epoch 341/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1068 - soft_acc: 0.9122 - val_loss: 1.4465 - val_soft_acc: 0.5674\n",
      "Epoch 342/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9291 - soft_acc: 0.5708 - val_loss: 2.0002 - val_soft_acc: 0.4627\n",
      "Epoch 343/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4956 - soft_acc: 0.6720 - val_loss: 1.3443 - val_soft_acc: 0.5781\n",
      "Epoch 344/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1893 - soft_acc: 0.8161 - val_loss: 1.2746 - val_soft_acc: 0.6293\n",
      "Epoch 345/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0907 - soft_acc: 0.8887 - val_loss: 1.1268 - val_soft_acc: 0.6707\n",
      "Epoch 346/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0409 - soft_acc: 0.9364 - val_loss: 1.1149 - val_soft_acc: 0.6793\n",
      "Epoch 347/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0273 - soft_acc: 0.9540 - val_loss: 1.0753 - val_soft_acc: 0.6900\n",
      "Epoch 348/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0207 - soft_acc: 0.9629 - val_loss: 1.0676 - val_soft_acc: 0.6926\n",
      "Epoch 349/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0165 - soft_acc: 0.9657 - val_loss: 1.1100 - val_soft_acc: 0.6926\n",
      "Epoch 350/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1956 - soft_acc: 0.8587 - val_loss: 1.4781 - val_soft_acc: 0.5645\n",
      "Epoch 351/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5586 - soft_acc: 0.6680 - val_loss: 1.6993 - val_soft_acc: 0.5322\n",
      "Epoch 352/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3138 - soft_acc: 0.7580 - val_loss: 1.3647 - val_soft_acc: 0.5982\n",
      "Epoch 353/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2577 - soft_acc: 0.7898 - val_loss: 1.3652 - val_soft_acc: 0.5891\n",
      "Epoch 354/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1857 - soft_acc: 0.8305 - val_loss: 1.2921 - val_soft_acc: 0.6270\n",
      "Epoch 355/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1750 - soft_acc: 0.8491 - val_loss: 1.1805 - val_soft_acc: 0.6441\n",
      "Epoch 356/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2279 - soft_acc: 0.8229 - val_loss: 1.2372 - val_soft_acc: 0.6262\n",
      "Epoch 357/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1375 - soft_acc: 0.8662 - val_loss: 1.1635 - val_soft_acc: 0.6602\n",
      "Epoch 358/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0657 - soft_acc: 0.9192 - val_loss: 1.0673 - val_soft_acc: 0.6895\n",
      "Epoch 359/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0389 - soft_acc: 0.9437 - val_loss: 1.0585 - val_soft_acc: 0.7000\n",
      "Epoch 360/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0219 - soft_acc: 0.9594 - val_loss: 1.0798 - val_soft_acc: 0.7047\n",
      "Epoch 361/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0253 - soft_acc: 0.9636 - val_loss: 1.1022 - val_soft_acc: 0.6922\n",
      "Epoch 362/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0506 - soft_acc: 0.9435 - val_loss: 1.1212 - val_soft_acc: 0.6781\n",
      "Epoch 363/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0998 - soft_acc: 0.9034 - val_loss: 1.3681 - val_soft_acc: 0.6115\n",
      "Epoch 364/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6463 - soft_acc: 0.6654 - val_loss: 1.8030 - val_soft_acc: 0.4752\n",
      "Epoch 365/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5082 - soft_acc: 0.6646 - val_loss: 1.3660 - val_soft_acc: 0.5881\n",
      "Epoch 366/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1761 - soft_acc: 0.8221 - val_loss: 1.3665 - val_soft_acc: 0.6193\n",
      "Epoch 367/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1679 - soft_acc: 0.8392 - val_loss: 1.2023 - val_soft_acc: 0.6510\n",
      "Epoch 368/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0604 - soft_acc: 0.9157 - val_loss: 1.0870 - val_soft_acc: 0.6861\n",
      "Epoch 369/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0314 - soft_acc: 0.9476 - val_loss: 1.0941 - val_soft_acc: 0.6809\n",
      "Epoch 370/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0245 - soft_acc: 0.9574 - val_loss: 1.0927 - val_soft_acc: 0.6848\n",
      "Epoch 371/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0758 - soft_acc: 0.9135 - val_loss: 1.1486 - val_soft_acc: 0.6848\n",
      "Epoch 372/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0589 - soft_acc: 0.9307 - val_loss: 1.1585 - val_soft_acc: 0.6596\n",
      "Epoch 373/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2142 - soft_acc: 0.8307 - val_loss: 1.3436 - val_soft_acc: 0.5918\n",
      "Epoch 374/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5442 - soft_acc: 0.6926 - val_loss: 1.5949 - val_soft_acc: 0.5318\n",
      "Epoch 375/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4512 - soft_acc: 0.7129 - val_loss: 1.4696 - val_soft_acc: 0.5748\n",
      "Epoch 376/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1324 - soft_acc: 0.8544 - val_loss: 1.1933 - val_soft_acc: 0.6348\n",
      "Epoch 377/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0715 - soft_acc: 0.9113 - val_loss: 1.1380 - val_soft_acc: 0.6742\n",
      "Epoch 378/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0849 - soft_acc: 0.9106 - val_loss: 1.2364 - val_soft_acc: 0.6488\n",
      "Epoch 379/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1473 - soft_acc: 0.8719 - val_loss: 1.1478 - val_soft_acc: 0.6568\n",
      "Epoch 380/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0743 - soft_acc: 0.9138 - val_loss: 1.1291 - val_soft_acc: 0.6740\n",
      "Epoch 381/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0485 - soft_acc: 0.9367 - val_loss: 1.1103 - val_soft_acc: 0.6814\n",
      "Epoch 382/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0561 - soft_acc: 0.9312 - val_loss: 1.1084 - val_soft_acc: 0.6885\n",
      "Epoch 383/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1128 - soft_acc: 0.8959 - val_loss: 1.3432 - val_soft_acc: 0.6256\n",
      "Epoch 384/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3115 - soft_acc: 0.7772 - val_loss: 1.6124 - val_soft_acc: 0.5516\n",
      "Epoch 385/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3597 - soft_acc: 0.7389 - val_loss: 1.3309 - val_soft_acc: 0.6068\n",
      "Epoch 386/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.1389 - soft_acc: 0.8475 - val_loss: 1.1634 - val_soft_acc: 0.6580\n",
      "Epoch 387/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0538 - soft_acc: 0.9222 - val_loss: 1.0930 - val_soft_acc: 0.6863\n",
      "Epoch 388/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0301 - soft_acc: 0.9495 - val_loss: 1.0871 - val_soft_acc: 0.7014\n",
      "Epoch 389/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0160 - soft_acc: 0.9670 - val_loss: 1.0470 - val_soft_acc: 0.7131\n",
      "Epoch 390/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0108 - soft_acc: 0.9733 - val_loss: 1.0515 - val_soft_acc: 0.7127\n",
      "Epoch 391/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0135 - soft_acc: 0.9731 - val_loss: 1.0854 - val_soft_acc: 0.6992\n",
      "Epoch 392/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0369 - soft_acc: 0.9600 - val_loss: 1.1859 - val_soft_acc: 0.6533\n",
      "Epoch 393/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 1.0183 - soft_acc: 0.56 - 1s 74us/step - loss: 1.0104 - soft_acc: 0.5687 - val_loss: 1.7327 - val_soft_acc: 0.4738\n",
      "Epoch 394/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4750 - soft_acc: 0.6878 - val_loss: 1.4114 - val_soft_acc: 0.5766\n",
      "Epoch 395/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1707 - soft_acc: 0.8252 - val_loss: 1.2119 - val_soft_acc: 0.6291\n",
      "Epoch 396/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1371 - soft_acc: 0.8666 - val_loss: 1.1474 - val_soft_acc: 0.6656\n",
      "Epoch 397/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0848 - soft_acc: 0.9032 - val_loss: 1.1474 - val_soft_acc: 0.6676\n",
      "Epoch 398/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0567 - soft_acc: 0.9283 - val_loss: 1.1895 - val_soft_acc: 0.6684\n",
      "Epoch 399/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0486 - soft_acc: 0.9394 - val_loss: 1.0979 - val_soft_acc: 0.6873\n",
      "Epoch 400/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0419 - soft_acc: 0.9497 - val_loss: 1.0788 - val_soft_acc: 0.6906\n",
      "Epoch 401/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0323 - soft_acc: 0.9553 - val_loss: 1.0799 - val_soft_acc: 0.6967\n",
      "Epoch 402/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0146 - soft_acc: 0.9700 - val_loss: 1.0621 - val_soft_acc: 0.6980\n",
      "Epoch 403/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0099 - soft_acc: 0.9755 - val_loss: 1.0601 - val_soft_acc: 0.7074\n",
      "Epoch 404/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0069 - soft_acc: 0.9795 - val_loss: 1.0587 - val_soft_acc: 0.7080\n",
      "Epoch 405/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0065 - soft_acc: 0.9823 - val_loss: 1.0635 - val_soft_acc: 0.7039\n",
      "Epoch 406/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0073 - soft_acc: 0.9803 - val_loss: 1.0622 - val_soft_acc: 0.7020\n",
      "Epoch 407/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0631 - soft_acc: 0.9433 - val_loss: 1.3846 - val_soft_acc: 0.5936\n",
      "Epoch 408/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.1503 - soft_acc: 0.5217 - val_loss: 1.8364 - val_soft_acc: 0.4412\n",
      "Epoch 409/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5084 - soft_acc: 0.6690 - val_loss: 1.3769 - val_soft_acc: 0.5863\n",
      "Epoch 410/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1780 - soft_acc: 0.8214 - val_loss: 1.1804 - val_soft_acc: 0.6447\n",
      "Epoch 411/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0758 - soft_acc: 0.9015 - val_loss: 1.1044 - val_soft_acc: 0.6775\n",
      "Epoch 412/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0346 - soft_acc: 0.9429 - val_loss: 1.0825 - val_soft_acc: 0.6918\n",
      "Epoch 413/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0195 - soft_acc: 0.9611 - val_loss: 1.0617 - val_soft_acc: 0.7014\n",
      "Epoch 414/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0124 - soft_acc: 0.9700 - val_loss: 1.0525 - val_soft_acc: 0.7035\n",
      "Epoch 415/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0101 - soft_acc: 0.9738 - val_loss: 1.0553 - val_soft_acc: 0.7041\n",
      "Epoch 416/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0075 - soft_acc: 0.9787 - val_loss: 1.0596 - val_soft_acc: 0.7047\n",
      "Epoch 417/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0154 - soft_acc: 0.9745 - val_loss: 1.1045 - val_soft_acc: 0.6844\n",
      "Epoch 418/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6145 - soft_acc: 0.7033 - val_loss: 1.8480 - val_soft_acc: 0.4648\n",
      "Epoch 419/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5183 - soft_acc: 0.6623 - val_loss: 1.4797 - val_soft_acc: 0.5723\n",
      "Epoch 420/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2673 - soft_acc: 0.7826 - val_loss: 1.3798 - val_soft_acc: 0.6029\n",
      "Epoch 421/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1563 - soft_acc: 0.8477 - val_loss: 1.1347 - val_soft_acc: 0.6596\n",
      "Epoch 422/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0606 - soft_acc: 0.9172 - val_loss: 1.0763 - val_soft_acc: 0.6721\n",
      "Epoch 423/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0325 - soft_acc: 0.9476 - val_loss: 1.0595 - val_soft_acc: 0.7014\n",
      "Epoch 424/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0339 - soft_acc: 0.9580 - val_loss: 1.0768 - val_soft_acc: 0.6869\n",
      "Epoch 425/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0243 - soft_acc: 0.9621 - val_loss: 1.0261 - val_soft_acc: 0.7053\n",
      "Epoch 426/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0107 - soft_acc: 0.9740 - val_loss: 1.0260 - val_soft_acc: 0.7098\n",
      "Epoch 427/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0080 - soft_acc: 0.9786 - val_loss: 1.0313 - val_soft_acc: 0.7113\n",
      "Epoch 428/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0054 - soft_acc: 0.9815 - val_loss: 1.0297 - val_soft_acc: 0.7084\n",
      "Epoch 429/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0039 - soft_acc: 0.9851 - val_loss: 1.0278 - val_soft_acc: 0.7102\n",
      "Epoch 430/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0055 - soft_acc: 0.9828 - val_loss: 1.0456 - val_soft_acc: 0.7082\n",
      "Epoch 431/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0091 - soft_acc: 0.9780 - val_loss: 1.0395 - val_soft_acc: 0.7125\n",
      "Epoch 432/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0510 - soft_acc: 0.9433 - val_loss: 1.6774 - val_soft_acc: 0.5240\n",
      "Epoch 433/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.0399 - soft_acc: 0.5272 - val_loss: 1.7275 - val_soft_acc: 0.4908\n",
      "Epoch 434/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3953 - soft_acc: 0.7028 - val_loss: 1.4704 - val_soft_acc: 0.5641\n",
      "Epoch 435/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1843 - soft_acc: 0.8232 - val_loss: 1.2205 - val_soft_acc: 0.6490\n",
      "Epoch 436/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0711 - soft_acc: 0.8994 - val_loss: 1.1039 - val_soft_acc: 0.6826\n",
      "Epoch 437/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0313 - soft_acc: 0.9467 - val_loss: 1.0675 - val_soft_acc: 0.7023\n",
      "Epoch 438/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0159 - soft_acc: 0.9657 - val_loss: 1.0519 - val_soft_acc: 0.7088\n",
      "Epoch 439/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0099 - soft_acc: 0.9748 - val_loss: 1.0364 - val_soft_acc: 0.7072\n",
      "Epoch 440/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0074 - soft_acc: 0.9775 - val_loss: 1.0411 - val_soft_acc: 0.7129\n",
      "Epoch 441/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0063 - soft_acc: 0.9797 - val_loss: 1.0384 - val_soft_acc: 0.7096\n",
      "Epoch 442/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1533 - soft_acc: 0.8893 - val_loss: 1.4253 - val_soft_acc: 0.5805\n",
      "Epoch 443/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4068 - soft_acc: 0.7288 - val_loss: 1.6338 - val_soft_acc: 0.5182\n",
      "Epoch 444/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4029 - soft_acc: 0.7146 - val_loss: 1.5478 - val_soft_acc: 0.5537\n",
      "Epoch 445/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1764 - soft_acc: 0.8252 - val_loss: 1.1231 - val_soft_acc: 0.6684\n",
      "Epoch 446/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0449 - soft_acc: 0.9275 - val_loss: 1.1280 - val_soft_acc: 0.6816\n",
      "Epoch 447/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0390 - soft_acc: 0.9456 - val_loss: 1.1062 - val_soft_acc: 0.6885\n",
      "Epoch 448/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0493 - soft_acc: 0.9400 - val_loss: 1.0750 - val_soft_acc: 0.6963\n",
      "Epoch 449/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0343 - soft_acc: 0.9536 - val_loss: 1.1434 - val_soft_acc: 0.6844\n",
      "Epoch 450/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0955 - soft_acc: 0.9174 - val_loss: 1.2122 - val_soft_acc: 0.6504\n",
      "Epoch 451/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0879 - soft_acc: 0.9024 - val_loss: 1.1858 - val_soft_acc: 0.6744\n",
      "Epoch 452/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0831 - soft_acc: 0.9127 - val_loss: 1.2334 - val_soft_acc: 0.6537\n",
      "Epoch 453/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5586 - soft_acc: 0.6819 - val_loss: 1.4740 - val_soft_acc: 0.5584\n",
      "Epoch 454/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2108 - soft_acc: 0.8002 - val_loss: 1.3565 - val_soft_acc: 0.6143\n",
      "Epoch 455/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1154 - soft_acc: 0.8756 - val_loss: 1.0874 - val_soft_acc: 0.6789\n",
      "Epoch 456/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0622 - soft_acc: 0.9178 - val_loss: 1.1442 - val_soft_acc: 0.6701\n",
      "Epoch 457/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0843 - soft_acc: 0.9086 - val_loss: 1.0638 - val_soft_acc: 0.6914\n",
      "Epoch 458/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0766 - soft_acc: 0.9219 - val_loss: 1.2155 - val_soft_acc: 0.6715\n",
      "Epoch 459/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0987 - soft_acc: 0.9105 - val_loss: 1.2182 - val_soft_acc: 0.6617\n",
      "Epoch 460/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1201 - soft_acc: 0.8947 - val_loss: 1.2311 - val_soft_acc: 0.6582\n",
      "Epoch 461/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1147 - soft_acc: 0.9052 - val_loss: 1.3683 - val_soft_acc: 0.6018\n",
      "Epoch 462/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2153 - soft_acc: 0.8281 - val_loss: 1.3240 - val_soft_acc: 0.6309\n",
      "Epoch 463/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0765 - soft_acc: 0.9039 - val_loss: 1.1103 - val_soft_acc: 0.6861\n",
      "Epoch 464/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0314 - soft_acc: 0.9514 - val_loss: 1.1048 - val_soft_acc: 0.6967\n",
      "Epoch 465/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0175 - soft_acc: 0.9643 - val_loss: 1.0727 - val_soft_acc: 0.7053\n",
      "Epoch 466/5000\n",
      "14640/14640 [==============================] - 1s 73us/step - loss: 0.0092 - soft_acc: 0.9754 - val_loss: 1.0594 - val_soft_acc: 0.7148\n",
      "Epoch 467/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0055 - soft_acc: 0.9820 - val_loss: 1.0580 - val_soft_acc: 0.7135\n",
      "Epoch 468/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0042 - soft_acc: 0.9843 - val_loss: 1.0581 - val_soft_acc: 0.7143\n",
      "Epoch 469/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0093 - soft_acc: 0.9806 - val_loss: 1.1002 - val_soft_acc: 0.7023\n",
      "Epoch 470/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0344 - soft_acc: 0.9586 - val_loss: 1.1002 - val_soft_acc: 0.6975\n",
      "Epoch 471/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1404 - soft_acc: 0.8816 - val_loss: 2.0044 - val_soft_acc: 0.5018\n",
      "Epoch 472/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.9728 - soft_acc: 0.5615 - val_loss: 1.5483 - val_soft_acc: 0.5484\n",
      "Epoch 473/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.2408 - soft_acc: 0.78 - 1s 75us/step - loss: 0.2407 - soft_acc: 0.7838 - val_loss: 1.1742 - val_soft_acc: 0.6518\n",
      "Epoch 474/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0688 - soft_acc: 0.9031 - val_loss: 1.1053 - val_soft_acc: 0.6877\n",
      "Epoch 475/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0261 - soft_acc: 0.9508 - val_loss: 1.0701 - val_soft_acc: 0.6971\n",
      "Epoch 476/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0143 - soft_acc: 0.9675 - val_loss: 1.0549 - val_soft_acc: 0.7020\n",
      "Epoch 00476: early stopping\n",
      ">0.702\n",
      "Train on 14640 samples, validate on 2440 samples\n",
      "Epoch 1/5000\n",
      "14640/14640 [==============================] - 2s 107us/step - loss: 4.8213 - soft_acc: 0.1528 - val_loss: 4.2161 - val_soft_acc: 0.1736\n",
      "Epoch 2/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 4.2278 - soft_acc: 0.1622 - val_loss: 3.9657 - val_soft_acc: 0.1469\n",
      "Epoch 3/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 4.1006 - soft_acc: 0.1684 - val_loss: 3.9078 - val_soft_acc: 0.1584\n",
      "Epoch 4/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 4.0408 - soft_acc: 0.1676 - val_loss: 3.8651 - val_soft_acc: 0.1523\n",
      "Epoch 5/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.9655 - soft_acc: 0.1715 - val_loss: 4.1481 - val_soft_acc: 0.1869\n",
      "Epoch 6/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.9031 - soft_acc: 0.1749 - val_loss: 3.8058 - val_soft_acc: 0.1844\n",
      "Epoch 7/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.8815 - soft_acc: 0.1778 - val_loss: 3.7203 - val_soft_acc: 0.1838\n",
      "Epoch 8/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.8315 - soft_acc: 0.1766 - val_loss: 4.1401 - val_soft_acc: 0.1809\n",
      "Epoch 9/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.8237 - soft_acc: 0.1769 - val_loss: 3.6904 - val_soft_acc: 0.1664\n",
      "Epoch 10/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.7752 - soft_acc: 0.1816 - val_loss: 3.7334 - val_soft_acc: 0.1869\n",
      "Epoch 11/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.7405 - soft_acc: 0.1797 - val_loss: 3.6606 - val_soft_acc: 0.1730\n",
      "Epoch 12/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.7468 - soft_acc: 0.1814 - val_loss: 3.6390 - val_soft_acc: 0.1797\n",
      "Epoch 13/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.7204 - soft_acc: 0.1805 - val_loss: 3.5950 - val_soft_acc: 0.1957\n",
      "Epoch 14/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 3.7228 - soft_acc: 0.18 - 1s 74us/step - loss: 3.7311 - soft_acc: 0.1845 - val_loss: 3.7676 - val_soft_acc: 0.1709\n",
      "Epoch 15/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.6845 - soft_acc: 0.1804 - val_loss: 3.6612 - val_soft_acc: 0.1787\n",
      "Epoch 16/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.6555 - soft_acc: 0.1814 - val_loss: 3.5156 - val_soft_acc: 0.1891\n",
      "Epoch 17/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.6031 - soft_acc: 0.1883 - val_loss: 3.6168 - val_soft_acc: 0.1709\n",
      "Epoch 18/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.6207 - soft_acc: 0.1886 - val_loss: 3.5005 - val_soft_acc: 0.1873\n",
      "Epoch 19/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.5408 - soft_acc: 0.1874 - val_loss: 3.4950 - val_soft_acc: 0.1914\n",
      "Epoch 20/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.5746 - soft_acc: 0.1895 - val_loss: 3.4888 - val_soft_acc: 0.2000\n",
      "Epoch 21/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.5263 - soft_acc: 0.1922 - val_loss: 3.4132 - val_soft_acc: 0.1934\n",
      "Epoch 22/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.5210 - soft_acc: 0.1931 - val_loss: 3.4862 - val_soft_acc: 0.1885\n",
      "Epoch 23/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.4990 - soft_acc: 0.1944 - val_loss: 3.4308 - val_soft_acc: 0.2000\n",
      "Epoch 24/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.4862 - soft_acc: 0.1941 - val_loss: 3.4765 - val_soft_acc: 0.2053\n",
      "Epoch 25/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.4405 - soft_acc: 0.1985 - val_loss: 3.3670 - val_soft_acc: 0.2041\n",
      "Epoch 26/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.4560 - soft_acc: 0.1947 - val_loss: 3.4712 - val_soft_acc: 0.2115\n",
      "Epoch 27/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.4427 - soft_acc: 0.1954 - val_loss: 3.3666 - val_soft_acc: 0.1885\n",
      "Epoch 28/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.4153 - soft_acc: 0.1992 - val_loss: 3.3650 - val_soft_acc: 0.2145\n",
      "Epoch 29/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.3755 - soft_acc: 0.2003 - val_loss: 3.3949 - val_soft_acc: 0.2043\n",
      "Epoch 30/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.3819 - soft_acc: 0.2024 - val_loss: 3.3030 - val_soft_acc: 0.2057\n",
      "Epoch 31/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.3319 - soft_acc: 0.2029 - val_loss: 3.4132 - val_soft_acc: 0.1928\n",
      "Epoch 32/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.3322 - soft_acc: 0.2053 - val_loss: 3.3619 - val_soft_acc: 0.2158\n",
      "Epoch 33/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.3068 - soft_acc: 0.2091 - val_loss: 3.2348 - val_soft_acc: 0.2033\n",
      "Epoch 34/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.2944 - soft_acc: 0.2078 - val_loss: 3.2067 - val_soft_acc: 0.2074\n",
      "Epoch 35/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.2910 - soft_acc: 0.2088 - val_loss: 3.2055 - val_soft_acc: 0.2037\n",
      "Epoch 36/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.2858 - soft_acc: 0.2070 - val_loss: 3.2073 - val_soft_acc: 0.2143\n",
      "Epoch 37/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.2445 - soft_acc: 0.2118 - val_loss: 3.2578 - val_soft_acc: 0.1982\n",
      "Epoch 38/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.2442 - soft_acc: 0.2114 - val_loss: 3.1662 - val_soft_acc: 0.2166\n",
      "Epoch 39/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.2172 - soft_acc: 0.2124 - val_loss: 3.2774 - val_soft_acc: 0.2166\n",
      "Epoch 40/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.1947 - soft_acc: 0.2157 - val_loss: 3.2034 - val_soft_acc: 0.2236\n",
      "Epoch 41/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.1850 - soft_acc: 0.2176 - val_loss: 3.1326 - val_soft_acc: 0.2191\n",
      "Epoch 42/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.1578 - soft_acc: 0.2202 - val_loss: 3.1117 - val_soft_acc: 0.2297\n",
      "Epoch 43/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.1284 - soft_acc: 0.2209 - val_loss: 3.1019 - val_soft_acc: 0.2189\n",
      "Epoch 44/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.1192 - soft_acc: 0.2177 - val_loss: 3.1744 - val_soft_acc: 0.2193\n",
      "Epoch 45/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.1115 - soft_acc: 0.2225 - val_loss: 3.1443 - val_soft_acc: 0.2361\n",
      "Epoch 46/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.1012 - soft_acc: 0.2230 - val_loss: 3.0211 - val_soft_acc: 0.2316\n",
      "Epoch 47/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.0545 - soft_acc: 0.2250 - val_loss: 3.0200 - val_soft_acc: 0.2322\n",
      "Epoch 48/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.0581 - soft_acc: 0.2256 - val_loss: 3.0279 - val_soft_acc: 0.2277\n",
      "Epoch 49/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.0442 - soft_acc: 0.2257 - val_loss: 3.0790 - val_soft_acc: 0.2262\n",
      "Epoch 50/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.9651 - soft_acc: 0.2315 - val_loss: 3.1077 - val_soft_acc: 0.2070\n",
      "Epoch 51/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.9885 - soft_acc: 0.2296 - val_loss: 2.9556 - val_soft_acc: 0.2506\n",
      "Epoch 52/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.9710 - soft_acc: 0.2320 - val_loss: 2.9808 - val_soft_acc: 0.2342\n",
      "Epoch 53/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.9596 - soft_acc: 0.2336 - val_loss: 3.1177 - val_soft_acc: 0.2463\n",
      "Epoch 54/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.9150 - soft_acc: 0.2351 - val_loss: 2.9833 - val_soft_acc: 0.2336\n",
      "Epoch 55/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.9197 - soft_acc: 0.2385 - val_loss: 2.9505 - val_soft_acc: 0.2393\n",
      "Epoch 56/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.8777 - soft_acc: 0.2442 - val_loss: 2.9476 - val_soft_acc: 0.2471\n",
      "Epoch 57/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.8798 - soft_acc: 0.2393 - val_loss: 2.8729 - val_soft_acc: 0.2465\n",
      "Epoch 58/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.8222 - soft_acc: 0.2450 - val_loss: 2.8890 - val_soft_acc: 0.2502\n",
      "Epoch 59/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.8116 - soft_acc: 0.2463 - val_loss: 2.8755 - val_soft_acc: 0.2551\n",
      "Epoch 60/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.7880 - soft_acc: 0.2486 - val_loss: 2.9159 - val_soft_acc: 0.2441\n",
      "Epoch 61/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.7727 - soft_acc: 0.2502 - val_loss: 2.8145 - val_soft_acc: 0.2684\n",
      "Epoch 62/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.7051 - soft_acc: 0.2538 - val_loss: 2.8971 - val_soft_acc: 0.2477\n",
      "Epoch 63/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.7096 - soft_acc: 0.2583 - val_loss: 2.7753 - val_soft_acc: 0.2609\n",
      "Epoch 64/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.7048 - soft_acc: 0.2567 - val_loss: 2.8332 - val_soft_acc: 0.2627\n",
      "Epoch 65/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.6426 - soft_acc: 0.2654 - val_loss: 2.7783 - val_soft_acc: 0.2570\n",
      "Epoch 66/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.6664 - soft_acc: 0.2591 - val_loss: 2.6665 - val_soft_acc: 0.2582\n",
      "Epoch 67/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.6166 - soft_acc: 0.2638 - val_loss: 2.7247 - val_soft_acc: 0.2650\n",
      "Epoch 68/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.6282 - soft_acc: 0.2664 - val_loss: 2.7812 - val_soft_acc: 0.2621\n",
      "Epoch 69/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.5650 - soft_acc: 0.2697 - val_loss: 2.6275 - val_soft_acc: 0.2650\n",
      "Epoch 70/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.5480 - soft_acc: 0.2700 - val_loss: 2.7661 - val_soft_acc: 0.2590\n",
      "Epoch 71/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.5203 - soft_acc: 0.2772 - val_loss: 2.6505 - val_soft_acc: 0.2564\n",
      "Epoch 72/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.4882 - soft_acc: 0.2774 - val_loss: 2.6244 - val_soft_acc: 0.2672\n",
      "Epoch 73/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.4275 - soft_acc: 0.2847 - val_loss: 2.5923 - val_soft_acc: 0.2842\n",
      "Epoch 74/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.4520 - soft_acc: 0.2809 - val_loss: 2.6182 - val_soft_acc: 0.2785\n",
      "Epoch 75/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.4084 - soft_acc: 0.2874 - val_loss: 2.5661 - val_soft_acc: 0.2697\n",
      "Epoch 76/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.3613 - soft_acc: 0.2887 - val_loss: 2.6125 - val_soft_acc: 0.2734\n",
      "Epoch 77/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.3481 - soft_acc: 0.2940 - val_loss: 2.5641 - val_soft_acc: 0.2760\n",
      "Epoch 78/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.3305 - soft_acc: 0.2943 - val_loss: 2.5506 - val_soft_acc: 0.2885\n",
      "Epoch 79/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.2594 - soft_acc: 0.3022 - val_loss: 2.5223 - val_soft_acc: 0.2795\n",
      "Epoch 80/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.2680 - soft_acc: 0.3014 - val_loss: 2.4495 - val_soft_acc: 0.2914\n",
      "Epoch 81/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.2093 - soft_acc: 0.3054 - val_loss: 2.5341 - val_soft_acc: 0.2846\n",
      "Epoch 82/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.1911 - soft_acc: 0.3093 - val_loss: 2.5355 - val_soft_acc: 0.2867\n",
      "Epoch 83/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.1814 - soft_acc: 0.3104 - val_loss: 2.4970 - val_soft_acc: 0.2865\n",
      "Epoch 84/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.1081 - soft_acc: 0.3220 - val_loss: 2.4194 - val_soft_acc: 0.3006\n",
      "Epoch 85/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.0923 - soft_acc: 0.3212 - val_loss: 2.3432 - val_soft_acc: 0.3023\n",
      "Epoch 86/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.0769 - soft_acc: 0.3226 - val_loss: 2.4104 - val_soft_acc: 0.2891\n",
      "Epoch 87/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.0430 - soft_acc: 0.3272 - val_loss: 2.3654 - val_soft_acc: 0.3074\n",
      "Epoch 88/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.0397 - soft_acc: 0.3300 - val_loss: 2.3910 - val_soft_acc: 0.3061\n",
      "Epoch 89/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.0009 - soft_acc: 0.3342 - val_loss: 2.3864 - val_soft_acc: 0.3094\n",
      "Epoch 90/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.9491 - soft_acc: 0.3379 - val_loss: 2.3055 - val_soft_acc: 0.3260\n",
      "Epoch 91/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.9649 - soft_acc: 0.3375 - val_loss: 2.2858 - val_soft_acc: 0.3102\n",
      "Epoch 92/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.8806 - soft_acc: 0.3492 - val_loss: 2.4138 - val_soft_acc: 0.3141\n",
      "Epoch 93/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.8864 - soft_acc: 0.3501 - val_loss: 2.3439 - val_soft_acc: 0.3100\n",
      "Epoch 94/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.8500 - soft_acc: 0.3487 - val_loss: 2.2997 - val_soft_acc: 0.3336\n",
      "Epoch 95/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.8043 - soft_acc: 0.3603 - val_loss: 2.2725 - val_soft_acc: 0.3217\n",
      "Epoch 96/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.7493 - soft_acc: 0.3653 - val_loss: 2.1594 - val_soft_acc: 0.3416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.7812 - soft_acc: 0.3606 - val_loss: 2.1616 - val_soft_acc: 0.3477\n",
      "Epoch 98/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.6795 - soft_acc: 0.3796 - val_loss: 2.2694 - val_soft_acc: 0.3367\n",
      "Epoch 99/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.7313 - soft_acc: 0.3700 - val_loss: 2.1132 - val_soft_acc: 0.3430\n",
      "Epoch 100/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.6330 - soft_acc: 0.3880 - val_loss: 2.1385 - val_soft_acc: 0.3318\n",
      "Epoch 101/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.6663 - soft_acc: 0.3804 - val_loss: 2.1015 - val_soft_acc: 0.3568\n",
      "Epoch 102/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.6082 - soft_acc: 0.3847 - val_loss: 2.1692 - val_soft_acc: 0.3424\n",
      "Epoch 103/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.5802 - soft_acc: 0.3952 - val_loss: 2.0991 - val_soft_acc: 0.3527\n",
      "Epoch 104/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.5475 - soft_acc: 0.3947 - val_loss: 2.0850 - val_soft_acc: 0.3576\n",
      "Epoch 105/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.5187 - soft_acc: 0.4028 - val_loss: 2.1213 - val_soft_acc: 0.3625\n",
      "Epoch 106/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.5016 - soft_acc: 0.4028 - val_loss: 2.0388 - val_soft_acc: 0.3541\n",
      "Epoch 107/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.4509 - soft_acc: 0.4135 - val_loss: 2.0419 - val_soft_acc: 0.3721\n",
      "Epoch 108/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.4508 - soft_acc: 0.4095 - val_loss: 2.0814 - val_soft_acc: 0.3598\n",
      "Epoch 109/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.4704 - soft_acc: 0.4083 - val_loss: 2.0353 - val_soft_acc: 0.3746\n",
      "Epoch 110/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.3992 - soft_acc: 0.4176 - val_loss: 2.0696 - val_soft_acc: 0.3643\n",
      "Epoch 111/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.4142 - soft_acc: 0.4227 - val_loss: 2.0607 - val_soft_acc: 0.3684\n",
      "Epoch 112/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.4473 - soft_acc: 0.4156 - val_loss: 2.0141 - val_soft_acc: 0.3783\n",
      "Epoch 113/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.3789 - soft_acc: 0.4284 - val_loss: 1.9467 - val_soft_acc: 0.3730\n",
      "Epoch 114/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.3750 - soft_acc: 0.4283 - val_loss: 1.8880 - val_soft_acc: 0.3830\n",
      "Epoch 115/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.2435 - soft_acc: 0.4537 - val_loss: 1.7982 - val_soft_acc: 0.4012\n",
      "Epoch 116/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.2763 - soft_acc: 0.4475 - val_loss: 1.9641 - val_soft_acc: 0.3910\n",
      "Epoch 117/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.2436 - soft_acc: 0.4532 - val_loss: 1.9305 - val_soft_acc: 0.3834\n",
      "Epoch 118/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.2588 - soft_acc: 0.4501 - val_loss: 1.8691 - val_soft_acc: 0.4014\n",
      "Epoch 119/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.2087 - soft_acc: 0.4565 - val_loss: 1.8446 - val_soft_acc: 0.4072\n",
      "Epoch 120/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.2330 - soft_acc: 0.4551 - val_loss: 2.0714 - val_soft_acc: 0.3777\n",
      "Epoch 121/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.2191 - soft_acc: 0.4545 - val_loss: 1.8478 - val_soft_acc: 0.4123\n",
      "Epoch 122/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.1566 - soft_acc: 0.4724 - val_loss: 1.7946 - val_soft_acc: 0.4266\n",
      "Epoch 123/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.0966 - soft_acc: 0.4859 - val_loss: 1.8784 - val_soft_acc: 0.4096\n",
      "Epoch 124/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.1240 - soft_acc: 0.4728 - val_loss: 1.8168 - val_soft_acc: 0.3994\n",
      "Epoch 125/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.1191 - soft_acc: 0.4832 - val_loss: 1.8136 - val_soft_acc: 0.4189\n",
      "Epoch 126/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.1055 - soft_acc: 0.4840 - val_loss: 1.7767 - val_soft_acc: 0.4121\n",
      "Epoch 127/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.0226 - soft_acc: 0.4943 - val_loss: 1.8342 - val_soft_acc: 0.4105\n",
      "Epoch 128/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.0244 - soft_acc: 0.5005 - val_loss: 1.8195 - val_soft_acc: 0.3924\n",
      "Epoch 129/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.0445 - soft_acc: 0.4950 - val_loss: 1.8034 - val_soft_acc: 0.4270\n",
      "Epoch 130/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.0369 - soft_acc: 0.5081 - val_loss: 1.7984 - val_soft_acc: 0.4242\n",
      "Epoch 131/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.2037 - soft_acc: 0.4663 - val_loss: 1.6769 - val_soft_acc: 0.4170\n",
      "Epoch 132/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.0095 - soft_acc: 0.5080 - val_loss: 1.6284 - val_soft_acc: 0.4406\n",
      "Epoch 133/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9850 - soft_acc: 0.5141 - val_loss: 1.7851 - val_soft_acc: 0.4295\n",
      "Epoch 134/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9023 - soft_acc: 0.5328 - val_loss: 1.9451 - val_soft_acc: 0.4068\n",
      "Epoch 135/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9925 - soft_acc: 0.5159 - val_loss: 1.7630 - val_soft_acc: 0.4352\n",
      "Epoch 136/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9540 - soft_acc: 0.5209 - val_loss: 1.7535 - val_soft_acc: 0.4279\n",
      "Epoch 137/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9322 - soft_acc: 0.5198 - val_loss: 1.7337 - val_soft_acc: 0.4482\n",
      "Epoch 138/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9401 - soft_acc: 0.5261 - val_loss: 1.7530 - val_soft_acc: 0.4301\n",
      "Epoch 139/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8686 - soft_acc: 0.5469 - val_loss: 1.7525 - val_soft_acc: 0.4391\n",
      "Epoch 140/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.9626 - soft_acc: 0.5230 - val_loss: 1.6675 - val_soft_acc: 0.4451\n",
      "Epoch 141/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8416 - soft_acc: 0.5457 - val_loss: 1.7081 - val_soft_acc: 0.4537\n",
      "Epoch 142/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8039 - soft_acc: 0.5653 - val_loss: 1.7366 - val_soft_acc: 0.4461\n",
      "Epoch 143/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8498 - soft_acc: 0.5514 - val_loss: 1.7338 - val_soft_acc: 0.4184\n",
      "Epoch 144/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.8761 - soft_acc: 0.5415 - val_loss: 1.7913 - val_soft_acc: 0.4326\n",
      "Epoch 145/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8832 - soft_acc: 0.5453 - val_loss: 1.7080 - val_soft_acc: 0.4443\n",
      "Epoch 146/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.9560 - soft_acc: 0.5235 - val_loss: 1.7237 - val_soft_acc: 0.4395\n",
      "Epoch 147/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.8786 - soft_acc: 0.5384 - val_loss: 1.6039 - val_soft_acc: 0.4773\n",
      "Epoch 148/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7177 - soft_acc: 0.5828 - val_loss: 1.6339 - val_soft_acc: 0.4693\n",
      "Epoch 149/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7150 - soft_acc: 0.5973 - val_loss: 1.5860 - val_soft_acc: 0.4791\n",
      "Epoch 150/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7521 - soft_acc: 0.5770 - val_loss: 1.5619 - val_soft_acc: 0.4916\n",
      "Epoch 151/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6681 - soft_acc: 0.6062 - val_loss: 1.6132 - val_soft_acc: 0.4730\n",
      "Epoch 152/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7360 - soft_acc: 0.5864 - val_loss: 1.6241 - val_soft_acc: 0.4697\n",
      "Epoch 153/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7273 - soft_acc: 0.5786 - val_loss: 1.5829 - val_soft_acc: 0.4803\n",
      "Epoch 154/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7081 - soft_acc: 0.5905 - val_loss: 1.6403 - val_soft_acc: 0.4721\n",
      "Epoch 155/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7560 - soft_acc: 0.5733 - val_loss: 1.6732 - val_soft_acc: 0.4572\n",
      "Epoch 156/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7201 - soft_acc: 0.5840 - val_loss: 1.6174 - val_soft_acc: 0.4715\n",
      "Epoch 157/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8577 - soft_acc: 0.5488 - val_loss: 1.9470 - val_soft_acc: 0.4307\n",
      "Epoch 158/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8626 - soft_acc: 0.5445 - val_loss: 1.6463 - val_soft_acc: 0.4814\n",
      "Epoch 159/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6865 - soft_acc: 0.5984 - val_loss: 1.6530 - val_soft_acc: 0.4633\n",
      "Epoch 160/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6667 - soft_acc: 0.6054 - val_loss: 1.5504 - val_soft_acc: 0.4695\n",
      "Epoch 161/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6556 - soft_acc: 0.6138 - val_loss: 1.5529 - val_soft_acc: 0.4760\n",
      "Epoch 162/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6347 - soft_acc: 0.6139 - val_loss: 1.5605 - val_soft_acc: 0.4980\n",
      "Epoch 163/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5474 - soft_acc: 0.6477 - val_loss: 1.5035 - val_soft_acc: 0.4893\n",
      "Epoch 164/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5654 - soft_acc: 0.6441 - val_loss: 1.5412 - val_soft_acc: 0.4816\n",
      "Epoch 165/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7695 - soft_acc: 0.5789 - val_loss: 1.5726 - val_soft_acc: 0.4709\n",
      "Epoch 166/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6616 - soft_acc: 0.6077 - val_loss: 1.5055 - val_soft_acc: 0.4877\n",
      "Epoch 167/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6191 - soft_acc: 0.6273 - val_loss: 1.5145 - val_soft_acc: 0.4996\n",
      "Epoch 168/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6003 - soft_acc: 0.6330 - val_loss: 1.4813 - val_soft_acc: 0.4893\n",
      "Epoch 169/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6266 - soft_acc: 0.6280 - val_loss: 1.6041 - val_soft_acc: 0.4840\n",
      "Epoch 170/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6552 - soft_acc: 0.6077 - val_loss: 1.4471 - val_soft_acc: 0.5049\n",
      "Epoch 171/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6027 - soft_acc: 0.6371 - val_loss: 1.7337 - val_soft_acc: 0.4582\n",
      "Epoch 172/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6064 - soft_acc: 0.6247 - val_loss: 1.5897 - val_soft_acc: 0.4826\n",
      "Epoch 173/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6170 - soft_acc: 0.6199 - val_loss: 1.4862 - val_soft_acc: 0.5023\n",
      "Epoch 174/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4939 - soft_acc: 0.6667 - val_loss: 1.4219 - val_soft_acc: 0.5316\n",
      "Epoch 175/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4837 - soft_acc: 0.6807 - val_loss: 1.4164 - val_soft_acc: 0.5316\n",
      "Epoch 176/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4865 - soft_acc: 0.6725 - val_loss: 1.5333 - val_soft_acc: 0.5094\n",
      "Epoch 177/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7796 - soft_acc: 0.5781 - val_loss: 1.7096 - val_soft_acc: 0.4547\n",
      "Epoch 178/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6352 - soft_acc: 0.6122 - val_loss: 1.5036 - val_soft_acc: 0.5068\n",
      "Epoch 179/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5210 - soft_acc: 0.6567 - val_loss: 1.6191 - val_soft_acc: 0.4975\n",
      "Epoch 180/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5898 - soft_acc: 0.6295 - val_loss: 1.5875 - val_soft_acc: 0.4770\n",
      "Epoch 181/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4925 - soft_acc: 0.6654 - val_loss: 1.4392 - val_soft_acc: 0.5299\n",
      "Epoch 182/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4352 - soft_acc: 0.7049 - val_loss: 1.4635 - val_soft_acc: 0.5273\n",
      "Epoch 183/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6129 - soft_acc: 0.6307 - val_loss: 1.5471 - val_soft_acc: 0.5172\n",
      "Epoch 184/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4944 - soft_acc: 0.6727 - val_loss: 1.5433 - val_soft_acc: 0.5139\n",
      "Epoch 185/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4297 - soft_acc: 0.7019 - val_loss: 1.3630 - val_soft_acc: 0.5391\n",
      "Epoch 186/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4330 - soft_acc: 0.7060 - val_loss: 1.4738 - val_soft_acc: 0.5236\n",
      "Epoch 187/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5815 - soft_acc: 0.6367 - val_loss: 1.6470 - val_soft_acc: 0.4883\n",
      "Epoch 188/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5928 - soft_acc: 0.6335 - val_loss: 1.6131 - val_soft_acc: 0.4996\n",
      "Epoch 189/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5714 - soft_acc: 0.6383 - val_loss: 1.4865 - val_soft_acc: 0.5252\n",
      "Epoch 190/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5087 - soft_acc: 0.6639 - val_loss: 1.5690 - val_soft_acc: 0.5066\n",
      "Epoch 191/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5006 - soft_acc: 0.6717 - val_loss: 1.5331 - val_soft_acc: 0.5162\n",
      "Epoch 192/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4991 - soft_acc: 0.6723 - val_loss: 1.4629 - val_soft_acc: 0.5367\n",
      "Epoch 193/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4957 - soft_acc: 0.6721 - val_loss: 1.5639 - val_soft_acc: 0.5217\n",
      "Epoch 194/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5135 - soft_acc: 0.6634 - val_loss: 1.5034 - val_soft_acc: 0.5156\n",
      "Epoch 195/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.4132 - soft_acc: 0.7141 - val_loss: 1.4493 - val_soft_acc: 0.5270\n",
      "Epoch 196/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5159 - soft_acc: 0.6767 - val_loss: 1.5652 - val_soft_acc: 0.5160\n",
      "Epoch 197/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4837 - soft_acc: 0.6784 - val_loss: 1.4650 - val_soft_acc: 0.5348\n",
      "Epoch 198/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4302 - soft_acc: 0.6984 - val_loss: 1.5219 - val_soft_acc: 0.5094\n",
      "Epoch 199/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3729 - soft_acc: 0.7303 - val_loss: 1.4145 - val_soft_acc: 0.5469\n",
      "Epoch 200/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3640 - soft_acc: 0.7428 - val_loss: 1.4506 - val_soft_acc: 0.5424\n",
      "Epoch 201/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4241 - soft_acc: 0.7196 - val_loss: 1.4305 - val_soft_acc: 0.5357\n",
      "Epoch 202/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4659 - soft_acc: 0.6890 - val_loss: 1.6105 - val_soft_acc: 0.5061\n",
      "Epoch 203/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5987 - soft_acc: 0.6285 - val_loss: 1.5521 - val_soft_acc: 0.5061\n",
      "Epoch 204/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5292 - soft_acc: 0.6531 - val_loss: 1.4843 - val_soft_acc: 0.5283\n",
      "Epoch 205/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5942 - soft_acc: 0.6335 - val_loss: 1.4382 - val_soft_acc: 0.5217\n",
      "Epoch 206/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4111 - soft_acc: 0.6988 - val_loss: 1.3731 - val_soft_acc: 0.5510\n",
      "Epoch 207/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3596 - soft_acc: 0.7311 - val_loss: 1.4529 - val_soft_acc: 0.5375\n",
      "Epoch 208/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3606 - soft_acc: 0.7369 - val_loss: 1.5274 - val_soft_acc: 0.5352\n",
      "Epoch 209/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4365 - soft_acc: 0.7084 - val_loss: 1.3822 - val_soft_acc: 0.5650\n",
      "Epoch 210/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3374 - soft_acc: 0.7530 - val_loss: 1.5256 - val_soft_acc: 0.5445\n",
      "Epoch 211/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5601 - soft_acc: 0.6550 - val_loss: 1.6046 - val_soft_acc: 0.4994\n",
      "Epoch 212/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5310 - soft_acc: 0.6610 - val_loss: 1.5277 - val_soft_acc: 0.5234\n",
      "Epoch 213/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3601 - soft_acc: 0.7256 - val_loss: 1.3880 - val_soft_acc: 0.5676\n",
      "Epoch 214/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2621 - soft_acc: 0.7850 - val_loss: 1.2835 - val_soft_acc: 0.5936\n",
      "Epoch 215/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2343 - soft_acc: 0.8133 - val_loss: 1.2878 - val_soft_acc: 0.5932\n",
      "Epoch 216/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3047 - soft_acc: 0.7766 - val_loss: 1.4369 - val_soft_acc: 0.5488\n",
      "Epoch 217/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3636 - soft_acc: 0.7429 - val_loss: 1.7789 - val_soft_acc: 0.5084\n",
      "Epoch 218/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5764 - soft_acc: 0.6446 - val_loss: 1.6286 - val_soft_acc: 0.4791\n",
      "Epoch 219/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6487 - soft_acc: 0.6190 - val_loss: 1.5817 - val_soft_acc: 0.5047\n",
      "Epoch 220/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3545 - soft_acc: 0.7257 - val_loss: 1.3670 - val_soft_acc: 0.5527\n",
      "Epoch 221/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2706 - soft_acc: 0.7775 - val_loss: 1.2661 - val_soft_acc: 0.5830\n",
      "Epoch 222/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2374 - soft_acc: 0.8048 - val_loss: 1.2729 - val_soft_acc: 0.5891\n",
      "Epoch 223/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2859 - soft_acc: 0.7924 - val_loss: 1.3978 - val_soft_acc: 0.5553\n",
      "Epoch 224/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3960 - soft_acc: 0.7151 - val_loss: 1.4103 - val_soft_acc: 0.5371\n",
      "Epoch 225/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6935 - soft_acc: 0.6148 - val_loss: 1.6373 - val_soft_acc: 0.4939\n",
      "Epoch 226/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4412 - soft_acc: 0.6974 - val_loss: 1.3685 - val_soft_acc: 0.5592\n",
      "Epoch 227/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3062 - soft_acc: 0.7591 - val_loss: 1.4289 - val_soft_acc: 0.5496\n",
      "Epoch 228/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2670 - soft_acc: 0.7798 - val_loss: 1.2430 - val_soft_acc: 0.5932\n",
      "Epoch 229/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1963 - soft_acc: 0.8304 - val_loss: 1.2178 - val_soft_acc: 0.6041\n",
      "Epoch 230/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2289 - soft_acc: 0.8234 - val_loss: 1.4225 - val_soft_acc: 0.5684\n",
      "Epoch 231/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5286 - soft_acc: 0.6741 - val_loss: 1.6556 - val_soft_acc: 0.4930\n",
      "Epoch 232/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5098 - soft_acc: 0.6674 - val_loss: 1.7428 - val_soft_acc: 0.4814\n",
      "Epoch 233/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4204 - soft_acc: 0.7000 - val_loss: 1.3296 - val_soft_acc: 0.5527\n",
      "Epoch 234/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2645 - soft_acc: 0.7791 - val_loss: 1.2589 - val_soft_acc: 0.6047\n",
      "Epoch 235/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2358 - soft_acc: 0.8177 - val_loss: 1.3289 - val_soft_acc: 0.5822\n",
      "Epoch 236/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3320 - soft_acc: 0.7793 - val_loss: 1.7844 - val_soft_acc: 0.4834\n",
      "Epoch 237/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5662 - soft_acc: 0.6548 - val_loss: 1.5586 - val_soft_acc: 0.5287\n",
      "Epoch 238/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3488 - soft_acc: 0.7275 - val_loss: 1.3734 - val_soft_acc: 0.5531\n",
      "Epoch 239/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2724 - soft_acc: 0.7873 - val_loss: 1.3184 - val_soft_acc: 0.5918\n",
      "Epoch 240/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2182 - soft_acc: 0.8147 - val_loss: 1.2510 - val_soft_acc: 0.6078\n",
      "Epoch 241/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1948 - soft_acc: 0.8378 - val_loss: 1.2272 - val_soft_acc: 0.5885\n",
      "Epoch 242/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2134 - soft_acc: 0.8360 - val_loss: 1.2112 - val_soft_acc: 0.6055\n",
      "Epoch 243/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3127 - soft_acc: 0.7774 - val_loss: 1.4043 - val_soft_acc: 0.5348\n",
      "Epoch 244/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.4965 - soft_acc: 0.6819 - val_loss: 1.5078 - val_soft_acc: 0.5258\n",
      "Epoch 245/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.7057 - soft_acc: 0.6154 - val_loss: 1.8504 - val_soft_acc: 0.4693\n",
      "Epoch 246/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4233 - soft_acc: 0.6904 - val_loss: 1.3587 - val_soft_acc: 0.5566\n",
      "Epoch 247/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2343 - soft_acc: 0.7944 - val_loss: 1.3363 - val_soft_acc: 0.5840\n",
      "Epoch 248/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2217 - soft_acc: 0.8166 - val_loss: 1.2736 - val_soft_acc: 0.5885\n",
      "Epoch 249/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1641 - soft_acc: 0.8528 - val_loss: 1.1595 - val_soft_acc: 0.6217\n",
      "Epoch 250/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1463 - soft_acc: 0.8666 - val_loss: 1.1611 - val_soft_acc: 0.6270\n",
      "Epoch 251/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1765 - soft_acc: 0.8504 - val_loss: 1.1891 - val_soft_acc: 0.6162\n",
      "Epoch 252/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3908 - soft_acc: 0.7301 - val_loss: 1.6129 - val_soft_acc: 0.5051\n",
      "Epoch 253/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5135 - soft_acc: 0.6645 - val_loss: 1.6165 - val_soft_acc: 0.5010\n",
      "Epoch 254/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5559 - soft_acc: 0.6527 - val_loss: 1.4854 - val_soft_acc: 0.5363\n",
      "Epoch 255/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2988 - soft_acc: 0.7540 - val_loss: 1.3939 - val_soft_acc: 0.5582\n",
      "Epoch 256/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2120 - soft_acc: 0.8179 - val_loss: 1.2705 - val_soft_acc: 0.6090\n",
      "Epoch 257/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2720 - soft_acc: 0.7996 - val_loss: 1.3758 - val_soft_acc: 0.5781\n",
      "Epoch 258/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2981 - soft_acc: 0.7765 - val_loss: 1.3091 - val_soft_acc: 0.5725\n",
      "Epoch 259/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1936 - soft_acc: 0.8256 - val_loss: 1.2578 - val_soft_acc: 0.6154\n",
      "Epoch 260/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1748 - soft_acc: 0.8467 - val_loss: 1.2069 - val_soft_acc: 0.6264\n",
      "Epoch 261/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.1511 - soft_acc: 0.86 - 1s 74us/step - loss: 0.1535 - soft_acc: 0.8678 - val_loss: 1.2837 - val_soft_acc: 0.6039\n",
      "Epoch 262/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1964 - soft_acc: 0.8420 - val_loss: 1.3848 - val_soft_acc: 0.5828\n",
      "Epoch 263/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2256 - soft_acc: 0.8183 - val_loss: 1.5400 - val_soft_acc: 0.5357\n",
      "Epoch 264/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3295 - soft_acc: 0.7452 - val_loss: 1.4653 - val_soft_acc: 0.5346\n",
      "Epoch 265/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4952 - soft_acc: 0.6691 - val_loss: 1.8401 - val_soft_acc: 0.4709\n",
      "Epoch 266/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7094 - soft_acc: 0.5991 - val_loss: 1.5137 - val_soft_acc: 0.5201\n",
      "Epoch 267/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3461 - soft_acc: 0.7294 - val_loss: 1.4073 - val_soft_acc: 0.5658\n",
      "Epoch 268/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1902 - soft_acc: 0.8186 - val_loss: 1.1648 - val_soft_acc: 0.6219\n",
      "Epoch 269/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1503 - soft_acc: 0.8616 - val_loss: 1.2369 - val_soft_acc: 0.6020\n",
      "Epoch 270/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1638 - soft_acc: 0.8630 - val_loss: 1.2765 - val_soft_acc: 0.5941\n",
      "Epoch 271/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1588 - soft_acc: 0.8582 - val_loss: 1.2341 - val_soft_acc: 0.6193\n",
      "Epoch 272/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1378 - soft_acc: 0.8767 - val_loss: 1.1764 - val_soft_acc: 0.6174\n",
      "Epoch 273/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1428 - soft_acc: 0.8702 - val_loss: 1.2252 - val_soft_acc: 0.6170\n",
      "Epoch 274/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2502 - soft_acc: 0.8006 - val_loss: 1.5713 - val_soft_acc: 0.5318\n",
      "Epoch 275/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5756 - soft_acc: 0.6488 - val_loss: 1.6052 - val_soft_acc: 0.4992\n",
      "Epoch 276/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4698 - soft_acc: 0.6858 - val_loss: 1.6343 - val_soft_acc: 0.5125\n",
      "Epoch 277/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3405 - soft_acc: 0.7331 - val_loss: 1.3484 - val_soft_acc: 0.5746\n",
      "Epoch 278/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1670 - soft_acc: 0.8387 - val_loss: 1.2079 - val_soft_acc: 0.6223\n",
      "Epoch 279/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1511 - soft_acc: 0.8588 - val_loss: 1.1927 - val_soft_acc: 0.6311\n",
      "Epoch 280/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1090 - soft_acc: 0.8913 - val_loss: 1.2038 - val_soft_acc: 0.6322\n",
      "Epoch 281/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1858 - soft_acc: 0.8587 - val_loss: 1.4661 - val_soft_acc: 0.5494\n",
      "Epoch 282/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5697 - soft_acc: 0.6569 - val_loss: 1.8437 - val_soft_acc: 0.4898\n",
      "Epoch 283/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3997 - soft_acc: 0.7078 - val_loss: 1.5124 - val_soft_acc: 0.5289\n",
      "Epoch 284/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2947 - soft_acc: 0.7594 - val_loss: 1.3348 - val_soft_acc: 0.5850\n",
      "Epoch 285/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1943 - soft_acc: 0.8249 - val_loss: 1.2162 - val_soft_acc: 0.6238\n",
      "Epoch 286/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0972 - soft_acc: 0.8913 - val_loss: 1.1454 - val_soft_acc: 0.6393\n",
      "Epoch 287/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1216 - soft_acc: 0.8809 - val_loss: 1.2575 - val_soft_acc: 0.6264\n",
      "Epoch 288/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1711 - soft_acc: 0.8578 - val_loss: 1.3205 - val_soft_acc: 0.6139\n",
      "Epoch 289/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3236 - soft_acc: 0.7724 - val_loss: 1.6003 - val_soft_acc: 0.5129\n",
      "Epoch 290/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4717 - soft_acc: 0.6880 - val_loss: 1.5361 - val_soft_acc: 0.5590\n",
      "Epoch 291/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2933 - soft_acc: 0.7664 - val_loss: 1.2724 - val_soft_acc: 0.6180\n",
      "Epoch 292/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1601 - soft_acc: 0.8433 - val_loss: 1.3374 - val_soft_acc: 0.5939\n",
      "Epoch 293/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2028 - soft_acc: 0.8286 - val_loss: 1.2655 - val_soft_acc: 0.6115\n",
      "Epoch 294/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1946 - soft_acc: 0.8327 - val_loss: 1.2201 - val_soft_acc: 0.6373\n",
      "Epoch 295/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1279 - soft_acc: 0.8823 - val_loss: 1.2631 - val_soft_acc: 0.6270\n",
      "Epoch 296/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2124 - soft_acc: 0.8380 - val_loss: 1.5069 - val_soft_acc: 0.5867\n",
      "Epoch 297/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2744 - soft_acc: 0.7859 - val_loss: 1.2895 - val_soft_acc: 0.6078\n",
      "Epoch 298/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2713 - soft_acc: 0.7897 - val_loss: 1.5668 - val_soft_acc: 0.5408\n",
      "Epoch 299/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5673 - soft_acc: 0.6620 - val_loss: 1.4122 - val_soft_acc: 0.5762\n",
      "Epoch 300/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1690 - soft_acc: 0.8260 - val_loss: 1.2531 - val_soft_acc: 0.6211\n",
      "Epoch 301/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1088 - soft_acc: 0.8856 - val_loss: 1.1640 - val_soft_acc: 0.6398\n",
      "Epoch 302/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0696 - soft_acc: 0.9166 - val_loss: 1.1588 - val_soft_acc: 0.6467\n",
      "Epoch 303/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0966 - soft_acc: 0.9098 - val_loss: 1.3671 - val_soft_acc: 0.5994\n",
      "Epoch 304/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1831 - soft_acc: 0.8486 - val_loss: 1.2837 - val_soft_acc: 0.6186\n",
      "Epoch 305/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4367 - soft_acc: 0.7096 - val_loss: 1.4972 - val_soft_acc: 0.5461\n",
      "Epoch 306/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3404 - soft_acc: 0.7367 - val_loss: 1.4224 - val_soft_acc: 0.5734\n",
      "Epoch 307/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3889 - soft_acc: 0.7234 - val_loss: 1.3657 - val_soft_acc: 0.5795\n",
      "Epoch 308/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2411 - soft_acc: 0.7960 - val_loss: 1.2964 - val_soft_acc: 0.6072\n",
      "Epoch 309/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1185 - soft_acc: 0.8723 - val_loss: 1.1732 - val_soft_acc: 0.6529\n",
      "Epoch 310/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0794 - soft_acc: 0.9093 - val_loss: 1.2439 - val_soft_acc: 0.6561\n",
      "Epoch 311/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0991 - soft_acc: 0.9012 - val_loss: 1.1594 - val_soft_acc: 0.6422\n",
      "Epoch 312/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1710 - soft_acc: 0.8767 - val_loss: 1.4079 - val_soft_acc: 0.5822\n",
      "Epoch 313/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2652 - soft_acc: 0.7964 - val_loss: 1.3322 - val_soft_acc: 0.5986\n",
      "Epoch 314/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2471 - soft_acc: 0.7933 - val_loss: 1.3020 - val_soft_acc: 0.6006\n",
      "Epoch 315/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1975 - soft_acc: 0.8237 - val_loss: 1.2406 - val_soft_acc: 0.6180\n",
      "Epoch 316/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1882 - soft_acc: 0.8257 - val_loss: 1.2383 - val_soft_acc: 0.6311\n",
      "Epoch 317/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1379 - soft_acc: 0.8547 - val_loss: 1.1767 - val_soft_acc: 0.6510\n",
      "Epoch 318/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.1794 - soft_acc: 0.8649 - val_loss: 1.2993 - val_soft_acc: 0.6049\n",
      "Epoch 319/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3933 - soft_acc: 0.7353 - val_loss: 1.6171 - val_soft_acc: 0.5414\n",
      "Epoch 320/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4238 - soft_acc: 0.7031 - val_loss: 1.5828 - val_soft_acc: 0.5518\n",
      "Epoch 321/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2888 - soft_acc: 0.7679 - val_loss: 1.3737 - val_soft_acc: 0.5920\n",
      "Epoch 322/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2349 - soft_acc: 0.7988 - val_loss: 1.2969 - val_soft_acc: 0.6100\n",
      "Epoch 323/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1094 - soft_acc: 0.8693 - val_loss: 1.1940 - val_soft_acc: 0.6559\n",
      "Epoch 324/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1239 - soft_acc: 0.8955 - val_loss: 1.2244 - val_soft_acc: 0.6404\n",
      "Epoch 325/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1634 - soft_acc: 0.8672 - val_loss: 1.3417 - val_soft_acc: 0.6105\n",
      "Epoch 326/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2743 - soft_acc: 0.7946 - val_loss: 1.3666 - val_soft_acc: 0.5707\n",
      "Epoch 327/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1442 - soft_acc: 0.8489 - val_loss: 1.2368 - val_soft_acc: 0.6242\n",
      "Epoch 328/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1363 - soft_acc: 0.8707 - val_loss: 1.3517 - val_soft_acc: 0.6107\n",
      "Epoch 329/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1289 - soft_acc: 0.8689 - val_loss: 1.1529 - val_soft_acc: 0.6492\n",
      "Epoch 330/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2606 - soft_acc: 0.8183 - val_loss: 1.4980 - val_soft_acc: 0.5418\n",
      "Epoch 331/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4196 - soft_acc: 0.7141 - val_loss: 1.5539 - val_soft_acc: 0.5463\n",
      "Epoch 332/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2249 - soft_acc: 0.7934 - val_loss: 1.3785 - val_soft_acc: 0.6127\n",
      "Epoch 333/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1500 - soft_acc: 0.8565 - val_loss: 1.2413 - val_soft_acc: 0.6412\n",
      "Epoch 334/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0677 - soft_acc: 0.9107 - val_loss: 1.1652 - val_soft_acc: 0.6639\n",
      "Epoch 335/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0481 - soft_acc: 0.9359 - val_loss: 1.1395 - val_soft_acc: 0.6711\n",
      "Epoch 336/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0910 - soft_acc: 0.9219 - val_loss: 1.1632 - val_soft_acc: 0.6510\n",
      "Epoch 337/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1207 - soft_acc: 0.8988 - val_loss: 1.2251 - val_soft_acc: 0.6316\n",
      "Epoch 338/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1145 - soft_acc: 0.8910 - val_loss: 1.2351 - val_soft_acc: 0.6299\n",
      "Epoch 339/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1142 - soft_acc: 0.8922 - val_loss: 1.4846 - val_soft_acc: 0.5781\n",
      "Epoch 340/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6261 - soft_acc: 0.6862 - val_loss: 2.0526 - val_soft_acc: 0.4234\n",
      "Epoch 341/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7654 - soft_acc: 0.5893 - val_loss: 1.4387 - val_soft_acc: 0.5592\n",
      "Epoch 342/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2817 - soft_acc: 0.7688 - val_loss: 1.4430 - val_soft_acc: 0.5777\n",
      "Epoch 343/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1258 - soft_acc: 0.8624 - val_loss: 1.1828 - val_soft_acc: 0.6482\n",
      "Epoch 344/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0561 - soft_acc: 0.9233 - val_loss: 1.1458 - val_soft_acc: 0.6709\n",
      "Epoch 345/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0361 - soft_acc: 0.9452 - val_loss: 1.1324 - val_soft_acc: 0.6816\n",
      "Epoch 346/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.0376 - soft_acc: 0.9483 - val_loss: 1.1103 - val_soft_acc: 0.6801\n",
      "Epoch 347/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0747 - soft_acc: 0.9343 - val_loss: 1.2032 - val_soft_acc: 0.6455\n",
      "Epoch 348/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3344 - soft_acc: 0.7692 - val_loss: 1.4796 - val_soft_acc: 0.5486\n",
      "Epoch 349/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2504 - soft_acc: 0.7864 - val_loss: 1.2849 - val_soft_acc: 0.6049\n",
      "Epoch 350/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1419 - soft_acc: 0.8598 - val_loss: 1.2904 - val_soft_acc: 0.6203\n",
      "Epoch 351/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0817 - soft_acc: 0.9010 - val_loss: 1.1594 - val_soft_acc: 0.6703\n",
      "Epoch 352/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0498 - soft_acc: 0.9340 - val_loss: 1.1272 - val_soft_acc: 0.6701\n",
      "Epoch 353/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0525 - soft_acc: 0.9353 - val_loss: 1.1677 - val_soft_acc: 0.6670\n",
      "Epoch 354/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1064 - soft_acc: 0.9006 - val_loss: 1.1912 - val_soft_acc: 0.6631\n",
      "Epoch 355/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0752 - soft_acc: 0.9167 - val_loss: 1.3569 - val_soft_acc: 0.6217\n",
      "Epoch 356/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4372 - soft_acc: 0.7352 - val_loss: 1.7997 - val_soft_acc: 0.4947\n",
      "Epoch 357/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7021 - soft_acc: 0.6185 - val_loss: 1.5490 - val_soft_acc: 0.5416\n",
      "Epoch 358/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2949 - soft_acc: 0.7600 - val_loss: 1.3609 - val_soft_acc: 0.5975\n",
      "Epoch 359/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1553 - soft_acc: 0.8466 - val_loss: 1.2132 - val_soft_acc: 0.6498\n",
      "Epoch 360/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0764 - soft_acc: 0.9095 - val_loss: 1.2014 - val_soft_acc: 0.6568\n",
      "Epoch 361/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0602 - soft_acc: 0.9243 - val_loss: 1.1648 - val_soft_acc: 0.6781\n",
      "Epoch 362/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0348 - soft_acc: 0.9472 - val_loss: 1.1097 - val_soft_acc: 0.6867\n",
      "Epoch 363/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0326 - soft_acc: 0.9555 - val_loss: 1.1548 - val_soft_acc: 0.6762\n",
      "Epoch 364/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0248 - soft_acc: 0.9602 - val_loss: 1.1275 - val_soft_acc: 0.6828\n",
      "Epoch 365/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0785 - soft_acc: 0.9268 - val_loss: 1.3719 - val_soft_acc: 0.6109\n",
      "Epoch 366/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5065 - soft_acc: 0.7167 - val_loss: 1.8938 - val_soft_acc: 0.4566\n",
      "Epoch 367/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6225 - soft_acc: 0.6448 - val_loss: 1.6316 - val_soft_acc: 0.5377\n",
      "Epoch 368/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2130 - soft_acc: 0.8004 - val_loss: 1.2624 - val_soft_acc: 0.6275\n",
      "Epoch 369/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1203 - soft_acc: 0.8752 - val_loss: 1.1949 - val_soft_acc: 0.6436\n",
      "Epoch 370/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1134 - soft_acc: 0.8821 - val_loss: 1.2383 - val_soft_acc: 0.6480\n",
      "Epoch 371/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1035 - soft_acc: 0.8952 - val_loss: 1.2469 - val_soft_acc: 0.6414\n",
      "Epoch 372/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0781 - soft_acc: 0.9140 - val_loss: 1.1022 - val_soft_acc: 0.6781\n",
      "Epoch 373/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0452 - soft_acc: 0.9402 - val_loss: 1.1154 - val_soft_acc: 0.6705\n",
      "Epoch 374/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0427 - soft_acc: 0.9448 - val_loss: 1.1169 - val_soft_acc: 0.6828\n",
      "Epoch 375/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0446 - soft_acc: 0.9450 - val_loss: 1.1686 - val_soft_acc: 0.6633\n",
      "Epoch 376/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.0510 - soft_acc: 0.9374 - val_loss: 1.1027 - val_soft_acc: 0.6801\n",
      "Epoch 377/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1007 - soft_acc: 0.9114 - val_loss: 1.1645 - val_soft_acc: 0.6545\n",
      "Epoch 378/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1443 - soft_acc: 0.8707 - val_loss: 1.3111 - val_soft_acc: 0.6180\n",
      "Epoch 379/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7397 - soft_acc: 0.6173 - val_loss: 1.5743 - val_soft_acc: 0.5297\n",
      "Epoch 380/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3692 - soft_acc: 0.7197 - val_loss: 1.4552 - val_soft_acc: 0.5588\n",
      "Epoch 381/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1564 - soft_acc: 0.8318 - val_loss: 1.2305 - val_soft_acc: 0.6385\n",
      "Epoch 382/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0875 - soft_acc: 0.9006 - val_loss: 1.1700 - val_soft_acc: 0.6637\n",
      "Epoch 383/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0776 - soft_acc: 0.9119 - val_loss: 1.1432 - val_soft_acc: 0.6592\n",
      "Epoch 384/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0847 - soft_acc: 0.9102 - val_loss: 1.1290 - val_soft_acc: 0.6697\n",
      "Epoch 385/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0376 - soft_acc: 0.9439 - val_loss: 1.0863 - val_soft_acc: 0.6867\n",
      "Epoch 386/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0300 - soft_acc: 0.9554 - val_loss: 1.0753 - val_soft_acc: 0.6891\n",
      "Epoch 387/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0372 - soft_acc: 0.9546 - val_loss: 1.0925 - val_soft_acc: 0.6846\n",
      "Epoch 388/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0709 - soft_acc: 0.9279 - val_loss: 1.2658 - val_soft_acc: 0.6465\n",
      "Epoch 389/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1384 - soft_acc: 0.8740 - val_loss: 1.3787 - val_soft_acc: 0.6111\n",
      "Epoch 390/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4429 - soft_acc: 0.7147 - val_loss: 1.5758 - val_soft_acc: 0.5283\n",
      "Epoch 391/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5026 - soft_acc: 0.6704 - val_loss: 1.6447 - val_soft_acc: 0.5326\n",
      "Epoch 392/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2700 - soft_acc: 0.7844 - val_loss: 1.3499 - val_soft_acc: 0.6000\n",
      "Epoch 393/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1429 - soft_acc: 0.8554 - val_loss: 1.2388 - val_soft_acc: 0.6527\n",
      "Epoch 394/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0701 - soft_acc: 0.9107 - val_loss: 1.1051 - val_soft_acc: 0.6865\n",
      "Epoch 395/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0300 - soft_acc: 0.9492 - val_loss: 1.0974 - val_soft_acc: 0.6893\n",
      "Epoch 396/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0179 - soft_acc: 0.9624 - val_loss: 1.0758 - val_soft_acc: 0.6969\n",
      "Epoch 397/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0131 - soft_acc: 0.9690 - val_loss: 1.0630 - val_soft_acc: 0.6934\n",
      "Epoch 398/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0092 - soft_acc: 0.9761 - val_loss: 1.0632 - val_soft_acc: 0.6986\n",
      "Epoch 399/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0126 - soft_acc: 0.9722 - val_loss: 1.0807 - val_soft_acc: 0.6957\n",
      "Epoch 400/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0181 - soft_acc: 0.9696 - val_loss: 1.1640 - val_soft_acc: 0.6830\n",
      "Epoch 401/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2934 - soft_acc: 0.8191 - val_loss: 2.0643 - val_soft_acc: 0.4498\n",
      "Epoch 402/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.9784 - soft_acc: 0.5486 - val_loss: 1.6907 - val_soft_acc: 0.5029\n",
      "Epoch 403/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2461 - soft_acc: 0.7719 - val_loss: 1.2441 - val_soft_acc: 0.6309\n",
      "Epoch 404/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2389 - soft_acc: 0.7909 - val_loss: 1.1934 - val_soft_acc: 0.6254\n",
      "Epoch 405/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0644 - soft_acc: 0.9060 - val_loss: 1.1289 - val_soft_acc: 0.6756\n",
      "Epoch 406/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0316 - soft_acc: 0.9450 - val_loss: 1.0685 - val_soft_acc: 0.6838\n",
      "Epoch 407/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0205 - soft_acc: 0.9617 - val_loss: 1.0717 - val_soft_acc: 0.6822\n",
      "Epoch 408/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0238 - soft_acc: 0.9620 - val_loss: 1.0680 - val_soft_acc: 0.6926\n",
      "Epoch 409/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1061 - soft_acc: 0.9130 - val_loss: 1.3389 - val_soft_acc: 0.6250\n",
      "Epoch 410/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3825 - soft_acc: 0.7547 - val_loss: 1.5871 - val_soft_acc: 0.5424\n",
      "Epoch 411/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4822 - soft_acc: 0.6976 - val_loss: 1.5162 - val_soft_acc: 0.5742\n",
      "Epoch 412/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1712 - soft_acc: 0.8257 - val_loss: 1.2173 - val_soft_acc: 0.6457\n",
      "Epoch 413/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0599 - soft_acc: 0.9122 - val_loss: 1.1161 - val_soft_acc: 0.6770\n",
      "Epoch 414/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0371 - soft_acc: 0.9440 - val_loss: 1.1105 - val_soft_acc: 0.6785\n",
      "Epoch 415/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0611 - soft_acc: 0.9325 - val_loss: 1.1278 - val_soft_acc: 0.6775\n",
      "Epoch 416/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0250 - soft_acc: 0.9553 - val_loss: 1.0835 - val_soft_acc: 0.6906\n",
      "Epoch 417/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0239 - soft_acc: 0.9644 - val_loss: 1.0923 - val_soft_acc: 0.6934\n",
      "Epoch 418/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0940 - soft_acc: 0.9183 - val_loss: 1.3519 - val_soft_acc: 0.6193\n",
      "Epoch 419/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5677 - soft_acc: 0.6810 - val_loss: 1.5435 - val_soft_acc: 0.5541\n",
      "Epoch 420/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2313 - soft_acc: 0.7875 - val_loss: 1.2113 - val_soft_acc: 0.6465\n",
      "Epoch 421/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0609 - soft_acc: 0.9108 - val_loss: 1.1548 - val_soft_acc: 0.6795\n",
      "Epoch 422/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0892 - soft_acc: 0.9190 - val_loss: 1.1883 - val_soft_acc: 0.6719\n",
      "Epoch 423/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0396 - soft_acc: 0.9461 - val_loss: 1.1206 - val_soft_acc: 0.6836\n",
      "Epoch 424/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0184 - soft_acc: 0.9631 - val_loss: 1.0977 - val_soft_acc: 0.6951\n",
      "Epoch 425/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0402 - soft_acc: 0.9566 - val_loss: 1.2152 - val_soft_acc: 0.6672\n",
      "Epoch 426/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0801 - soft_acc: 0.9214 - val_loss: 1.2270 - val_soft_acc: 0.6701\n",
      "Epoch 427/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0613 - soft_acc: 0.9243 - val_loss: 1.2098 - val_soft_acc: 0.6564\n",
      "Epoch 428/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3986 - soft_acc: 0.7421 - val_loss: 1.5104 - val_soft_acc: 0.5559\n",
      "Epoch 429/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3229 - soft_acc: 0.7584 - val_loss: 1.4461 - val_soft_acc: 0.5793\n",
      "Epoch 430/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1745 - soft_acc: 0.8332 - val_loss: 1.2163 - val_soft_acc: 0.6461\n",
      "Epoch 431/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0620 - soft_acc: 0.9121 - val_loss: 1.2078 - val_soft_acc: 0.6578\n",
      "Epoch 432/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0321 - soft_acc: 0.9458 - val_loss: 1.1121 - val_soft_acc: 0.6912\n",
      "Epoch 433/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0281 - soft_acc: 0.9540 - val_loss: 1.1028 - val_soft_acc: 0.6875\n",
      "Epoch 434/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0455 - soft_acc: 0.9485 - val_loss: 1.1447 - val_soft_acc: 0.6631\n",
      "Epoch 435/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1375 - soft_acc: 0.8900 - val_loss: 1.3601 - val_soft_acc: 0.6059\n",
      "Epoch 436/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2596 - soft_acc: 0.7988 - val_loss: 1.3753 - val_soft_acc: 0.6014\n",
      "Epoch 437/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3757 - soft_acc: 0.7374 - val_loss: 1.4220 - val_soft_acc: 0.5830\n",
      "Epoch 438/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3773 - soft_acc: 0.7446 - val_loss: 1.3627 - val_soft_acc: 0.6137\n",
      "Epoch 439/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1075 - soft_acc: 0.8767 - val_loss: 1.2231 - val_soft_acc: 0.6625\n",
      "Epoch 440/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0599 - soft_acc: 0.9242 - val_loss: 1.1498 - val_soft_acc: 0.6695\n",
      "Epoch 441/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0476 - soft_acc: 0.9399 - val_loss: 1.1075 - val_soft_acc: 0.6832\n",
      "Epoch 442/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0232 - soft_acc: 0.9585 - val_loss: 1.0699 - val_soft_acc: 0.7025\n",
      "Epoch 443/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0109 - soft_acc: 0.9725 - val_loss: 1.0765 - val_soft_acc: 0.6988\n",
      "Epoch 444/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0194 - soft_acc: 0.9670 - val_loss: 1.0827 - val_soft_acc: 0.6992\n",
      "Epoch 445/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0627 - soft_acc: 0.9397 - val_loss: 1.2061 - val_soft_acc: 0.6637\n",
      "Epoch 446/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1429 - soft_acc: 0.8765 - val_loss: 1.4067 - val_soft_acc: 0.6031\n",
      "Epoch 447/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3923 - soft_acc: 0.7286 - val_loss: 1.8078 - val_soft_acc: 0.5180\n",
      "Epoch 00447: early stopping\n",
      ">0.518\n",
      "Train on 14640 samples, validate on 2440 samples\n",
      "Epoch 1/5000\n",
      "14640/14640 [==============================] - 2s 107us/step - loss: 4.9979 - soft_acc: 0.1523 - val_loss: 4.1315 - val_soft_acc: 0.1531\n",
      "Epoch 2/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 4.2170 - soft_acc: 0.1623 - val_loss: 4.5310 - val_soft_acc: 0.1520\n",
      "Epoch 3/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 4.1396 - soft_acc: 0.1643 - val_loss: 4.3324 - val_soft_acc: 0.1695\n",
      "Epoch 4/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 4.0979 - soft_acc: 0.1688 - val_loss: 3.8696 - val_soft_acc: 0.1734\n",
      "Epoch 5/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.9689 - soft_acc: 0.1709 - val_loss: 3.8201 - val_soft_acc: 0.1838\n",
      "Epoch 6/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.9105 - soft_acc: 0.1748 - val_loss: 3.7843 - val_soft_acc: 0.1820\n",
      "Epoch 7/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.8782 - soft_acc: 0.1730 - val_loss: 3.7021 - val_soft_acc: 0.1723\n",
      "Epoch 8/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.8227 - soft_acc: 0.1777 - val_loss: 3.6857 - val_soft_acc: 0.1859\n",
      "Epoch 9/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.8103 - soft_acc: 0.1802 - val_loss: 3.7683 - val_soft_acc: 0.2000\n",
      "Epoch 10/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.7953 - soft_acc: 0.1769 - val_loss: 3.6716 - val_soft_acc: 0.1783\n",
      "Epoch 11/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.7961 - soft_acc: 0.1807 - val_loss: 3.6266 - val_soft_acc: 0.1842\n",
      "Epoch 12/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.7468 - soft_acc: 0.1828 - val_loss: 3.6590 - val_soft_acc: 0.1908\n",
      "Epoch 13/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.7481 - soft_acc: 0.1801 - val_loss: 3.7400 - val_soft_acc: 0.1713\n",
      "Epoch 14/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.7315 - soft_acc: 0.1803 - val_loss: 3.6223 - val_soft_acc: 0.2002\n",
      "Epoch 15/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.7173 - soft_acc: 0.1810 - val_loss: 3.8712 - val_soft_acc: 0.1730\n",
      "Epoch 16/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.7007 - soft_acc: 0.1803 - val_loss: 3.5834 - val_soft_acc: 0.1893\n",
      "Epoch 17/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.6157 - soft_acc: 0.1853 - val_loss: 3.6168 - val_soft_acc: 0.1922\n",
      "Epoch 18/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.6506 - soft_acc: 0.1847 - val_loss: 3.6826 - val_soft_acc: 0.1912\n",
      "Epoch 19/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.6235 - soft_acc: 0.1877 - val_loss: 3.4808 - val_soft_acc: 0.1887\n",
      "Epoch 20/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.5910 - soft_acc: 0.1900 - val_loss: 3.6642 - val_soft_acc: 0.1836\n",
      "Epoch 21/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.5317 - soft_acc: 0.1901 - val_loss: 3.4639 - val_soft_acc: 0.2080\n",
      "Epoch 22/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.5596 - soft_acc: 0.1905 - val_loss: 3.5729 - val_soft_acc: 0.1828\n",
      "Epoch 23/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.5237 - soft_acc: 0.1942 - val_loss: 3.5470 - val_soft_acc: 0.1842\n",
      "Epoch 24/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.4832 - soft_acc: 0.1942 - val_loss: 3.4382 - val_soft_acc: 0.2041\n",
      "Epoch 25/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.4711 - soft_acc: 0.1936 - val_loss: 3.7318 - val_soft_acc: 0.1986\n",
      "Epoch 26/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.4596 - soft_acc: 0.1942 - val_loss: 3.3981 - val_soft_acc: 0.1971\n",
      "Epoch 27/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.4312 - soft_acc: 0.1970 - val_loss: 3.3442 - val_soft_acc: 0.2016\n",
      "Epoch 28/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.4369 - soft_acc: 0.1963 - val_loss: 3.5156 - val_soft_acc: 0.2141\n",
      "Epoch 29/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.3822 - soft_acc: 0.2000 - val_loss: 3.3283 - val_soft_acc: 0.1939\n",
      "Epoch 30/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.3898 - soft_acc: 0.1991 - val_loss: 3.3200 - val_soft_acc: 0.2107\n",
      "Epoch 31/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.3629 - soft_acc: 0.2070 - val_loss: 3.3965 - val_soft_acc: 0.2174\n",
      "Epoch 32/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.3516 - soft_acc: 0.2003 - val_loss: 3.2844 - val_soft_acc: 0.2031\n",
      "Epoch 33/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.3000 - soft_acc: 0.2038 - val_loss: 3.2806 - val_soft_acc: 0.2082\n",
      "Epoch 34/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.3141 - soft_acc: 0.2070 - val_loss: 3.3276 - val_soft_acc: 0.1963\n",
      "Epoch 35/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.2935 - soft_acc: 0.2081 - val_loss: 3.2558 - val_soft_acc: 0.2010\n",
      "Epoch 36/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.2898 - soft_acc: 0.2096 - val_loss: 3.2831 - val_soft_acc: 0.2141\n",
      "Epoch 37/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.2382 - soft_acc: 0.2089 - val_loss: 3.2012 - val_soft_acc: 0.2186\n",
      "Epoch 38/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.2299 - soft_acc: 0.2128 - val_loss: 3.2670 - val_soft_acc: 0.2158\n",
      "Epoch 39/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.2023 - soft_acc: 0.2133 - val_loss: 3.1960 - val_soft_acc: 0.2189\n",
      "Epoch 40/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.2046 - soft_acc: 0.2173 - val_loss: 3.1169 - val_soft_acc: 0.2162\n",
      "Epoch 41/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.1561 - soft_acc: 0.2188 - val_loss: 3.1916 - val_soft_acc: 0.2156\n",
      "Epoch 42/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.1319 - soft_acc: 0.2206 - val_loss: 3.2221 - val_soft_acc: 0.2133\n",
      "Epoch 43/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.1114 - soft_acc: 0.2237 - val_loss: 3.0909 - val_soft_acc: 0.2148\n",
      "Epoch 44/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.1318 - soft_acc: 0.2192 - val_loss: 3.1008 - val_soft_acc: 0.2281\n",
      "Epoch 45/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.0882 - soft_acc: 0.2202 - val_loss: 3.0851 - val_soft_acc: 0.2367\n",
      "Epoch 46/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.0806 - soft_acc: 0.2246 - val_loss: 3.0730 - val_soft_acc: 0.2236\n",
      "Epoch 47/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.0501 - soft_acc: 0.2323 - val_loss: 3.1105 - val_soft_acc: 0.2279\n",
      "Epoch 48/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.0310 - soft_acc: 0.2304 - val_loss: 3.0260 - val_soft_acc: 0.2383\n",
      "Epoch 49/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.0062 - soft_acc: 0.2343 - val_loss: 3.0919 - val_soft_acc: 0.2322\n",
      "Epoch 50/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.9812 - soft_acc: 0.2355 - val_loss: 3.0371 - val_soft_acc: 0.2348\n",
      "Epoch 51/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.9636 - soft_acc: 0.2338 - val_loss: 2.9685 - val_soft_acc: 0.2504\n",
      "Epoch 52/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.9487 - soft_acc: 0.2368 - val_loss: 2.9395 - val_soft_acc: 0.2365\n",
      "Epoch 53/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.9040 - soft_acc: 0.2406 - val_loss: 2.9008 - val_soft_acc: 0.2395\n",
      "Epoch 54/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.9260 - soft_acc: 0.2415 - val_loss: 3.0854 - val_soft_acc: 0.2457\n",
      "Epoch 55/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.8911 - soft_acc: 0.2412 - val_loss: 2.9779 - val_soft_acc: 0.2469\n",
      "Epoch 56/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.8057 - soft_acc: 0.2496 - val_loss: 2.8673 - val_soft_acc: 0.2588\n",
      "Epoch 57/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.8151 - soft_acc: 0.2497 - val_loss: 2.8530 - val_soft_acc: 0.2553\n",
      "Epoch 58/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.8138 - soft_acc: 0.2501 - val_loss: 2.7993 - val_soft_acc: 0.2531\n",
      "Epoch 59/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.7557 - soft_acc: 0.2537 - val_loss: 2.8439 - val_soft_acc: 0.2545\n",
      "Epoch 60/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.7437 - soft_acc: 0.2532 - val_loss: 2.9304 - val_soft_acc: 0.2633\n",
      "Epoch 61/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.7498 - soft_acc: 0.2559 - val_loss: 2.7393 - val_soft_acc: 0.2641\n",
      "Epoch 62/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.6885 - soft_acc: 0.2605 - val_loss: 2.7743 - val_soft_acc: 0.2609\n",
      "Epoch 63/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.7445 - soft_acc: 0.2567 - val_loss: 2.7498 - val_soft_acc: 0.2586\n",
      "Epoch 64/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.6573 - soft_acc: 0.2615 - val_loss: 2.7877 - val_soft_acc: 0.2701\n",
      "Epoch 65/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.6491 - soft_acc: 0.2615 - val_loss: 2.7294 - val_soft_acc: 0.2715\n",
      "Epoch 66/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.6145 - soft_acc: 0.2679 - val_loss: 2.8616 - val_soft_acc: 0.2490\n",
      "Epoch 67/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.5750 - soft_acc: 0.2730 - val_loss: 2.6260 - val_soft_acc: 0.2662\n",
      "Epoch 68/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.5490 - soft_acc: 0.2764 - val_loss: 2.6022 - val_soft_acc: 0.2707\n",
      "Epoch 69/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.5367 - soft_acc: 0.2725 - val_loss: 2.6555 - val_soft_acc: 0.2613\n",
      "Epoch 70/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.4983 - soft_acc: 0.2760 - val_loss: 2.6436 - val_soft_acc: 0.2775\n",
      "Epoch 71/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.4663 - soft_acc: 0.2814 - val_loss: 2.6814 - val_soft_acc: 0.2795\n",
      "Epoch 72/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.4471 - soft_acc: 0.2849 - val_loss: 2.5318 - val_soft_acc: 0.2797\n",
      "Epoch 73/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.4225 - soft_acc: 0.2903 - val_loss: 2.6321 - val_soft_acc: 0.2758\n",
      "Epoch 74/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.3885 - soft_acc: 0.2862 - val_loss: 2.6318 - val_soft_acc: 0.2920\n",
      "Epoch 75/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.3489 - soft_acc: 0.2944 - val_loss: 2.6121 - val_soft_acc: 0.2902\n",
      "Epoch 76/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.3641 - soft_acc: 0.2948 - val_loss: 2.5957 - val_soft_acc: 0.2836\n",
      "Epoch 77/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.3281 - soft_acc: 0.3025 - val_loss: 2.5427 - val_soft_acc: 0.2873\n",
      "Epoch 78/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.2929 - soft_acc: 0.3006 - val_loss: 2.4539 - val_soft_acc: 0.2939\n",
      "Epoch 79/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.2393 - soft_acc: 0.3056 - val_loss: 2.4507 - val_soft_acc: 0.3014\n",
      "Epoch 80/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.1931 - soft_acc: 0.3114 - val_loss: 2.3708 - val_soft_acc: 0.2928\n",
      "Epoch 81/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.2217 - soft_acc: 0.3078 - val_loss: 2.4227 - val_soft_acc: 0.3059\n",
      "Epoch 82/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.1596 - soft_acc: 0.3155 - val_loss: 2.4165 - val_soft_acc: 0.3025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.2095 - soft_acc: 0.3111 - val_loss: 2.4085 - val_soft_acc: 0.3068\n",
      "Epoch 84/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.0806 - soft_acc: 0.3306 - val_loss: 2.3042 - val_soft_acc: 0.3152\n",
      "Epoch 85/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.1052 - soft_acc: 0.3248 - val_loss: 2.3030 - val_soft_acc: 0.3141\n",
      "Epoch 86/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.9957 - soft_acc: 0.3316 - val_loss: 2.3908 - val_soft_acc: 0.3125\n",
      "Epoch 87/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.0332 - soft_acc: 0.3363 - val_loss: 2.3276 - val_soft_acc: 0.3191\n",
      "Epoch 88/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.9871 - soft_acc: 0.3352 - val_loss: 2.2527 - val_soft_acc: 0.3203\n",
      "Epoch 89/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 1.9607 - soft_acc: 0.3393 - val_loss: 2.1476 - val_soft_acc: 0.3363\n",
      "Epoch 90/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.9306 - soft_acc: 0.3431 - val_loss: 2.2492 - val_soft_acc: 0.3248\n",
      "Epoch 91/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.9304 - soft_acc: 0.3448 - val_loss: 2.2973 - val_soft_acc: 0.3361\n",
      "Epoch 92/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.9469 - soft_acc: 0.3443 - val_loss: 2.3829 - val_soft_acc: 0.3203\n",
      "Epoch 93/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.9262 - soft_acc: 0.3452 - val_loss: 2.2128 - val_soft_acc: 0.3314\n",
      "Epoch 94/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.7908 - soft_acc: 0.3638 - val_loss: 2.1486 - val_soft_acc: 0.3457\n",
      "Epoch 95/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.7527 - soft_acc: 0.3698 - val_loss: 2.1835 - val_soft_acc: 0.3357\n",
      "Epoch 96/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.7782 - soft_acc: 0.3690 - val_loss: 2.1112 - val_soft_acc: 0.3400\n",
      "Epoch 97/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.7092 - soft_acc: 0.3728 - val_loss: 2.1517 - val_soft_acc: 0.3527\n",
      "Epoch 98/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.6669 - soft_acc: 0.3786 - val_loss: 2.0824 - val_soft_acc: 0.3566\n",
      "Epoch 99/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.6852 - soft_acc: 0.3791 - val_loss: 2.0848 - val_soft_acc: 0.3465\n",
      "Epoch 100/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.6698 - soft_acc: 0.3795 - val_loss: 2.1489 - val_soft_acc: 0.3395\n",
      "Epoch 101/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.6378 - soft_acc: 0.3889 - val_loss: 2.1101 - val_soft_acc: 0.3613\n",
      "Epoch 102/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.5661 - soft_acc: 0.3973 - val_loss: 2.0863 - val_soft_acc: 0.3576\n",
      "Epoch 103/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 1.5468 - soft_acc: 0.3985 - val_loss: 1.9833 - val_soft_acc: 0.3709\n",
      "Epoch 104/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.5664 - soft_acc: 0.4015 - val_loss: 2.0540 - val_soft_acc: 0.3590\n",
      "Epoch 105/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.5035 - soft_acc: 0.4087 - val_loss: 2.0409 - val_soft_acc: 0.3682\n",
      "Epoch 106/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.4753 - soft_acc: 0.4144 - val_loss: 2.0360 - val_soft_acc: 0.3570\n",
      "Epoch 107/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.4435 - soft_acc: 0.4196 - val_loss: 1.9202 - val_soft_acc: 0.3857\n",
      "Epoch 108/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.4135 - soft_acc: 0.4253 - val_loss: 1.9811 - val_soft_acc: 0.3713\n",
      "Epoch 109/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.4427 - soft_acc: 0.4214 - val_loss: 1.9383 - val_soft_acc: 0.3877\n",
      "Epoch 110/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.4218 - soft_acc: 0.4277 - val_loss: 1.9057 - val_soft_acc: 0.3932\n",
      "Epoch 111/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.3740 - soft_acc: 0.4299 - val_loss: 1.9220 - val_soft_acc: 0.3834\n",
      "Epoch 112/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.3868 - soft_acc: 0.4290 - val_loss: 1.9096 - val_soft_acc: 0.3922\n",
      "Epoch 113/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.3172 - soft_acc: 0.4395 - val_loss: 1.8900 - val_soft_acc: 0.3848\n",
      "Epoch 114/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.3016 - soft_acc: 0.4500 - val_loss: 1.8955 - val_soft_acc: 0.3957\n",
      "Epoch 115/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.2514 - soft_acc: 0.4600 - val_loss: 1.8793 - val_soft_acc: 0.4039\n",
      "Epoch 116/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.3606 - soft_acc: 0.4323 - val_loss: 1.9777 - val_soft_acc: 0.3844\n",
      "Epoch 117/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.2433 - soft_acc: 0.4585 - val_loss: 1.9655 - val_soft_acc: 0.3818\n",
      "Epoch 118/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.1955 - soft_acc: 0.4720 - val_loss: 1.8633 - val_soft_acc: 0.4084\n",
      "Epoch 119/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.2058 - soft_acc: 0.4714 - val_loss: 1.7766 - val_soft_acc: 0.4242\n",
      "Epoch 120/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.1450 - soft_acc: 0.4814 - val_loss: 1.8919 - val_soft_acc: 0.4039\n",
      "Epoch 121/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.1549 - soft_acc: 0.4765 - val_loss: 1.7619 - val_soft_acc: 0.4293\n",
      "Epoch 122/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.1183 - soft_acc: 0.4836 - val_loss: 1.6963 - val_soft_acc: 0.4174\n",
      "Epoch 123/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.1059 - soft_acc: 0.4901 - val_loss: 1.7908 - val_soft_acc: 0.4291\n",
      "Epoch 124/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.0834 - soft_acc: 0.4962 - val_loss: 1.8605 - val_soft_acc: 0.4016\n",
      "Epoch 125/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.0821 - soft_acc: 0.4920 - val_loss: 1.8288 - val_soft_acc: 0.4176\n",
      "Epoch 126/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.0435 - soft_acc: 0.5021 - val_loss: 1.7546 - val_soft_acc: 0.4344\n",
      "Epoch 127/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.0158 - soft_acc: 0.5082 - val_loss: 1.8190 - val_soft_acc: 0.4344\n",
      "Epoch 128/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.2426 - soft_acc: 0.4592 - val_loss: 1.9463 - val_soft_acc: 0.4018\n",
      "Epoch 129/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.0612 - soft_acc: 0.4954 - val_loss: 1.7215 - val_soft_acc: 0.4244\n",
      "Epoch 130/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.0417 - soft_acc: 0.5054 - val_loss: 1.6804 - val_soft_acc: 0.4205\n",
      "Epoch 131/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.9628 - soft_acc: 0.5193 - val_loss: 1.7656 - val_soft_acc: 0.4234\n",
      "Epoch 132/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.9556 - soft_acc: 0.5269 - val_loss: 1.6916 - val_soft_acc: 0.4434\n",
      "Epoch 133/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.9709 - soft_acc: 0.5263 - val_loss: 1.7583 - val_soft_acc: 0.4457\n",
      "Epoch 134/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9509 - soft_acc: 0.5237 - val_loss: 1.8088 - val_soft_acc: 0.4408\n",
      "Epoch 135/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.9028 - soft_acc: 0.5350 - val_loss: 1.5560 - val_soft_acc: 0.4656\n",
      "Epoch 136/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.8802 - soft_acc: 0.5460 - val_loss: 1.7486 - val_soft_acc: 0.4447\n",
      "Epoch 137/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.1860 - soft_acc: 0.4753 - val_loss: 1.7844 - val_soft_acc: 0.4420\n",
      "Epoch 138/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9214 - soft_acc: 0.5369 - val_loss: 1.6793 - val_soft_acc: 0.4611\n",
      "Epoch 139/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8605 - soft_acc: 0.5506 - val_loss: 1.5905 - val_soft_acc: 0.4670\n",
      "Epoch 140/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8016 - soft_acc: 0.5667 - val_loss: 1.7088 - val_soft_acc: 0.4543\n",
      "Epoch 141/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.8687 - soft_acc: 0.5486 - val_loss: 1.6267 - val_soft_acc: 0.4820\n",
      "Epoch 142/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8584 - soft_acc: 0.5527 - val_loss: 1.6519 - val_soft_acc: 0.4725\n",
      "Epoch 143/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9038 - soft_acc: 0.5455 - val_loss: 1.8092 - val_soft_acc: 0.4436\n",
      "Epoch 144/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7922 - soft_acc: 0.5659 - val_loss: 1.5354 - val_soft_acc: 0.4816\n",
      "Epoch 145/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7610 - soft_acc: 0.5841 - val_loss: 1.6271 - val_soft_acc: 0.4682\n",
      "Epoch 146/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7866 - soft_acc: 0.5690 - val_loss: 1.6647 - val_soft_acc: 0.4631\n",
      "Epoch 147/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.8805 - soft_acc: 0.5462 - val_loss: 1.6216 - val_soft_acc: 0.4746\n",
      "Epoch 148/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7128 - soft_acc: 0.5935 - val_loss: 1.6285 - val_soft_acc: 0.4758\n",
      "Epoch 149/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6997 - soft_acc: 0.5964 - val_loss: 1.5204 - val_soft_acc: 0.4871\n",
      "Epoch 150/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7533 - soft_acc: 0.5842 - val_loss: 1.7422 - val_soft_acc: 0.4537\n",
      "Epoch 151/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8993 - soft_acc: 0.5414 - val_loss: 1.8385 - val_soft_acc: 0.4512\n",
      "Epoch 152/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.9492 - soft_acc: 0.5285 - val_loss: 1.7063 - val_soft_acc: 0.4500\n",
      "Epoch 153/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7297 - soft_acc: 0.5890 - val_loss: 1.6765 - val_soft_acc: 0.4611\n",
      "Epoch 154/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7317 - soft_acc: 0.5863 - val_loss: 1.5855 - val_soft_acc: 0.4658\n",
      "Epoch 155/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6587 - soft_acc: 0.6081 - val_loss: 1.5401 - val_soft_acc: 0.5053\n",
      "Epoch 156/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6705 - soft_acc: 0.6106 - val_loss: 1.5571 - val_soft_acc: 0.4779\n",
      "Epoch 157/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6638 - soft_acc: 0.6155 - val_loss: 1.5724 - val_soft_acc: 0.4953\n",
      "Epoch 158/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6762 - soft_acc: 0.6027 - val_loss: 1.6294 - val_soft_acc: 0.4898\n",
      "Epoch 159/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6655 - soft_acc: 0.6144 - val_loss: 1.5505 - val_soft_acc: 0.5037\n",
      "Epoch 160/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7026 - soft_acc: 0.5986 - val_loss: 1.5324 - val_soft_acc: 0.4967\n",
      "Epoch 161/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6594 - soft_acc: 0.6167 - val_loss: 1.5626 - val_soft_acc: 0.4879\n",
      "Epoch 162/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6388 - soft_acc: 0.6156 - val_loss: 1.5674 - val_soft_acc: 0.5008\n",
      "Epoch 163/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5504 - soft_acc: 0.6458 - val_loss: 1.4587 - val_soft_acc: 0.5234\n",
      "Epoch 164/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5603 - soft_acc: 0.6534 - val_loss: 1.5538 - val_soft_acc: 0.4893\n",
      "Epoch 165/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5581 - soft_acc: 0.6484 - val_loss: 1.5676 - val_soft_acc: 0.5109\n",
      "Epoch 166/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6054 - soft_acc: 0.6315 - val_loss: 1.6334 - val_soft_acc: 0.4842\n",
      "Epoch 167/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6152 - soft_acc: 0.6251 - val_loss: 1.5163 - val_soft_acc: 0.5059\n",
      "Epoch 168/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6318 - soft_acc: 0.6260 - val_loss: 1.9517 - val_soft_acc: 0.4143\n",
      "Epoch 169/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7605 - soft_acc: 0.5758 - val_loss: 1.6623 - val_soft_acc: 0.4879\n",
      "Epoch 170/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6101 - soft_acc: 0.6244 - val_loss: 1.5413 - val_soft_acc: 0.5014\n",
      "Epoch 171/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5143 - soft_acc: 0.6628 - val_loss: 1.5000 - val_soft_acc: 0.5250\n",
      "Epoch 172/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.6498 - soft_acc: 0.6299 - val_loss: 1.4618 - val_soft_acc: 0.5209\n",
      "Epoch 173/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5314 - soft_acc: 0.6698 - val_loss: 1.5479 - val_soft_acc: 0.5129\n",
      "Epoch 174/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5750 - soft_acc: 0.6528 - val_loss: 1.5426 - val_soft_acc: 0.5172\n",
      "Epoch 175/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5997 - soft_acc: 0.6339 - val_loss: 1.4893 - val_soft_acc: 0.5178\n",
      "Epoch 176/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4706 - soft_acc: 0.6793 - val_loss: 1.4553 - val_soft_acc: 0.5486\n",
      "Epoch 177/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5798 - soft_acc: 0.6477 - val_loss: 1.5239 - val_soft_acc: 0.5031\n",
      "Epoch 178/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6413 - soft_acc: 0.6206 - val_loss: 1.4295 - val_soft_acc: 0.5301\n",
      "Epoch 179/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4850 - soft_acc: 0.6775 - val_loss: 1.5307 - val_soft_acc: 0.5205\n",
      "Epoch 180/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5363 - soft_acc: 0.6593 - val_loss: 1.4863 - val_soft_acc: 0.5297\n",
      "Epoch 181/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5001 - soft_acc: 0.6658 - val_loss: 1.5163 - val_soft_acc: 0.5264\n",
      "Epoch 182/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4658 - soft_acc: 0.6883 - val_loss: 1.4334 - val_soft_acc: 0.5311\n",
      "Epoch 183/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4213 - soft_acc: 0.7100 - val_loss: 1.3512 - val_soft_acc: 0.5580\n",
      "Epoch 184/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4835 - soft_acc: 0.6902 - val_loss: 1.7831 - val_soft_acc: 0.4686\n",
      "Epoch 185/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.7228 - soft_acc: 0.5969 - val_loss: 1.6551 - val_soft_acc: 0.4895\n",
      "Epoch 186/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5197 - soft_acc: 0.6604 - val_loss: 1.3902 - val_soft_acc: 0.5418\n",
      "Epoch 187/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3562 - soft_acc: 0.7331 - val_loss: 1.3601 - val_soft_acc: 0.5480\n",
      "Epoch 188/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3579 - soft_acc: 0.7335 - val_loss: 1.3965 - val_soft_acc: 0.5666\n",
      "Epoch 189/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3598 - soft_acc: 0.7401 - val_loss: 1.3856 - val_soft_acc: 0.5619\n",
      "Epoch 190/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4604 - soft_acc: 0.6913 - val_loss: 1.6210 - val_soft_acc: 0.4982\n",
      "Epoch 191/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8718 - soft_acc: 0.5623 - val_loss: 1.5375 - val_soft_acc: 0.4895\n",
      "Epoch 192/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5574 - soft_acc: 0.6490 - val_loss: 1.5523 - val_soft_acc: 0.5127\n",
      "Epoch 193/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5485 - soft_acc: 0.6536 - val_loss: 1.3956 - val_soft_acc: 0.5299\n",
      "Epoch 194/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4270 - soft_acc: 0.6954 - val_loss: 1.4040 - val_soft_acc: 0.5467\n",
      "Epoch 195/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4450 - soft_acc: 0.6968 - val_loss: 1.4938 - val_soft_acc: 0.5504\n",
      "Epoch 196/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3585 - soft_acc: 0.7300 - val_loss: 1.3295 - val_soft_acc: 0.5820\n",
      "Epoch 197/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3850 - soft_acc: 0.7324 - val_loss: 1.4287 - val_soft_acc: 0.5586\n",
      "Epoch 198/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4615 - soft_acc: 0.7010 - val_loss: 1.3664 - val_soft_acc: 0.5504\n",
      "Epoch 199/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3353 - soft_acc: 0.7421 - val_loss: 1.3413 - val_soft_acc: 0.5584\n",
      "Epoch 200/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3792 - soft_acc: 0.7342 - val_loss: 1.3060 - val_soft_acc: 0.5838\n",
      "Epoch 201/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3982 - soft_acc: 0.7180 - val_loss: 1.3774 - val_soft_acc: 0.5592\n",
      "Epoch 202/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4996 - soft_acc: 0.6762 - val_loss: 1.6845 - val_soft_acc: 0.4969\n",
      "Epoch 203/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5530 - soft_acc: 0.6407 - val_loss: 1.4786 - val_soft_acc: 0.5281\n",
      "Epoch 204/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4028 - soft_acc: 0.7029 - val_loss: 1.4746 - val_soft_acc: 0.5385\n",
      "Epoch 205/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4784 - soft_acc: 0.6874 - val_loss: 1.4813 - val_soft_acc: 0.5449\n",
      "Epoch 206/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3579 - soft_acc: 0.7294 - val_loss: 1.3470 - val_soft_acc: 0.5703\n",
      "Epoch 207/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2992 - soft_acc: 0.7672 - val_loss: 1.4344 - val_soft_acc: 0.5650\n",
      "Epoch 208/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3094 - soft_acc: 0.7617 - val_loss: 1.3301 - val_soft_acc: 0.5988\n",
      "Epoch 209/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3717 - soft_acc: 0.7421 - val_loss: 1.3880 - val_soft_acc: 0.5553\n",
      "Epoch 210/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3785 - soft_acc: 0.7332 - val_loss: 1.5462 - val_soft_acc: 0.5359\n",
      "Epoch 211/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4507 - soft_acc: 0.6986 - val_loss: 1.3160 - val_soft_acc: 0.5691\n",
      "Epoch 212/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4284 - soft_acc: 0.7155 - val_loss: 1.7323 - val_soft_acc: 0.4986\n",
      "Epoch 213/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7983 - soft_acc: 0.5884 - val_loss: 1.5315 - val_soft_acc: 0.5238\n",
      "Epoch 214/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3699 - soft_acc: 0.7183 - val_loss: 1.4486 - val_soft_acc: 0.5570\n",
      "Epoch 215/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3821 - soft_acc: 0.7187 - val_loss: 1.3816 - val_soft_acc: 0.5705\n",
      "Epoch 216/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2796 - soft_acc: 0.7781 - val_loss: 1.3597 - val_soft_acc: 0.5754\n",
      "Epoch 217/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2277 - soft_acc: 0.8072 - val_loss: 1.2708 - val_soft_acc: 0.6094\n",
      "Epoch 218/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2149 - soft_acc: 0.8206 - val_loss: 1.2554 - val_soft_acc: 0.6049\n",
      "Epoch 219/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3267 - soft_acc: 0.7584 - val_loss: 1.2755 - val_soft_acc: 0.5805\n",
      "Epoch 220/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3090 - soft_acc: 0.7664 - val_loss: 1.3675 - val_soft_acc: 0.5803\n",
      "Epoch 221/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3934 - soft_acc: 0.7364 - val_loss: 1.6673 - val_soft_acc: 0.4875\n",
      "Epoch 222/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.5697 - soft_acc: 0.6491 - val_loss: 1.4450 - val_soft_acc: 0.5246\n",
      "Epoch 223/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4249 - soft_acc: 0.7002 - val_loss: 1.4670 - val_soft_acc: 0.5488\n",
      "Epoch 224/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3192 - soft_acc: 0.7522 - val_loss: 1.2824 - val_soft_acc: 0.5848\n",
      "Epoch 225/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2285 - soft_acc: 0.8026 - val_loss: 1.2435 - val_soft_acc: 0.6150\n",
      "Epoch 226/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2429 - soft_acc: 0.7999 - val_loss: 1.3361 - val_soft_acc: 0.5791\n",
      "Epoch 227/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3733 - soft_acc: 0.7291 - val_loss: 1.4851 - val_soft_acc: 0.5580\n",
      "Epoch 228/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4330 - soft_acc: 0.7028 - val_loss: 1.4245 - val_soft_acc: 0.5383\n",
      "Epoch 229/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5795 - soft_acc: 0.6474 - val_loss: 1.4211 - val_soft_acc: 0.5488\n",
      "Epoch 230/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3655 - soft_acc: 0.7251 - val_loss: 1.3403 - val_soft_acc: 0.5746\n",
      "Epoch 231/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2188 - soft_acc: 0.8026 - val_loss: 1.2323 - val_soft_acc: 0.6152\n",
      "Epoch 232/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2133 - soft_acc: 0.8184 - val_loss: 1.2417 - val_soft_acc: 0.6191\n",
      "Epoch 233/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1737 - soft_acc: 0.8447 - val_loss: 1.1573 - val_soft_acc: 0.6262\n",
      "Epoch 234/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1914 - soft_acc: 0.8370 - val_loss: 1.3402 - val_soft_acc: 0.5881\n",
      "Epoch 235/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3295 - soft_acc: 0.7484 - val_loss: 1.3654 - val_soft_acc: 0.5814\n",
      "Epoch 236/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5216 - soft_acc: 0.6725 - val_loss: 1.6675 - val_soft_acc: 0.5012\n",
      "Epoch 237/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5596 - soft_acc: 0.6391 - val_loss: 1.4510 - val_soft_acc: 0.5768\n",
      "Epoch 238/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2316 - soft_acc: 0.7838 - val_loss: 1.3101 - val_soft_acc: 0.6023\n",
      "Epoch 239/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2260 - soft_acc: 0.8197 - val_loss: 1.2323 - val_soft_acc: 0.6047\n",
      "Epoch 240/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1756 - soft_acc: 0.8448 - val_loss: 1.2456 - val_soft_acc: 0.6209\n",
      "Epoch 241/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2323 - soft_acc: 0.8209 - val_loss: 1.2979 - val_soft_acc: 0.6066\n",
      "Epoch 242/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3205 - soft_acc: 0.7602 - val_loss: 1.2961 - val_soft_acc: 0.6072\n",
      "Epoch 243/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3064 - soft_acc: 0.7700 - val_loss: 1.3401 - val_soft_acc: 0.5723\n",
      "Epoch 244/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4850 - soft_acc: 0.6914 - val_loss: 1.5660 - val_soft_acc: 0.5227\n",
      "Epoch 245/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3807 - soft_acc: 0.7142 - val_loss: 1.4857 - val_soft_acc: 0.5611\n",
      "Epoch 246/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3976 - soft_acc: 0.7205 - val_loss: 1.3747 - val_soft_acc: 0.5631\n",
      "Epoch 247/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4257 - soft_acc: 0.6999 - val_loss: 1.4148 - val_soft_acc: 0.5615\n",
      "Epoch 248/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2916 - soft_acc: 0.7667 - val_loss: 1.3140 - val_soft_acc: 0.5975\n",
      "Epoch 249/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2268 - soft_acc: 0.8032 - val_loss: 1.3344 - val_soft_acc: 0.6059\n",
      "Epoch 250/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1475 - soft_acc: 0.8480 - val_loss: 1.1342 - val_soft_acc: 0.6510\n",
      "Epoch 251/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1141 - soft_acc: 0.8837 - val_loss: 1.1353 - val_soft_acc: 0.6459\n",
      "Epoch 252/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1346 - soft_acc: 0.8798 - val_loss: 1.2827 - val_soft_acc: 0.6215\n",
      "Epoch 253/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1654 - soft_acc: 0.8447 - val_loss: 1.3038 - val_soft_acc: 0.6182\n",
      "Epoch 254/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4453 - soft_acc: 0.7133 - val_loss: 2.0289 - val_soft_acc: 0.4250\n",
      "Epoch 255/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.8200 - soft_acc: 0.5766 - val_loss: 1.4623 - val_soft_acc: 0.5400\n",
      "Epoch 256/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3188 - soft_acc: 0.7363 - val_loss: 1.2102 - val_soft_acc: 0.6154\n",
      "Epoch 257/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1709 - soft_acc: 0.8289 - val_loss: 1.2377 - val_soft_acc: 0.6207\n",
      "Epoch 258/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1607 - soft_acc: 0.8411 - val_loss: 1.1671 - val_soft_acc: 0.6336\n",
      "Epoch 259/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1651 - soft_acc: 0.8524 - val_loss: 1.2418 - val_soft_acc: 0.6078\n",
      "Epoch 260/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2003 - soft_acc: 0.8330 - val_loss: 1.2260 - val_soft_acc: 0.6172\n",
      "Epoch 261/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1369 - soft_acc: 0.8606 - val_loss: 1.1247 - val_soft_acc: 0.6295\n",
      "Epoch 262/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3200 - soft_acc: 0.7896 - val_loss: 1.7725 - val_soft_acc: 0.5064\n",
      "Epoch 263/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6276 - soft_acc: 0.6402 - val_loss: 1.4146 - val_soft_acc: 0.5633\n",
      "Epoch 264/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5279 - soft_acc: 0.6857 - val_loss: 1.4183 - val_soft_acc: 0.5670\n",
      "Epoch 265/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3171 - soft_acc: 0.7672 - val_loss: 1.2458 - val_soft_acc: 0.5988\n",
      "Epoch 266/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1626 - soft_acc: 0.8428 - val_loss: 1.1459 - val_soft_acc: 0.6385\n",
      "Epoch 267/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1453 - soft_acc: 0.8649 - val_loss: 1.1559 - val_soft_acc: 0.6383\n",
      "Epoch 268/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1272 - soft_acc: 0.8825 - val_loss: 1.1564 - val_soft_acc: 0.6510\n",
      "Epoch 269/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1489 - soft_acc: 0.8720 - val_loss: 1.3243 - val_soft_acc: 0.6037\n",
      "Epoch 270/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2817 - soft_acc: 0.7815 - val_loss: 1.4111 - val_soft_acc: 0.5869\n",
      "Epoch 271/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2203 - soft_acc: 0.8058 - val_loss: 1.2717 - val_soft_acc: 0.6027\n",
      "Epoch 272/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2427 - soft_acc: 0.8028 - val_loss: 1.3008 - val_soft_acc: 0.6012\n",
      "Epoch 273/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2596 - soft_acc: 0.7830 - val_loss: 1.3423 - val_soft_acc: 0.5988\n",
      "Epoch 274/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3673 - soft_acc: 0.7400 - val_loss: 1.5654 - val_soft_acc: 0.5268\n",
      "Epoch 275/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4420 - soft_acc: 0.6900 - val_loss: 1.3498 - val_soft_acc: 0.5684\n",
      "Epoch 276/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2811 - soft_acc: 0.7643 - val_loss: 1.2223 - val_soft_acc: 0.6119\n",
      "Epoch 277/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2097 - soft_acc: 0.8241 - val_loss: 1.1513 - val_soft_acc: 0.6213\n",
      "Epoch 278/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1274 - soft_acc: 0.8672 - val_loss: 1.1010 - val_soft_acc: 0.6543\n",
      "Epoch 279/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0793 - soft_acc: 0.9068 - val_loss: 1.0802 - val_soft_acc: 0.6564\n",
      "Epoch 280/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1501 - soft_acc: 0.8775 - val_loss: 1.2256 - val_soft_acc: 0.6170\n",
      "Epoch 281/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2429 - soft_acc: 0.7920 - val_loss: 1.4139 - val_soft_acc: 0.5691\n",
      "Epoch 282/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6945 - soft_acc: 0.6232 - val_loss: 1.5200 - val_soft_acc: 0.5221\n",
      "Epoch 283/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.3759 - soft_acc: 0.7169 - val_loss: 1.3012 - val_soft_acc: 0.5969\n",
      "Epoch 284/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1644 - soft_acc: 0.8335 - val_loss: 1.1821 - val_soft_acc: 0.6416\n",
      "Epoch 285/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1122 - soft_acc: 0.8803 - val_loss: 1.0901 - val_soft_acc: 0.6494\n",
      "Epoch 286/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1336 - soft_acc: 0.8804 - val_loss: 1.2153 - val_soft_acc: 0.6375\n",
      "Epoch 287/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1381 - soft_acc: 0.8672 - val_loss: 1.1580 - val_soft_acc: 0.6480\n",
      "Epoch 288/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0898 - soft_acc: 0.9008 - val_loss: 1.1281 - val_soft_acc: 0.6619\n",
      "Epoch 289/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0703 - soft_acc: 0.9176 - val_loss: 1.1615 - val_soft_acc: 0.6605\n",
      "Epoch 290/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0847 - soft_acc: 0.9069 - val_loss: 1.1264 - val_soft_acc: 0.6609\n",
      "Epoch 291/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2519 - soft_acc: 0.8228 - val_loss: 1.7729 - val_soft_acc: 0.4863\n",
      "Epoch 292/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.9926 - soft_acc: 0.5503 - val_loss: 1.5659 - val_soft_acc: 0.5070\n",
      "Epoch 293/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4243 - soft_acc: 0.6933 - val_loss: 1.3055 - val_soft_acc: 0.5877\n",
      "Epoch 294/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2046 - soft_acc: 0.8061 - val_loss: 1.2231 - val_soft_acc: 0.6383\n",
      "Epoch 295/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1251 - soft_acc: 0.8672 - val_loss: 1.1358 - val_soft_acc: 0.6596\n",
      "Epoch 296/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0652 - soft_acc: 0.9141 - val_loss: 1.0679 - val_soft_acc: 0.6717\n",
      "Epoch 297/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0560 - soft_acc: 0.9276 - val_loss: 1.0816 - val_soft_acc: 0.6725\n",
      "Epoch 298/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0563 - soft_acc: 0.9322 - val_loss: 1.0817 - val_soft_acc: 0.6750\n",
      "Epoch 299/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0603 - soft_acc: 0.9285 - val_loss: 1.1080 - val_soft_acc: 0.6641\n",
      "Epoch 300/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1357 - soft_acc: 0.8856 - val_loss: 1.3022 - val_soft_acc: 0.6064\n",
      "Epoch 301/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.6983 - soft_acc: 0.6482 - val_loss: 1.8192 - val_soft_acc: 0.4684\n",
      "Epoch 302/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4828 - soft_acc: 0.6690 - val_loss: 1.3699 - val_soft_acc: 0.5697\n",
      "Epoch 303/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1908 - soft_acc: 0.8023 - val_loss: 1.1740 - val_soft_acc: 0.6242\n",
      "Epoch 304/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1367 - soft_acc: 0.8663 - val_loss: 1.3823 - val_soft_acc: 0.6072\n",
      "Epoch 305/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1435 - soft_acc: 0.8562 - val_loss: 1.0985 - val_soft_acc: 0.6660\n",
      "Epoch 306/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0681 - soft_acc: 0.9127 - val_loss: 1.1772 - val_soft_acc: 0.6566\n",
      "Epoch 307/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0645 - soft_acc: 0.9203 - val_loss: 1.0509 - val_soft_acc: 0.6871\n",
      "Epoch 308/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0956 - soft_acc: 0.9095 - val_loss: 1.2437 - val_soft_acc: 0.6162\n",
      "Epoch 309/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1054 - soft_acc: 0.8852 - val_loss: 1.1963 - val_soft_acc: 0.6496\n",
      "Epoch 310/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6619 - soft_acc: 0.6536 - val_loss: 1.8575 - val_soft_acc: 0.4654\n",
      "Epoch 311/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.4057 - soft_acc: 0.7037 - val_loss: 1.3229 - val_soft_acc: 0.6006\n",
      "Epoch 312/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1255 - soft_acc: 0.8479 - val_loss: 1.1652 - val_soft_acc: 0.6467\n",
      "Epoch 313/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1210 - soft_acc: 0.8775 - val_loss: 1.2628 - val_soft_acc: 0.6330\n",
      "Epoch 314/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.1610 - soft_acc: 0.8548 - val_loss: 1.1780 - val_soft_acc: 0.6500\n",
      "Epoch 315/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.1246 - soft_acc: 0.87 - 1s 75us/step - loss: 0.1254 - soft_acc: 0.8771 - val_loss: 1.1594 - val_soft_acc: 0.6529\n",
      "Epoch 316/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0897 - soft_acc: 0.8938 - val_loss: 1.1314 - val_soft_acc: 0.6639\n",
      "Epoch 317/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0717 - soft_acc: 0.9141 - val_loss: 1.0629 - val_soft_acc: 0.6781\n",
      "Epoch 318/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0511 - soft_acc: 0.9368 - val_loss: 1.1222 - val_soft_acc: 0.6736\n",
      "Epoch 319/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0592 - soft_acc: 0.9310 - val_loss: 1.0386 - val_soft_acc: 0.6830\n",
      "Epoch 320/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1657 - soft_acc: 0.8773 - val_loss: 1.8742 - val_soft_acc: 0.4941\n",
      "Epoch 321/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.8194 - soft_acc: 0.5849 - val_loss: 1.5215 - val_soft_acc: 0.5355\n",
      "Epoch 322/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.3380 - soft_acc: 0.7263 - val_loss: 1.2320 - val_soft_acc: 0.6117\n",
      "Epoch 323/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1311 - soft_acc: 0.8504 - val_loss: 1.1518 - val_soft_acc: 0.6477\n",
      "Epoch 324/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.1407 - soft_acc: 0.87 - 1s 74us/step - loss: 0.1385 - soft_acc: 0.8736 - val_loss: 1.0809 - val_soft_acc: 0.6639\n",
      "Epoch 325/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0858 - soft_acc: 0.9029 - val_loss: 1.1263 - val_soft_acc: 0.6523\n",
      "Epoch 326/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2330 - soft_acc: 0.8361 - val_loss: 1.3885 - val_soft_acc: 0.5861\n",
      "Epoch 327/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2924 - soft_acc: 0.7705 - val_loss: 1.3441 - val_soft_acc: 0.5971\n",
      "Epoch 328/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1656 - soft_acc: 0.8427 - val_loss: 1.1272 - val_soft_acc: 0.6480\n",
      "Epoch 329/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0742 - soft_acc: 0.9076 - val_loss: 1.0520 - val_soft_acc: 0.6770\n",
      "Epoch 330/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0675 - soft_acc: 0.9216 - val_loss: 1.1582 - val_soft_acc: 0.6639\n",
      "Epoch 331/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.0785 - soft_acc: 0.91 - 1s 74us/step - loss: 0.0796 - soft_acc: 0.9146 - val_loss: 1.1346 - val_soft_acc: 0.6680\n",
      "Epoch 332/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2965 - soft_acc: 0.7949 - val_loss: 1.5850 - val_soft_acc: 0.5402\n",
      "Epoch 333/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4540 - soft_acc: 0.6914 - val_loss: 1.4649 - val_soft_acc: 0.5691\n",
      "Epoch 334/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3520 - soft_acc: 0.7428 - val_loss: 1.2637 - val_soft_acc: 0.6119\n",
      "Epoch 335/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1186 - soft_acc: 0.8666 - val_loss: 1.1035 - val_soft_acc: 0.6670\n",
      "Epoch 336/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.0790 - soft_acc: 0.9055 - val_loss: 1.0896 - val_soft_acc: 0.6770\n",
      "Epoch 337/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0549 - soft_acc: 0.9305 - val_loss: 1.0939 - val_soft_acc: 0.6781\n",
      "Epoch 338/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0459 - soft_acc: 0.9398 - val_loss: 1.0415 - val_soft_acc: 0.6969\n",
      "Epoch 339/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0332 - soft_acc: 0.9500 - val_loss: 1.0461 - val_soft_acc: 0.6986\n",
      "Epoch 340/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0188 - soft_acc: 0.9623 - val_loss: 1.0056 - val_soft_acc: 0.7066\n",
      "Epoch 341/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0324 - soft_acc: 0.9528 - val_loss: 1.0809 - val_soft_acc: 0.6730\n",
      "Epoch 342/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2154 - soft_acc: 0.8426 - val_loss: 1.7959 - val_soft_acc: 0.4936\n",
      "Epoch 343/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.9257 - soft_acc: 0.5548 - val_loss: 1.6564 - val_soft_acc: 0.5221\n",
      "Epoch 344/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3185 - soft_acc: 0.7418 - val_loss: 1.3330 - val_soft_acc: 0.5885\n",
      "Epoch 345/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1336 - soft_acc: 0.8559 - val_loss: 1.0664 - val_soft_acc: 0.6617\n",
      "Epoch 346/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0702 - soft_acc: 0.9078 - val_loss: 1.0871 - val_soft_acc: 0.6777\n",
      "Epoch 347/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0434 - soft_acc: 0.9328 - val_loss: 1.0515 - val_soft_acc: 0.6855\n",
      "Epoch 348/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0506 - soft_acc: 0.9395 - val_loss: 1.0321 - val_soft_acc: 0.6930\n",
      "Epoch 349/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0684 - soft_acc: 0.9291 - val_loss: 1.0732 - val_soft_acc: 0.6818\n",
      "Epoch 350/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2053 - soft_acc: 0.8494 - val_loss: 1.5630 - val_soft_acc: 0.5410\n",
      "Epoch 351/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5769 - soft_acc: 0.6625 - val_loss: 1.4603 - val_soft_acc: 0.5682\n",
      "Epoch 352/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2529 - soft_acc: 0.7858 - val_loss: 1.1943 - val_soft_acc: 0.6264\n",
      "Epoch 353/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.1035 - soft_acc: 0.8769 - val_loss: 1.1896 - val_soft_acc: 0.6482\n",
      "Epoch 354/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0909 - soft_acc: 0.9017 - val_loss: 1.1373 - val_soft_acc: 0.6461\n",
      "Epoch 355/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0608 - soft_acc: 0.9241 - val_loss: 1.0933 - val_soft_acc: 0.6754\n",
      "Epoch 356/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0316 - soft_acc: 0.9460 - val_loss: 1.0700 - val_soft_acc: 0.6832\n",
      "Epoch 357/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0332 - soft_acc: 0.9489 - val_loss: 1.0708 - val_soft_acc: 0.6770\n",
      "Epoch 358/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0648 - soft_acc: 0.9301 - val_loss: 1.1109 - val_soft_acc: 0.6678\n",
      "Epoch 359/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0643 - soft_acc: 0.9232 - val_loss: 1.1054 - val_soft_acc: 0.6678\n",
      "Epoch 360/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2915 - soft_acc: 0.8106 - val_loss: 1.8615 - val_soft_acc: 0.4740\n",
      "Epoch 361/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.8184 - soft_acc: 0.5876 - val_loss: 1.4713 - val_soft_acc: 0.5475\n",
      "Epoch 362/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2529 - soft_acc: 0.7758 - val_loss: 1.2683 - val_soft_acc: 0.6117\n",
      "Epoch 363/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0990 - soft_acc: 0.8775 - val_loss: 1.0659 - val_soft_acc: 0.6787\n",
      "Epoch 364/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0435 - soft_acc: 0.9317 - val_loss: 1.0206 - val_soft_acc: 0.6973\n",
      "Epoch 365/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0228 - soft_acc: 0.9572 - val_loss: 1.0239 - val_soft_acc: 0.6965\n",
      "Epoch 366/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0155 - soft_acc: 0.9667 - val_loss: 1.0244 - val_soft_acc: 0.7066\n",
      "Epoch 367/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0111 - soft_acc: 0.9715 - val_loss: 1.0065 - val_soft_acc: 0.7053\n",
      "Epoch 368/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0105 - soft_acc: 0.9754 - val_loss: 1.0181 - val_soft_acc: 0.7045\n",
      "Epoch 369/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0098 - soft_acc: 0.9755 - val_loss: 1.0381 - val_soft_acc: 0.6922\n",
      "Epoch 370/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0285 - soft_acc: 0.9652 - val_loss: 1.1659 - val_soft_acc: 0.6551\n",
      "Epoch 371/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6191 - soft_acc: 0.6933 - val_loss: 1.9149 - val_soft_acc: 0.4412\n",
      "Epoch 372/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5591 - soft_acc: 0.6434 - val_loss: 1.3438 - val_soft_acc: 0.5822\n",
      "Epoch 373/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2418 - soft_acc: 0.7845 - val_loss: 1.3208 - val_soft_acc: 0.6109\n",
      "Epoch 374/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.1807 - soft_acc: 0.83 - 1s 74us/step - loss: 0.1800 - soft_acc: 0.8390 - val_loss: 1.2702 - val_soft_acc: 0.6105\n",
      "Epoch 375/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0999 - soft_acc: 0.8932 - val_loss: 1.0826 - val_soft_acc: 0.6869\n",
      "Epoch 376/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.0437 - soft_acc: 0.9365 - val_loss: 1.0560 - val_soft_acc: 0.6893\n",
      "Epoch 377/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0209 - soft_acc: 0.9599 - val_loss: 1.0397 - val_soft_acc: 0.6969\n",
      "Epoch 378/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0128 - soft_acc: 0.9688 - val_loss: 1.0260 - val_soft_acc: 0.7035\n",
      "Epoch 379/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1073 - soft_acc: 0.8961 - val_loss: 1.0684 - val_soft_acc: 0.6721\n",
      "Epoch 380/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0973 - soft_acc: 0.8936 - val_loss: 1.3233 - val_soft_acc: 0.6203\n",
      "Epoch 381/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4751 - soft_acc: 0.6988 - val_loss: 1.5313 - val_soft_acc: 0.5328\n",
      "Epoch 382/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3757 - soft_acc: 0.7259 - val_loss: 1.3680 - val_soft_acc: 0.6025\n",
      "Epoch 383/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1420 - soft_acc: 0.8473 - val_loss: 1.1413 - val_soft_acc: 0.6555\n",
      "Epoch 384/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0579 - soft_acc: 0.9142 - val_loss: 1.0708 - val_soft_acc: 0.6873\n",
      "Epoch 385/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0333 - soft_acc: 0.9472 - val_loss: 1.1884 - val_soft_acc: 0.6613\n",
      "Epoch 386/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1628 - soft_acc: 0.8701 - val_loss: 1.4685 - val_soft_acc: 0.5895\n",
      "Epoch 387/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2466 - soft_acc: 0.8126 - val_loss: 1.1779 - val_soft_acc: 0.6633\n",
      "Epoch 388/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0693 - soft_acc: 0.9125 - val_loss: 1.1179 - val_soft_acc: 0.6795\n",
      "Epoch 389/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0546 - soft_acc: 0.9358 - val_loss: 1.0541 - val_soft_acc: 0.6965\n",
      "Epoch 390/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0915 - soft_acc: 0.9161 - val_loss: 1.1476 - val_soft_acc: 0.6637\n",
      "Epoch 00390: early stopping\n",
      ">0.664\n",
      "Train on 14640 samples, validate on 2440 samples\n",
      "Epoch 1/5000\n",
      "14640/14640 [==============================] - 2s 106us/step - loss: 4.8093 - soft_acc: 0.1520 - val_loss: 4.4594 - val_soft_acc: 0.1508\n",
      "Epoch 2/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 4.2198 - soft_acc: 0.1600 - val_loss: 3.9774 - val_soft_acc: 0.1709\n",
      "Epoch 3/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 4.0638 - soft_acc: 0.1671 - val_loss: 4.0635 - val_soft_acc: 0.1811\n",
      "Epoch 4/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.9945 - soft_acc: 0.1678 - val_loss: 3.7956 - val_soft_acc: 0.1609\n",
      "Epoch 5/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.9408 - soft_acc: 0.1705 - val_loss: 3.7753 - val_soft_acc: 0.1715\n",
      "Epoch 6/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.8930 - soft_acc: 0.1726 - val_loss: 3.7367 - val_soft_acc: 0.1805\n",
      "Epoch 7/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.9076 - soft_acc: 0.1723 - val_loss: 3.8615 - val_soft_acc: 0.1607\n",
      "Epoch 8/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.8295 - soft_acc: 0.1753 - val_loss: 3.7355 - val_soft_acc: 0.1947\n",
      "Epoch 9/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.8081 - soft_acc: 0.1761 - val_loss: 3.8397 - val_soft_acc: 0.1650\n",
      "Epoch 10/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.7928 - soft_acc: 0.1773 - val_loss: 3.7394 - val_soft_acc: 0.1908\n",
      "Epoch 11/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.7787 - soft_acc: 0.1774 - val_loss: 3.6520 - val_soft_acc: 0.1785\n",
      "Epoch 12/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.7502 - soft_acc: 0.1816 - val_loss: 3.7619 - val_soft_acc: 0.1785\n",
      "Epoch 13/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.7239 - soft_acc: 0.1791 - val_loss: 3.6182 - val_soft_acc: 0.1664\n",
      "Epoch 14/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.7021 - soft_acc: 0.1799 - val_loss: 3.5883 - val_soft_acc: 0.1832\n",
      "Epoch 15/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.6697 - soft_acc: 0.1821 - val_loss: 3.8014 - val_soft_acc: 0.1752\n",
      "Epoch 16/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.6940 - soft_acc: 0.1810 - val_loss: 3.6789 - val_soft_acc: 0.1914\n",
      "Epoch 17/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.6469 - soft_acc: 0.1837 - val_loss: 3.5725 - val_soft_acc: 0.1793\n",
      "Epoch 18/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.6060 - soft_acc: 0.1857 - val_loss: 3.4855 - val_soft_acc: 0.1822\n",
      "Epoch 19/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.5864 - soft_acc: 0.1858 - val_loss: 3.6308 - val_soft_acc: 0.1836\n",
      "Epoch 20/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.5777 - soft_acc: 0.1869 - val_loss: 3.4557 - val_soft_acc: 0.2111\n",
      "Epoch 21/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.5379 - soft_acc: 0.1892 - val_loss: 3.5764 - val_soft_acc: 0.1805\n",
      "Epoch 22/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.5334 - soft_acc: 0.1879 - val_loss: 3.3995 - val_soft_acc: 0.1846\n",
      "Epoch 23/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.5097 - soft_acc: 0.1869 - val_loss: 3.4462 - val_soft_acc: 0.2018\n",
      "Epoch 24/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.4895 - soft_acc: 0.1952 - val_loss: 3.4986 - val_soft_acc: 0.1686\n",
      "Epoch 25/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 3.4794 - soft_acc: 0.1948 - val_loss: 3.3645 - val_soft_acc: 0.1939\n",
      "Epoch 26/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.4271 - soft_acc: 0.1931 - val_loss: 3.3758 - val_soft_acc: 0.2061\n",
      "Epoch 27/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.4333 - soft_acc: 0.1964 - val_loss: 3.4624 - val_soft_acc: 0.2064\n",
      "Epoch 28/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.4291 - soft_acc: 0.1961 - val_loss: 3.4078 - val_soft_acc: 0.2078\n",
      "Epoch 29/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.3696 - soft_acc: 0.1985 - val_loss: 3.4203 - val_soft_acc: 0.1926\n",
      "Epoch 30/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.3879 - soft_acc: 0.2000 - val_loss: 3.3518 - val_soft_acc: 0.1898\n",
      "Epoch 31/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.3520 - soft_acc: 0.2035 - val_loss: 3.3823 - val_soft_acc: 0.2061\n",
      "Epoch 32/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.3302 - soft_acc: 0.2003 - val_loss: 3.3188 - val_soft_acc: 0.2080\n",
      "Epoch 33/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.3157 - soft_acc: 0.2064 - val_loss: 3.2514 - val_soft_acc: 0.1934\n",
      "Epoch 34/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.2862 - soft_acc: 0.2084 - val_loss: 3.4603 - val_soft_acc: 0.1994\n",
      "Epoch 35/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.2928 - soft_acc: 0.2061 - val_loss: 3.2501 - val_soft_acc: 0.2076\n",
      "Epoch 36/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.2327 - soft_acc: 0.2148 - val_loss: 3.1676 - val_soft_acc: 0.2115\n",
      "Epoch 37/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.2391 - soft_acc: 0.2129 - val_loss: 3.2232 - val_soft_acc: 0.2086\n",
      "Epoch 38/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.2098 - soft_acc: 0.2153 - val_loss: 3.1715 - val_soft_acc: 0.2162\n",
      "Epoch 39/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.1900 - soft_acc: 0.2126 - val_loss: 3.1342 - val_soft_acc: 0.2324\n",
      "Epoch 40/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.1707 - soft_acc: 0.2145 - val_loss: 3.1464 - val_soft_acc: 0.2213\n",
      "Epoch 41/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.1547 - soft_acc: 0.2196 - val_loss: 3.0532 - val_soft_acc: 0.2273\n",
      "Epoch 42/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.1366 - soft_acc: 0.2222 - val_loss: 3.1516 - val_soft_acc: 0.2211\n",
      "Epoch 43/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.1234 - soft_acc: 0.2221 - val_loss: 3.0882 - val_soft_acc: 0.2156\n",
      "Epoch 44/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.0893 - soft_acc: 0.2244 - val_loss: 3.1166 - val_soft_acc: 0.2377\n",
      "Epoch 45/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.0660 - soft_acc: 0.2256 - val_loss: 3.1111 - val_soft_acc: 0.2240\n",
      "Epoch 46/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.0606 - soft_acc: 0.2295 - val_loss: 3.0498 - val_soft_acc: 0.2293\n",
      "Epoch 47/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.0432 - soft_acc: 0.2272 - val_loss: 3.0432 - val_soft_acc: 0.2299\n",
      "Epoch 48/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.0476 - soft_acc: 0.2308 - val_loss: 3.1586 - val_soft_acc: 0.2275\n",
      "Epoch 49/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.9930 - soft_acc: 0.2337 - val_loss: 3.0421 - val_soft_acc: 0.2262\n",
      "Epoch 50/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.9770 - soft_acc: 0.2339 - val_loss: 3.0894 - val_soft_acc: 0.2400\n",
      "Epoch 51/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.9584 - soft_acc: 0.2356 - val_loss: 3.0174 - val_soft_acc: 0.2301\n",
      "Epoch 52/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.9207 - soft_acc: 0.2403 - val_loss: 3.0101 - val_soft_acc: 0.2367\n",
      "Epoch 53/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.9054 - soft_acc: 0.2437 - val_loss: 3.0319 - val_soft_acc: 0.2346\n",
      "Epoch 54/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.9353 - soft_acc: 0.2388 - val_loss: 2.8606 - val_soft_acc: 0.2533\n",
      "Epoch 55/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.8803 - soft_acc: 0.2420 - val_loss: 2.8315 - val_soft_acc: 0.2496\n",
      "Epoch 56/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.8264 - soft_acc: 0.2486 - val_loss: 2.8682 - val_soft_acc: 0.2576\n",
      "Epoch 57/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.8302 - soft_acc: 0.2490 - val_loss: 2.8960 - val_soft_acc: 0.2455\n",
      "Epoch 58/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.8073 - soft_acc: 0.2475 - val_loss: 2.9088 - val_soft_acc: 0.2525\n",
      "Epoch 59/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.7789 - soft_acc: 0.2523 - val_loss: 2.7795 - val_soft_acc: 0.2527\n",
      "Epoch 60/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.7547 - soft_acc: 0.2527 - val_loss: 2.8453 - val_soft_acc: 0.2529\n",
      "Epoch 61/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.7360 - soft_acc: 0.2566 - val_loss: 2.7688 - val_soft_acc: 0.2625\n",
      "Epoch 62/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 2.7054 - soft_acc: 0.2580 - val_loss: 2.9617 - val_soft_acc: 0.2506\n",
      "Epoch 63/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.6985 - soft_acc: 0.2567 - val_loss: 2.8670 - val_soft_acc: 0.2428\n",
      "Epoch 64/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.6660 - soft_acc: 0.2629 - val_loss: 2.7720 - val_soft_acc: 0.2498\n",
      "Epoch 65/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.6425 - soft_acc: 0.2639 - val_loss: 2.7189 - val_soft_acc: 0.2658\n",
      "Epoch 66/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.6345 - soft_acc: 0.2663 - val_loss: 2.7454 - val_soft_acc: 0.2580\n",
      "Epoch 67/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.5950 - soft_acc: 0.2693 - val_loss: 2.6243 - val_soft_acc: 0.2703\n",
      "Epoch 68/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.5705 - soft_acc: 0.2720 - val_loss: 2.7122 - val_soft_acc: 0.2633\n",
      "Epoch 69/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.5224 - soft_acc: 0.2745 - val_loss: 2.7447 - val_soft_acc: 0.2561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.5067 - soft_acc: 0.2744 - val_loss: 2.6333 - val_soft_acc: 0.2760\n",
      "Epoch 71/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.5063 - soft_acc: 0.2794 - val_loss: 2.5685 - val_soft_acc: 0.2789\n",
      "Epoch 72/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.4569 - soft_acc: 0.2847 - val_loss: 2.5422 - val_soft_acc: 0.2820\n",
      "Epoch 73/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.4261 - soft_acc: 0.2854 - val_loss: 2.5637 - val_soft_acc: 0.2846\n",
      "Epoch 74/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.3712 - soft_acc: 0.2894 - val_loss: 2.5957 - val_soft_acc: 0.2801\n",
      "Epoch 75/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.3572 - soft_acc: 0.2933 - val_loss: 2.6001 - val_soft_acc: 0.2799\n",
      "Epoch 76/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.3163 - soft_acc: 0.2925 - val_loss: 2.6286 - val_soft_acc: 0.2758\n",
      "Epoch 77/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.3537 - soft_acc: 0.2907 - val_loss: 2.5605 - val_soft_acc: 0.2955\n",
      "Epoch 78/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.2528 - soft_acc: 0.3019 - val_loss: 2.5164 - val_soft_acc: 0.2996\n",
      "Epoch 79/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.2609 - soft_acc: 0.3018 - val_loss: 2.4361 - val_soft_acc: 0.2893\n",
      "Epoch 80/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.2242 - soft_acc: 0.3052 - val_loss: 2.4781 - val_soft_acc: 0.3051\n",
      "Epoch 81/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.2042 - soft_acc: 0.3122 - val_loss: 2.4430 - val_soft_acc: 0.3004\n",
      "Epoch 82/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.1300 - soft_acc: 0.3173 - val_loss: 2.4199 - val_soft_acc: 0.2947\n",
      "Epoch 83/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.1387 - soft_acc: 0.3195 - val_loss: 2.4222 - val_soft_acc: 0.2945\n",
      "Epoch 84/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.1173 - soft_acc: 0.3159 - val_loss: 2.4195 - val_soft_acc: 0.2965\n",
      "Epoch 85/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.0491 - soft_acc: 0.3282 - val_loss: 2.4649 - val_soft_acc: 0.3117\n",
      "Epoch 86/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 2.1289 - soft_acc: 0.3158 - val_loss: 2.3603 - val_soft_acc: 0.3057\n",
      "Epoch 87/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.0056 - soft_acc: 0.3338 - val_loss: 2.4853 - val_soft_acc: 0.2998\n",
      "Epoch 88/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.9666 - soft_acc: 0.3381 - val_loss: 2.3187 - val_soft_acc: 0.3293\n",
      "Epoch 89/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.9459 - soft_acc: 0.3417 - val_loss: 2.3147 - val_soft_acc: 0.3172\n",
      "Epoch 90/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.9175 - soft_acc: 0.3441 - val_loss: 2.3986 - val_soft_acc: 0.3010\n",
      "Epoch 91/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.9348 - soft_acc: 0.3433 - val_loss: 2.2827 - val_soft_acc: 0.3320\n",
      "Epoch 92/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.8529 - soft_acc: 0.3594 - val_loss: 2.2521 - val_soft_acc: 0.3398\n",
      "Epoch 93/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.7838 - soft_acc: 0.3586 - val_loss: 2.2095 - val_soft_acc: 0.3254\n",
      "Epoch 94/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.8072 - soft_acc: 0.3617 - val_loss: 2.3261 - val_soft_acc: 0.3113\n",
      "Epoch 95/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.7456 - soft_acc: 0.3653 - val_loss: 2.2899 - val_soft_acc: 0.3340\n",
      "Epoch 96/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.7387 - soft_acc: 0.3686 - val_loss: 2.3764 - val_soft_acc: 0.3355\n",
      "Epoch 97/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.7370 - soft_acc: 0.3710 - val_loss: 2.1742 - val_soft_acc: 0.3340\n",
      "Epoch 98/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 1.6353 - soft_acc: 0.3880 - val_loss: 2.1644 - val_soft_acc: 0.3568\n",
      "Epoch 99/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.6170 - soft_acc: 0.3910 - val_loss: 2.1843 - val_soft_acc: 0.3293\n",
      "Epoch 100/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.6165 - soft_acc: 0.3894 - val_loss: 2.0328 - val_soft_acc: 0.3561\n",
      "Epoch 101/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.5601 - soft_acc: 0.3977 - val_loss: 2.0436 - val_soft_acc: 0.3594\n",
      "Epoch 102/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.5351 - soft_acc: 0.3994 - val_loss: 2.0829 - val_soft_acc: 0.3537\n",
      "Epoch 103/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.5337 - soft_acc: 0.3988 - val_loss: 2.1612 - val_soft_acc: 0.3553\n",
      "Epoch 104/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.5088 - soft_acc: 0.4082 - val_loss: 2.0960 - val_soft_acc: 0.3602\n",
      "Epoch 105/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.4372 - soft_acc: 0.4212 - val_loss: 2.0256 - val_soft_acc: 0.3723\n",
      "Epoch 106/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.4229 - soft_acc: 0.4202 - val_loss: 2.0355 - val_soft_acc: 0.3619\n",
      "Epoch 107/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.4280 - soft_acc: 0.4186 - val_loss: 2.0341 - val_soft_acc: 0.3744\n",
      "Epoch 108/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.3763 - soft_acc: 0.4290 - val_loss: 1.9770 - val_soft_acc: 0.3834\n",
      "Epoch 109/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.3610 - soft_acc: 0.4298 - val_loss: 2.0103 - val_soft_acc: 0.3842\n",
      "Epoch 110/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.3426 - soft_acc: 0.4310 - val_loss: 1.9804 - val_soft_acc: 0.3750\n",
      "Epoch 111/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.3595 - soft_acc: 0.4306 - val_loss: 1.9785 - val_soft_acc: 0.3842\n",
      "Epoch 112/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.2605 - soft_acc: 0.4535 - val_loss: 1.9027 - val_soft_acc: 0.3988\n",
      "Epoch 113/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.2860 - soft_acc: 0.4440 - val_loss: 2.0116 - val_soft_acc: 0.3707\n",
      "Epoch 114/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.2694 - soft_acc: 0.4451 - val_loss: 1.9356 - val_soft_acc: 0.3930\n",
      "Epoch 115/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.2400 - soft_acc: 0.4542 - val_loss: 1.8566 - val_soft_acc: 0.3967\n",
      "Epoch 116/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.2182 - soft_acc: 0.4663 - val_loss: 1.9339 - val_soft_acc: 0.3895\n",
      "Epoch 117/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.1862 - soft_acc: 0.4680 - val_loss: 1.9911 - val_soft_acc: 0.3852\n",
      "Epoch 118/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.1447 - soft_acc: 0.4732 - val_loss: 1.7902 - val_soft_acc: 0.4172\n",
      "Epoch 119/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.1172 - soft_acc: 0.4810 - val_loss: 1.8154 - val_soft_acc: 0.4217\n",
      "Epoch 120/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.0733 - soft_acc: 0.4913 - val_loss: 1.8341 - val_soft_acc: 0.4049\n",
      "Epoch 121/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.1153 - soft_acc: 0.4828 - val_loss: 1.8438 - val_soft_acc: 0.4041\n",
      "Epoch 122/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.0675 - soft_acc: 0.4900 - val_loss: 1.8933 - val_soft_acc: 0.4168\n",
      "Epoch 123/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.1353 - soft_acc: 0.4794 - val_loss: 1.7904 - val_soft_acc: 0.4000\n",
      "Epoch 124/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.0720 - soft_acc: 0.4904 - val_loss: 1.8046 - val_soft_acc: 0.4207\n",
      "Epoch 125/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.9812 - soft_acc: 0.51 - 1s 75us/step - loss: 0.9814 - soft_acc: 0.5150 - val_loss: 1.7530 - val_soft_acc: 0.4219\n",
      "Epoch 126/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.9916 - soft_acc: 0.5129 - val_loss: 1.7806 - val_soft_acc: 0.4279\n",
      "Epoch 127/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.9981 - soft_acc: 0.5138 - val_loss: 1.8733 - val_soft_acc: 0.4137\n",
      "Epoch 128/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.0015 - soft_acc: 0.5094 - val_loss: 1.8188 - val_soft_acc: 0.4117\n",
      "Epoch 129/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.0296 - soft_acc: 0.5072 - val_loss: 1.7867 - val_soft_acc: 0.4293\n",
      "Epoch 130/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.9510 - soft_acc: 0.5243 - val_loss: 1.7490 - val_soft_acc: 0.4293\n",
      "Epoch 131/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.0593 - soft_acc: 0.4928 - val_loss: 1.7393 - val_soft_acc: 0.4344\n",
      "Epoch 132/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.9403 - soft_acc: 0.5239 - val_loss: 1.7382 - val_soft_acc: 0.4273\n",
      "Epoch 133/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.8801 - soft_acc: 0.5367 - val_loss: 1.7666 - val_soft_acc: 0.4316\n",
      "Epoch 134/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8845 - soft_acc: 0.5384 - val_loss: 1.7025 - val_soft_acc: 0.4572\n",
      "Epoch 135/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8241 - soft_acc: 0.5586 - val_loss: 1.7223 - val_soft_acc: 0.4418\n",
      "Epoch 136/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.9634 - soft_acc: 0.5187 - val_loss: 1.8526 - val_soft_acc: 0.4387\n",
      "Epoch 137/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.8038 - soft_acc: 0.5617 - val_loss: 1.7855 - val_soft_acc: 0.4445\n",
      "Epoch 138/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.8359 - soft_acc: 0.5584 - val_loss: 1.6563 - val_soft_acc: 0.4609\n",
      "Epoch 139/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7758 - soft_acc: 0.5755 - val_loss: 1.6512 - val_soft_acc: 0.4572\n",
      "Epoch 140/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.8076 - soft_acc: 0.5624 - val_loss: 1.6156 - val_soft_acc: 0.4658\n",
      "Epoch 141/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7749 - soft_acc: 0.5699 - val_loss: 1.5957 - val_soft_acc: 0.4658\n",
      "Epoch 142/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7661 - soft_acc: 0.5716 - val_loss: 1.7663 - val_soft_acc: 0.4506\n",
      "Epoch 143/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.8865 - soft_acc: 0.5422 - val_loss: 1.8021 - val_soft_acc: 0.4314\n",
      "Epoch 144/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7979 - soft_acc: 0.5679 - val_loss: 1.6389 - val_soft_acc: 0.4703\n",
      "Epoch 145/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7726 - soft_acc: 0.5676 - val_loss: 1.7002 - val_soft_acc: 0.4529\n",
      "Epoch 146/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7988 - soft_acc: 0.5686 - val_loss: 1.6264 - val_soft_acc: 0.4787\n",
      "Epoch 147/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.7934 - soft_acc: 0.5707 - val_loss: 1.6881 - val_soft_acc: 0.4793\n",
      "Epoch 148/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7071 - soft_acc: 0.5992 - val_loss: 1.6835 - val_soft_acc: 0.4584\n",
      "Epoch 149/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.7354 - soft_acc: 0.5868 - val_loss: 1.6995 - val_soft_acc: 0.4703\n",
      "Epoch 150/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7525 - soft_acc: 0.5786 - val_loss: 1.6927 - val_soft_acc: 0.4760\n",
      "Epoch 151/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6988 - soft_acc: 0.5947 - val_loss: 1.5865 - val_soft_acc: 0.4861\n",
      "Epoch 152/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6446 - soft_acc: 0.6077 - val_loss: 1.5192 - val_soft_acc: 0.5020\n",
      "Epoch 153/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5909 - soft_acc: 0.6372 - val_loss: 1.5343 - val_soft_acc: 0.4883\n",
      "Epoch 154/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6715 - soft_acc: 0.6067 - val_loss: 1.7691 - val_soft_acc: 0.4596\n",
      "Epoch 155/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.8176 - soft_acc: 0.5624 - val_loss: 1.6936 - val_soft_acc: 0.4641\n",
      "Epoch 156/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6980 - soft_acc: 0.5919 - val_loss: 1.5756 - val_soft_acc: 0.4863\n",
      "Epoch 157/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6192 - soft_acc: 0.6234 - val_loss: 1.5778 - val_soft_acc: 0.4980\n",
      "Epoch 158/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7043 - soft_acc: 0.5915 - val_loss: 1.7565 - val_soft_acc: 0.4703\n",
      "Epoch 159/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6470 - soft_acc: 0.6134 - val_loss: 1.6944 - val_soft_acc: 0.4508\n",
      "Epoch 160/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6843 - soft_acc: 0.6022 - val_loss: 1.7046 - val_soft_acc: 0.4613\n",
      "Epoch 161/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7498 - soft_acc: 0.5820 - val_loss: 1.5515 - val_soft_acc: 0.4955\n",
      "Epoch 162/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5489 - soft_acc: 0.6480 - val_loss: 1.5184 - val_soft_acc: 0.5162\n",
      "Epoch 163/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.5491 - soft_acc: 0.6554 - val_loss: 1.5829 - val_soft_acc: 0.5076\n",
      "Epoch 164/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5789 - soft_acc: 0.6497 - val_loss: 1.6621 - val_soft_acc: 0.4844\n",
      "Epoch 165/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5950 - soft_acc: 0.6380 - val_loss: 1.5569 - val_soft_acc: 0.5018\n",
      "Epoch 166/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6868 - soft_acc: 0.5998 - val_loss: 1.6844 - val_soft_acc: 0.4717\n",
      "Epoch 167/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6306 - soft_acc: 0.6107 - val_loss: 1.7743 - val_soft_acc: 0.4742\n",
      "Epoch 168/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7428 - soft_acc: 0.5834 - val_loss: 1.6196 - val_soft_acc: 0.4855\n",
      "Epoch 169/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4901 - soft_acc: 0.6751 - val_loss: 1.4418 - val_soft_acc: 0.5334\n",
      "Epoch 170/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4674 - soft_acc: 0.6865 - val_loss: 1.5963 - val_soft_acc: 0.5080\n",
      "Epoch 171/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5294 - soft_acc: 0.6605 - val_loss: 1.5301 - val_soft_acc: 0.5111\n",
      "Epoch 172/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5165 - soft_acc: 0.6644 - val_loss: 1.5101 - val_soft_acc: 0.4996\n",
      "Epoch 173/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5600 - soft_acc: 0.6494 - val_loss: 1.7187 - val_soft_acc: 0.4799\n",
      "Epoch 174/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5642 - soft_acc: 0.6387 - val_loss: 1.5515 - val_soft_acc: 0.5070\n",
      "Epoch 175/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5803 - soft_acc: 0.6412 - val_loss: 1.6116 - val_soft_acc: 0.5006\n",
      "Epoch 176/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5998 - soft_acc: 0.6380 - val_loss: 1.6016 - val_soft_acc: 0.5137\n",
      "Epoch 177/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5039 - soft_acc: 0.6716 - val_loss: 1.5520 - val_soft_acc: 0.5238\n",
      "Epoch 178/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4867 - soft_acc: 0.6753 - val_loss: 1.5651 - val_soft_acc: 0.5184\n",
      "Epoch 179/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5114 - soft_acc: 0.6760 - val_loss: 1.4934 - val_soft_acc: 0.5139\n",
      "Epoch 180/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5896 - soft_acc: 0.6427 - val_loss: 1.6927 - val_soft_acc: 0.4789\n",
      "Epoch 181/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5363 - soft_acc: 0.6532 - val_loss: 1.6361 - val_soft_acc: 0.5010\n",
      "Epoch 182/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5779 - soft_acc: 0.6423 - val_loss: 1.5841 - val_soft_acc: 0.5129\n",
      "Epoch 183/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5288 - soft_acc: 0.6589 - val_loss: 1.6734 - val_soft_acc: 0.5117\n",
      "Epoch 184/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4639 - soft_acc: 0.6843 - val_loss: 1.4209 - val_soft_acc: 0.5314\n",
      "Epoch 185/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5357 - soft_acc: 0.6632 - val_loss: 1.4417 - val_soft_acc: 0.5355\n",
      "Epoch 186/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4228 - soft_acc: 0.7105 - val_loss: 1.4182 - val_soft_acc: 0.5486\n",
      "Epoch 187/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3606 - soft_acc: 0.7322 - val_loss: 1.4195 - val_soft_acc: 0.5629\n",
      "Epoch 188/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3260 - soft_acc: 0.7582 - val_loss: 1.3152 - val_soft_acc: 0.5676\n",
      "Epoch 189/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5847 - soft_acc: 0.6516 - val_loss: 1.6704 - val_soft_acc: 0.4826\n",
      "Epoch 190/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6333 - soft_acc: 0.6183 - val_loss: 1.6430 - val_soft_acc: 0.4957\n",
      "Epoch 191/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5108 - soft_acc: 0.6568 - val_loss: 1.5455 - val_soft_acc: 0.5113\n",
      "Epoch 192/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4553 - soft_acc: 0.6837 - val_loss: 1.5607 - val_soft_acc: 0.5166\n",
      "Epoch 193/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4531 - soft_acc: 0.6967 - val_loss: 1.5762 - val_soft_acc: 0.5215\n",
      "Epoch 194/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4298 - soft_acc: 0.7014 - val_loss: 1.3680 - val_soft_acc: 0.5393\n",
      "Epoch 195/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4162 - soft_acc: 0.7129 - val_loss: 1.8169 - val_soft_acc: 0.4711\n",
      "Epoch 196/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6996 - soft_acc: 0.6105 - val_loss: 1.6320 - val_soft_acc: 0.5037\n",
      "Epoch 197/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5453 - soft_acc: 0.6468 - val_loss: 1.4516 - val_soft_acc: 0.5262\n",
      "Epoch 198/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3301 - soft_acc: 0.7469 - val_loss: 1.3084 - val_soft_acc: 0.5746\n",
      "Epoch 199/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.3253 - soft_acc: 0.76 - 1s 75us/step - loss: 0.3248 - soft_acc: 0.7668 - val_loss: 1.3523 - val_soft_acc: 0.5615\n",
      "Epoch 200/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3661 - soft_acc: 0.7431 - val_loss: 1.8043 - val_soft_acc: 0.4666\n",
      "Epoch 201/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5849 - soft_acc: 0.6456 - val_loss: 1.4422 - val_soft_acc: 0.5297\n",
      "Epoch 202/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3980 - soft_acc: 0.7148 - val_loss: 1.3881 - val_soft_acc: 0.5553\n",
      "Epoch 203/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3451 - soft_acc: 0.7396 - val_loss: 1.3418 - val_soft_acc: 0.5684\n",
      "Epoch 204/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3258 - soft_acc: 0.7622 - val_loss: 1.3315 - val_soft_acc: 0.5730\n",
      "Epoch 205/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3070 - soft_acc: 0.7731 - val_loss: 1.5951 - val_soft_acc: 0.5400\n",
      "Epoch 206/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4701 - soft_acc: 0.6969 - val_loss: 1.4220 - val_soft_acc: 0.5486\n",
      "Epoch 207/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4333 - soft_acc: 0.7043 - val_loss: 1.5373 - val_soft_acc: 0.5293\n",
      "Epoch 208/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4880 - soft_acc: 0.6721 - val_loss: 1.3742 - val_soft_acc: 0.5609\n",
      "Epoch 209/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4105 - soft_acc: 0.7033 - val_loss: 1.4760 - val_soft_acc: 0.5428\n",
      "Epoch 210/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3577 - soft_acc: 0.7332 - val_loss: 1.5838 - val_soft_acc: 0.5172\n",
      "Epoch 211/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3226 - soft_acc: 0.7466 - val_loss: 1.3688 - val_soft_acc: 0.5660\n",
      "Epoch 212/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2882 - soft_acc: 0.7708 - val_loss: 1.3298 - val_soft_acc: 0.5865\n",
      "Epoch 213/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2997 - soft_acc: 0.7747 - val_loss: 1.3488 - val_soft_acc: 0.5787\n",
      "Epoch 214/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2773 - soft_acc: 0.7835 - val_loss: 1.3402 - val_soft_acc: 0.5826\n",
      "Epoch 215/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2805 - soft_acc: 0.7773 - val_loss: 1.3897 - val_soft_acc: 0.5703\n",
      "Epoch 216/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3498 - soft_acc: 0.7416 - val_loss: 1.6581 - val_soft_acc: 0.5033\n",
      "Epoch 217/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.7713 - soft_acc: 0.5838 - val_loss: 1.7099 - val_soft_acc: 0.4928\n",
      "Epoch 218/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5324 - soft_acc: 0.6480 - val_loss: 1.5188 - val_soft_acc: 0.5305\n",
      "Epoch 219/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3432 - soft_acc: 0.7322 - val_loss: 1.3295 - val_soft_acc: 0.5803\n",
      "Epoch 220/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2314 - soft_acc: 0.7946 - val_loss: 1.2632 - val_soft_acc: 0.5951\n",
      "Epoch 221/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3256 - soft_acc: 0.7691 - val_loss: 1.5221 - val_soft_acc: 0.5244\n",
      "Epoch 222/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5520 - soft_acc: 0.6550 - val_loss: 1.5373 - val_soft_acc: 0.5209\n",
      "Epoch 223/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4424 - soft_acc: 0.6963 - val_loss: 1.5699 - val_soft_acc: 0.5184\n",
      "Epoch 224/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3124 - soft_acc: 0.7498 - val_loss: 1.2228 - val_soft_acc: 0.6111\n",
      "Epoch 225/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2890 - soft_acc: 0.7860 - val_loss: 1.2717 - val_soft_acc: 0.5902\n",
      "Epoch 226/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3702 - soft_acc: 0.7292 - val_loss: 1.4890 - val_soft_acc: 0.5406\n",
      "Epoch 227/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3983 - soft_acc: 0.7183 - val_loss: 1.4249 - val_soft_acc: 0.5756\n",
      "Epoch 228/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3661 - soft_acc: 0.7297 - val_loss: 1.3849 - val_soft_acc: 0.5799\n",
      "Epoch 229/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3318 - soft_acc: 0.7585 - val_loss: 1.2555 - val_soft_acc: 0.5922\n",
      "Epoch 230/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2100 - soft_acc: 0.8174 - val_loss: 1.2264 - val_soft_acc: 0.6098\n",
      "Epoch 231/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2285 - soft_acc: 0.8174 - val_loss: 1.2165 - val_soft_acc: 0.6115\n",
      "Epoch 232/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2422 - soft_acc: 0.8084 - val_loss: 1.4392 - val_soft_acc: 0.5660\n",
      "Epoch 233/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5685 - soft_acc: 0.6651 - val_loss: 1.7350 - val_soft_acc: 0.4914\n",
      "Epoch 234/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5209 - soft_acc: 0.6697 - val_loss: 1.6599 - val_soft_acc: 0.5076\n",
      "Epoch 235/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3827 - soft_acc: 0.7184 - val_loss: 1.3810 - val_soft_acc: 0.5717\n",
      "Epoch 236/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.3758 - soft_acc: 0.7435 - val_loss: 1.2983 - val_soft_acc: 0.5988\n",
      "Epoch 237/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2061 - soft_acc: 0.8267 - val_loss: 1.3156 - val_soft_acc: 0.6035\n",
      "Epoch 238/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2139 - soft_acc: 0.8214 - val_loss: 1.3109 - val_soft_acc: 0.5910\n",
      "Epoch 239/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1753 - soft_acc: 0.8464 - val_loss: 1.1902 - val_soft_acc: 0.6252\n",
      "Epoch 240/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1700 - soft_acc: 0.8542 - val_loss: 1.2374 - val_soft_acc: 0.6115\n",
      "Epoch 241/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4723 - soft_acc: 0.7090 - val_loss: 1.6711 - val_soft_acc: 0.5029\n",
      "Epoch 242/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5419 - soft_acc: 0.6580 - val_loss: 1.4829 - val_soft_acc: 0.5330\n",
      "Epoch 243/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4357 - soft_acc: 0.6879 - val_loss: 1.4981 - val_soft_acc: 0.5602\n",
      "Epoch 244/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.3219 - soft_acc: 0.73 - 1s 75us/step - loss: 0.3218 - soft_acc: 0.7382 - val_loss: 1.3612 - val_soft_acc: 0.5826\n",
      "Epoch 245/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1929 - soft_acc: 0.8193 - val_loss: 1.2378 - val_soft_acc: 0.6172\n",
      "Epoch 246/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1539 - soft_acc: 0.8579 - val_loss: 1.2538 - val_soft_acc: 0.6195\n",
      "Epoch 247/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1362 - soft_acc: 0.8772 - val_loss: 1.2251 - val_soft_acc: 0.6135\n",
      "Epoch 248/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1854 - soft_acc: 0.8496 - val_loss: 1.3067 - val_soft_acc: 0.6168\n",
      "Epoch 249/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1953 - soft_acc: 0.8433 - val_loss: 1.5416 - val_soft_acc: 0.5545\n",
      "Epoch 250/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4014 - soft_acc: 0.7198 - val_loss: 1.5450 - val_soft_acc: 0.5447\n",
      "Epoch 251/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5596 - soft_acc: 0.6532 - val_loss: 1.8582 - val_soft_acc: 0.4943\n",
      "Epoch 252/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6266 - soft_acc: 0.6292 - val_loss: 1.7519 - val_soft_acc: 0.4996\n",
      "Epoch 253/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4447 - soft_acc: 0.6853 - val_loss: 1.4670 - val_soft_acc: 0.5576\n",
      "Epoch 254/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2292 - soft_acc: 0.7854 - val_loss: 1.3451 - val_soft_acc: 0.5879\n",
      "Epoch 255/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1890 - soft_acc: 0.8295 - val_loss: 1.2147 - val_soft_acc: 0.6330\n",
      "Epoch 256/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1360 - soft_acc: 0.8712 - val_loss: 1.1618 - val_soft_acc: 0.6436\n",
      "Epoch 257/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1096 - soft_acc: 0.8952 - val_loss: 1.1603 - val_soft_acc: 0.6455\n",
      "Epoch 258/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1418 - soft_acc: 0.8788 - val_loss: 1.1878 - val_soft_acc: 0.6258\n",
      "Epoch 259/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3794 - soft_acc: 0.7600 - val_loss: 1.6983 - val_soft_acc: 0.5127\n",
      "Epoch 260/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6864 - soft_acc: 0.6208 - val_loss: 1.5590 - val_soft_acc: 0.5203\n",
      "Epoch 261/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3223 - soft_acc: 0.7400 - val_loss: 1.2925 - val_soft_acc: 0.5908\n",
      "Epoch 262/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1489 - soft_acc: 0.8428 - val_loss: 1.2226 - val_soft_acc: 0.6328\n",
      "Epoch 263/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1029 - soft_acc: 0.8932 - val_loss: 1.1853 - val_soft_acc: 0.6426\n",
      "Epoch 264/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0907 - soft_acc: 0.9043 - val_loss: 1.1487 - val_soft_acc: 0.6537\n",
      "Epoch 265/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1236 - soft_acc: 0.8929 - val_loss: 1.2375 - val_soft_acc: 0.6320\n",
      "Epoch 266/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1739 - soft_acc: 0.8569 - val_loss: 1.2137 - val_soft_acc: 0.6273\n",
      "Epoch 267/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1574 - soft_acc: 0.8548 - val_loss: 1.2734 - val_soft_acc: 0.6182\n",
      "Epoch 268/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6477 - soft_acc: 0.6435 - val_loss: 2.0608 - val_soft_acc: 0.4295\n",
      "Epoch 269/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6468 - soft_acc: 0.6279 - val_loss: 1.5558 - val_soft_acc: 0.5084\n",
      "Epoch 270/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3529 - soft_acc: 0.7271 - val_loss: 1.4502 - val_soft_acc: 0.5705\n",
      "Epoch 271/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1926 - soft_acc: 0.8235 - val_loss: 1.2094 - val_soft_acc: 0.6375\n",
      "Epoch 272/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1320 - soft_acc: 0.8725 - val_loss: 1.1910 - val_soft_acc: 0.6318\n",
      "Epoch 273/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1332 - soft_acc: 0.8780 - val_loss: 1.1791 - val_soft_acc: 0.6332\n",
      "Epoch 274/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1084 - soft_acc: 0.8922 - val_loss: 1.1710 - val_soft_acc: 0.6432\n",
      "Epoch 275/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1191 - soft_acc: 0.8897 - val_loss: 1.1080 - val_soft_acc: 0.6641\n",
      "Epoch 276/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0768 - soft_acc: 0.9171 - val_loss: 1.1554 - val_soft_acc: 0.6537\n",
      "Epoch 277/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.9467 - soft_acc: 0.5707 - val_loss: 2.1419 - val_soft_acc: 0.4086\n",
      "Epoch 278/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6267 - soft_acc: 0.6243 - val_loss: 1.5456 - val_soft_acc: 0.5486\n",
      "Epoch 279/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2432 - soft_acc: 0.7766 - val_loss: 1.2489 - val_soft_acc: 0.6186\n",
      "Epoch 280/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1717 - soft_acc: 0.8436 - val_loss: 1.2098 - val_soft_acc: 0.6289\n",
      "Epoch 281/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0955 - soft_acc: 0.8920 - val_loss: 1.1408 - val_soft_acc: 0.6502\n",
      "Epoch 282/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0828 - soft_acc: 0.9112 - val_loss: 1.1226 - val_soft_acc: 0.6615\n",
      "Epoch 283/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0605 - soft_acc: 0.9245 - val_loss: 1.1098 - val_soft_acc: 0.6650\n",
      "Epoch 284/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0752 - soft_acc: 0.9231 - val_loss: 1.1396 - val_soft_acc: 0.6609\n",
      "Epoch 285/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2059 - soft_acc: 0.8367 - val_loss: 1.3972 - val_soft_acc: 0.5490\n",
      "Epoch 286/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4625 - soft_acc: 0.7056 - val_loss: 1.7652 - val_soft_acc: 0.4721\n",
      "Epoch 287/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7620 - soft_acc: 0.6038 - val_loss: 1.3440 - val_soft_acc: 0.5623\n",
      "Epoch 288/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3289 - soft_acc: 0.7479 - val_loss: 1.3844 - val_soft_acc: 0.5719\n",
      "Epoch 289/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2646 - soft_acc: 0.7808 - val_loss: 1.1843 - val_soft_acc: 0.6176\n",
      "Epoch 290/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1392 - soft_acc: 0.8589 - val_loss: 1.2978 - val_soft_acc: 0.6221\n",
      "Epoch 291/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1617 - soft_acc: 0.8528 - val_loss: 1.1270 - val_soft_acc: 0.6539\n",
      "Epoch 292/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0826 - soft_acc: 0.9076 - val_loss: 1.0847 - val_soft_acc: 0.6541\n",
      "Epoch 293/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0701 - soft_acc: 0.9174 - val_loss: 1.1092 - val_soft_acc: 0.6609\n",
      "Epoch 294/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0802 - soft_acc: 0.9183 - val_loss: 1.1032 - val_soft_acc: 0.6621\n",
      "Epoch 295/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0723 - soft_acc: 0.9194 - val_loss: 1.0880 - val_soft_acc: 0.6607\n",
      "Epoch 296/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1717 - soft_acc: 0.8560 - val_loss: 1.3353 - val_soft_acc: 0.5912\n",
      "Epoch 297/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6166 - soft_acc: 0.6556 - val_loss: 2.1200 - val_soft_acc: 0.4352\n",
      "Epoch 298/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7545 - soft_acc: 0.5956 - val_loss: 1.6356 - val_soft_acc: 0.5211\n",
      "Epoch 299/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2626 - soft_acc: 0.7658 - val_loss: 1.3704 - val_soft_acc: 0.5891\n",
      "Epoch 300/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1149 - soft_acc: 0.8676 - val_loss: 1.2116 - val_soft_acc: 0.6367\n",
      "Epoch 301/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0938 - soft_acc: 0.8894 - val_loss: 1.2022 - val_soft_acc: 0.6428\n",
      "Epoch 302/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0697 - soft_acc: 0.9173 - val_loss: 1.1367 - val_soft_acc: 0.6582\n",
      "Epoch 303/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0574 - soft_acc: 0.9298 - val_loss: 1.1199 - val_soft_acc: 0.6623\n",
      "Epoch 304/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0672 - soft_acc: 0.9213 - val_loss: 1.2002 - val_soft_acc: 0.6535\n",
      "Epoch 305/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1546 - soft_acc: 0.8753 - val_loss: 1.3972 - val_soft_acc: 0.5998\n",
      "Epoch 306/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4916 - soft_acc: 0.6880 - val_loss: 1.7432 - val_soft_acc: 0.4957\n",
      "Epoch 307/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7082 - soft_acc: 0.6077 - val_loss: 1.5857 - val_soft_acc: 0.5162\n",
      "Epoch 308/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2378 - soft_acc: 0.7739 - val_loss: 1.2793 - val_soft_acc: 0.6166\n",
      "Epoch 309/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1149 - soft_acc: 0.8700 - val_loss: 1.1963 - val_soft_acc: 0.6373\n",
      "Epoch 310/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1374 - soft_acc: 0.8756 - val_loss: 1.2646 - val_soft_acc: 0.6246\n",
      "Epoch 311/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1137 - soft_acc: 0.8853 - val_loss: 1.2291 - val_soft_acc: 0.6287\n",
      "Epoch 312/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1585 - soft_acc: 0.8547 - val_loss: 1.3444 - val_soft_acc: 0.6141\n",
      "Epoch 313/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0965 - soft_acc: 0.8881 - val_loss: 1.1878 - val_soft_acc: 0.6506\n",
      "Epoch 314/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0894 - soft_acc: 0.9100 - val_loss: 1.1879 - val_soft_acc: 0.6471\n",
      "Epoch 315/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0686 - soft_acc: 0.9185 - val_loss: 1.1553 - val_soft_acc: 0.6547\n",
      "Epoch 316/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0873 - soft_acc: 0.9084 - val_loss: 1.1710 - val_soft_acc: 0.6537\n",
      "Epoch 317/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2033 - soft_acc: 0.8336 - val_loss: 1.8576 - val_soft_acc: 0.5047\n",
      "Epoch 318/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.8272 - soft_acc: 0.5826 - val_loss: 1.6568 - val_soft_acc: 0.5027\n",
      "Epoch 319/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3468 - soft_acc: 0.7313 - val_loss: 1.3099 - val_soft_acc: 0.5967\n",
      "Epoch 320/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1183 - soft_acc: 0.8652 - val_loss: 1.1982 - val_soft_acc: 0.6377\n",
      "Epoch 321/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0699 - soft_acc: 0.9150 - val_loss: 1.1561 - val_soft_acc: 0.6695\n",
      "Epoch 322/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0620 - soft_acc: 0.9282 - val_loss: 1.1495 - val_soft_acc: 0.6635\n",
      "Epoch 323/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.0699 - soft_acc: 0.9213 - val_loss: 1.1950 - val_soft_acc: 0.6559\n",
      "Epoch 324/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1940 - soft_acc: 0.8421 - val_loss: 1.5753 - val_soft_acc: 0.5535\n",
      "Epoch 325/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4705 - soft_acc: 0.6926 - val_loss: 1.4527 - val_soft_acc: 0.5615\n",
      "Epoch 326/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3401 - soft_acc: 0.7429 - val_loss: 1.3651 - val_soft_acc: 0.5832\n",
      "Epoch 327/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2331 - soft_acc: 0.8076 - val_loss: 1.2860 - val_soft_acc: 0.6143\n",
      "Epoch 328/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1761 - soft_acc: 0.8373 - val_loss: 1.2507 - val_soft_acc: 0.6344\n",
      "Epoch 329/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1004 - soft_acc: 0.8894 - val_loss: 1.1692 - val_soft_acc: 0.6543\n",
      "Epoch 330/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0783 - soft_acc: 0.9122 - val_loss: 1.1957 - val_soft_acc: 0.6576\n",
      "Epoch 331/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1033 - soft_acc: 0.8967 - val_loss: 1.2206 - val_soft_acc: 0.6375\n",
      "Epoch 332/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0975 - soft_acc: 0.9005 - val_loss: 1.1647 - val_soft_acc: 0.6467\n",
      "Epoch 333/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1870 - soft_acc: 0.8401 - val_loss: 1.5126 - val_soft_acc: 0.5568\n",
      "Epoch 334/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.8948 - soft_acc: 0.5664 - val_loss: 1.5147 - val_soft_acc: 0.5555\n",
      "Epoch 335/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3064 - soft_acc: 0.7495 - val_loss: 1.2514 - val_soft_acc: 0.6154\n",
      "Epoch 336/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1278 - soft_acc: 0.8632 - val_loss: 1.1177 - val_soft_acc: 0.6590\n",
      "Epoch 337/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0830 - soft_acc: 0.9061 - val_loss: 1.1071 - val_soft_acc: 0.6643\n",
      "Epoch 338/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0622 - soft_acc: 0.9268 - val_loss: 1.0639 - val_soft_acc: 0.6777\n",
      "Epoch 339/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0592 - soft_acc: 0.9371 - val_loss: 1.0953 - val_soft_acc: 0.6652\n",
      "Epoch 340/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1251 - soft_acc: 0.8917 - val_loss: 1.1830 - val_soft_acc: 0.6535\n",
      "Epoch 341/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1041 - soft_acc: 0.9005 - val_loss: 1.2494 - val_soft_acc: 0.6293\n",
      "Epoch 342/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2052 - soft_acc: 0.8255 - val_loss: 1.4094 - val_soft_acc: 0.5721\n",
      "Epoch 343/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4112 - soft_acc: 0.7180 - val_loss: 1.3994 - val_soft_acc: 0.5684\n",
      "Epoch 344/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4081 - soft_acc: 0.7178 - val_loss: 1.2399 - val_soft_acc: 0.6156\n",
      "Epoch 345/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1303 - soft_acc: 0.8494 - val_loss: 1.1359 - val_soft_acc: 0.6553\n",
      "Epoch 346/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0708 - soft_acc: 0.9182 - val_loss: 1.0775 - val_soft_acc: 0.6629\n",
      "Epoch 347/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0567 - soft_acc: 0.9320 - val_loss: 1.1087 - val_soft_acc: 0.6658\n",
      "Epoch 348/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0627 - soft_acc: 0.9318 - val_loss: 1.1431 - val_soft_acc: 0.6652\n",
      "Epoch 349/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1005 - soft_acc: 0.9099 - val_loss: 1.1205 - val_soft_acc: 0.6617\n",
      "Epoch 350/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0768 - soft_acc: 0.9083 - val_loss: 1.1182 - val_soft_acc: 0.6750\n",
      "Epoch 351/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0513 - soft_acc: 0.9342 - val_loss: 1.0570 - val_soft_acc: 0.6785\n",
      "Epoch 352/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.0810 - soft_acc: 0.9199 - val_loss: 1.2021 - val_soft_acc: 0.6395\n",
      "Epoch 353/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6329 - soft_acc: 0.6442 - val_loss: 1.6875 - val_soft_acc: 0.4955\n",
      "Epoch 354/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4795 - soft_acc: 0.6903 - val_loss: 1.4244 - val_soft_acc: 0.5613\n",
      "Epoch 355/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2934 - soft_acc: 0.7581 - val_loss: 1.2335 - val_soft_acc: 0.6236\n",
      "Epoch 356/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1191 - soft_acc: 0.8651 - val_loss: 1.1452 - val_soft_acc: 0.6543\n",
      "Epoch 357/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0704 - soft_acc: 0.9118 - val_loss: 1.0554 - val_soft_acc: 0.6801\n",
      "Epoch 358/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.0413 - soft_acc: 0.9427 - val_loss: 1.0470 - val_soft_acc: 0.6936\n",
      "Epoch 359/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0559 - soft_acc: 0.9422 - val_loss: 1.1631 - val_soft_acc: 0.6643\n",
      "Epoch 360/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0864 - soft_acc: 0.9134 - val_loss: 1.1405 - val_soft_acc: 0.6662\n",
      "Epoch 361/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0624 - soft_acc: 0.9268 - val_loss: 1.1106 - val_soft_acc: 0.6754\n",
      "Epoch 362/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0800 - soft_acc: 0.9247 - val_loss: 1.2927 - val_soft_acc: 0.6262\n",
      "Epoch 363/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5498 - soft_acc: 0.6863 - val_loss: 1.8191 - val_soft_acc: 0.4664\n",
      "Epoch 364/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5506 - soft_acc: 0.6642 - val_loss: 1.4531 - val_soft_acc: 0.5643\n",
      "Epoch 365/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1711 - soft_acc: 0.8241 - val_loss: 1.1599 - val_soft_acc: 0.6398\n",
      "Epoch 366/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0999 - soft_acc: 0.8944 - val_loss: 1.0802 - val_soft_acc: 0.6701\n",
      "Epoch 367/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0467 - soft_acc: 0.9342 - val_loss: 1.0899 - val_soft_acc: 0.6775\n",
      "Epoch 368/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0351 - soft_acc: 0.9471 - val_loss: 1.0614 - val_soft_acc: 0.6816\n",
      "Epoch 369/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0275 - soft_acc: 0.9559 - val_loss: 1.0518 - val_soft_acc: 0.6939\n",
      "Epoch 370/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0263 - soft_acc: 0.9612 - val_loss: 1.1189 - val_soft_acc: 0.6662\n",
      "Epoch 371/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0840 - soft_acc: 0.9173 - val_loss: 1.1743 - val_soft_acc: 0.6615\n",
      "Epoch 372/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1519 - soft_acc: 0.8703 - val_loss: 1.5348 - val_soft_acc: 0.5615\n",
      "Epoch 373/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6926 - soft_acc: 0.6188 - val_loss: 1.5847 - val_soft_acc: 0.5352\n",
      "Epoch 374/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2549 - soft_acc: 0.7701 - val_loss: 1.1923 - val_soft_acc: 0.6248\n",
      "Epoch 375/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0822 - soft_acc: 0.8913 - val_loss: 1.0831 - val_soft_acc: 0.6779\n",
      "Epoch 376/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0471 - soft_acc: 0.9325 - val_loss: 1.1614 - val_soft_acc: 0.6764\n",
      "Epoch 377/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0585 - soft_acc: 0.9315 - val_loss: 1.0670 - val_soft_acc: 0.6912\n",
      "Epoch 378/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0356 - soft_acc: 0.9513 - val_loss: 1.0721 - val_soft_acc: 0.6857\n",
      "Epoch 379/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0612 - soft_acc: 0.9366 - val_loss: 1.1966 - val_soft_acc: 0.6658\n",
      "Epoch 380/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0621 - soft_acc: 0.9232 - val_loss: 1.0952 - val_soft_acc: 0.6889\n",
      "Epoch 381/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.0435 - soft_acc: 0.9398 - val_loss: 1.1665 - val_soft_acc: 0.6596\n",
      "Epoch 382/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5696 - soft_acc: 0.6964 - val_loss: 1.6434 - val_soft_acc: 0.4936\n",
      "Epoch 383/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4131 - soft_acc: 0.7082 - val_loss: 1.4664 - val_soft_acc: 0.5684\n",
      "Epoch 384/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2216 - soft_acc: 0.8026 - val_loss: 1.1695 - val_soft_acc: 0.6447\n",
      "Epoch 385/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0637 - soft_acc: 0.9090 - val_loss: 1.0636 - val_soft_acc: 0.6848\n",
      "Epoch 386/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0370 - soft_acc: 0.9457 - val_loss: 1.1145 - val_soft_acc: 0.6762\n",
      "Epoch 387/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1193 - soft_acc: 0.9046 - val_loss: 1.2219 - val_soft_acc: 0.6449\n",
      "Epoch 388/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1028 - soft_acc: 0.9018 - val_loss: 1.1078 - val_soft_acc: 0.6742\n",
      "Epoch 389/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0733 - soft_acc: 0.9188 - val_loss: 1.0803 - val_soft_acc: 0.6684\n",
      "Epoch 390/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0779 - soft_acc: 0.9122 - val_loss: 1.0734 - val_soft_acc: 0.6775\n",
      "Epoch 391/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0680 - soft_acc: 0.9230 - val_loss: 1.0557 - val_soft_acc: 0.6775\n",
      "Epoch 392/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0396 - soft_acc: 0.9437 - val_loss: 1.0223 - val_soft_acc: 0.6930\n",
      "Epoch 393/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0634 - soft_acc: 0.9344 - val_loss: 1.1896 - val_soft_acc: 0.6414\n",
      "Epoch 394/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5232 - soft_acc: 0.7034 - val_loss: 2.0663 - val_soft_acc: 0.4309\n",
      "Epoch 395/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5814 - soft_acc: 0.6512 - val_loss: 1.3778 - val_soft_acc: 0.5617\n",
      "Epoch 396/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1701 - soft_acc: 0.8245 - val_loss: 1.2517 - val_soft_acc: 0.6129\n",
      "Epoch 397/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0676 - soft_acc: 0.9073 - val_loss: 1.0652 - val_soft_acc: 0.6799\n",
      "Epoch 398/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0537 - soft_acc: 0.9348 - val_loss: 1.0871 - val_soft_acc: 0.6703\n",
      "Epoch 399/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0737 - soft_acc: 0.9284 - val_loss: 1.1519 - val_soft_acc: 0.6613\n",
      "Epoch 400/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1051 - soft_acc: 0.8969 - val_loss: 1.0783 - val_soft_acc: 0.6852\n",
      "Epoch 401/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0293 - soft_acc: 0.9500 - val_loss: 1.0494 - val_soft_acc: 0.6945\n",
      "Epoch 402/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0290 - soft_acc: 0.9578 - val_loss: 1.0932 - val_soft_acc: 0.6871\n",
      "Epoch 403/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0507 - soft_acc: 0.9404 - val_loss: 1.1671 - val_soft_acc: 0.6615\n",
      "Epoch 404/5000\n",
      "14640/14640 [==============================] - 1s 74us/step - loss: 0.2101 - soft_acc: 0.8496 - val_loss: 1.3893 - val_soft_acc: 0.6025\n",
      "Epoch 405/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3727 - soft_acc: 0.7439 - val_loss: 1.5336 - val_soft_acc: 0.5543\n",
      "Epoch 406/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3399 - soft_acc: 0.7497 - val_loss: 1.6449 - val_soft_acc: 0.5328\n",
      "Epoch 407/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2233 - soft_acc: 0.8008 - val_loss: 1.1774 - val_soft_acc: 0.6365\n",
      "Epoch 408/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0729 - soft_acc: 0.9067 - val_loss: 1.0863 - val_soft_acc: 0.6721\n",
      "Epoch 409/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0364 - soft_acc: 0.9439 - val_loss: 1.0372 - val_soft_acc: 0.6904\n",
      "Epoch 410/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0303 - soft_acc: 0.9547 - val_loss: 1.0310 - val_soft_acc: 0.6977\n",
      "Epoch 411/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0243 - soft_acc: 0.9594 - val_loss: 1.0035 - val_soft_acc: 0.6984\n",
      "Epoch 412/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0372 - soft_acc: 0.9517 - val_loss: 1.1428 - val_soft_acc: 0.6865\n",
      "Epoch 413/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1265 - soft_acc: 0.8885 - val_loss: 1.2292 - val_soft_acc: 0.6201\n",
      "Epoch 414/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4866 - soft_acc: 0.7017 - val_loss: 1.3707 - val_soft_acc: 0.5738\n",
      "Epoch 415/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3103 - soft_acc: 0.7547 - val_loss: 1.4575 - val_soft_acc: 0.5680\n",
      "Epoch 416/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3121 - soft_acc: 0.7739 - val_loss: 1.1907 - val_soft_acc: 0.6246\n",
      "Epoch 417/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1051 - soft_acc: 0.8762 - val_loss: 1.1188 - val_soft_acc: 0.6838\n",
      "Epoch 418/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0354 - soft_acc: 0.9392 - val_loss: 1.0449 - val_soft_acc: 0.6957\n",
      "Epoch 419/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0215 - soft_acc: 0.9606 - val_loss: 1.0405 - val_soft_acc: 0.7025\n",
      "Epoch 420/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0150 - soft_acc: 0.9702 - val_loss: 1.0319 - val_soft_acc: 0.7098\n",
      "Epoch 421/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0134 - soft_acc: 0.9726 - val_loss: 1.0324 - val_soft_acc: 0.7109\n",
      "Epoch 422/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0204 - soft_acc: 0.9657 - val_loss: 1.0370 - val_soft_acc: 0.7057\n",
      "Epoch 423/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2386 - soft_acc: 0.8469 - val_loss: 1.5401 - val_soft_acc: 0.5420\n",
      "Epoch 424/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4714 - soft_acc: 0.6996 - val_loss: 1.6486 - val_soft_acc: 0.5299\n",
      "Epoch 425/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3166 - soft_acc: 0.7611 - val_loss: 1.2647 - val_soft_acc: 0.6201\n",
      "Epoch 426/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0841 - soft_acc: 0.8939 - val_loss: 1.1361 - val_soft_acc: 0.6559\n",
      "Epoch 427/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0768 - soft_acc: 0.9135 - val_loss: 1.1350 - val_soft_acc: 0.6762\n",
      "Epoch 428/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0363 - soft_acc: 0.9449 - val_loss: 1.0519 - val_soft_acc: 0.6918\n",
      "Epoch 429/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0319 - soft_acc: 0.9547 - val_loss: 1.0979 - val_soft_acc: 0.6893\n",
      "Epoch 430/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0828 - soft_acc: 0.9195 - val_loss: 1.2024 - val_soft_acc: 0.6471\n",
      "Epoch 431/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1233 - soft_acc: 0.8890 - val_loss: 1.2215 - val_soft_acc: 0.6539\n",
      "Epoch 432/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1730 - soft_acc: 0.8513 - val_loss: 1.6625 - val_soft_acc: 0.5619\n",
      "Epoch 433/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4313 - soft_acc: 0.7144 - val_loss: 1.6061 - val_soft_acc: 0.5742\n",
      "Epoch 434/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2437 - soft_acc: 0.8043 - val_loss: 1.2354 - val_soft_acc: 0.6426\n",
      "Epoch 435/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.1540 - soft_acc: 0.8529 - val_loss: 1.1683 - val_soft_acc: 0.6613\n",
      "Epoch 436/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0442 - soft_acc: 0.9320 - val_loss: 1.0670 - val_soft_acc: 0.6982\n",
      "Epoch 437/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0208 - soft_acc: 0.9597 - val_loss: 1.0595 - val_soft_acc: 0.6988\n",
      "Epoch 438/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0167 - soft_acc: 0.9675 - val_loss: 1.0585 - val_soft_acc: 0.7041\n",
      "Epoch 439/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0098 - soft_acc: 0.9759 - val_loss: 1.0428 - val_soft_acc: 0.7061\n",
      "Epoch 440/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0489 - soft_acc: 0.9577 - val_loss: 1.2133 - val_soft_acc: 0.6617\n",
      "Epoch 441/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4734 - soft_acc: 0.7569 - val_loss: 1.8972 - val_soft_acc: 0.4543\n",
      "Epoch 442/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4666 - soft_acc: 0.6971 - val_loss: 1.3276 - val_soft_acc: 0.6113\n",
      "Epoch 443/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1296 - soft_acc: 0.8555 - val_loss: 1.2239 - val_soft_acc: 0.6598\n",
      "Epoch 444/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0546 - soft_acc: 0.9227 - val_loss: 1.0725 - val_soft_acc: 0.6914\n",
      "Epoch 445/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0226 - soft_acc: 0.9584 - val_loss: 1.0622 - val_soft_acc: 0.7025\n",
      "Epoch 446/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0114 - soft_acc: 0.9705 - val_loss: 1.0611 - val_soft_acc: 0.7094\n",
      "Epoch 447/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0079 - soft_acc: 0.9766 - val_loss: 1.0613 - val_soft_acc: 0.7109\n",
      "Epoch 448/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0056 - soft_acc: 0.9803 - val_loss: 1.0607 - val_soft_acc: 0.7090\n",
      "Epoch 449/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0129 - soft_acc: 0.9752 - val_loss: 1.0624 - val_soft_acc: 0.7113\n",
      "Epoch 450/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0147 - soft_acc: 0.9720 - val_loss: 1.0840 - val_soft_acc: 0.7039\n",
      "Epoch 451/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0272 - soft_acc: 0.9650 - val_loss: 1.1508 - val_soft_acc: 0.6773\n",
      "Epoch 452/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6946 - soft_acc: 0.6778 - val_loss: 2.0540 - val_soft_acc: 0.4547\n",
      "Epoch 453/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5300 - soft_acc: 0.6644 - val_loss: 1.3484 - val_soft_acc: 0.5824\n",
      "Epoch 454/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1421 - soft_acc: 0.8375 - val_loss: 1.1600 - val_soft_acc: 0.6580\n",
      "Epoch 455/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0539 - soft_acc: 0.9244 - val_loss: 1.1826 - val_soft_acc: 0.6559\n",
      "Epoch 456/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0425 - soft_acc: 0.9416 - val_loss: 1.1086 - val_soft_acc: 0.6842\n",
      "Epoch 457/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0198 - soft_acc: 0.9620 - val_loss: 1.0726 - val_soft_acc: 0.6996\n",
      "Epoch 458/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0104 - soft_acc: 0.9723 - val_loss: 1.0505 - val_soft_acc: 0.7045\n",
      "Epoch 459/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0066 - soft_acc: 0.9790 - val_loss: 1.0565 - val_soft_acc: 0.7045\n",
      "Epoch 460/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0334 - soft_acc: 0.9596 - val_loss: 1.2976 - val_soft_acc: 0.6494\n",
      "Epoch 461/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0721 - soft_acc: 0.9237 - val_loss: 1.0772 - val_soft_acc: 0.6830\n",
      "Epoch 00461: early stopping\n",
      ">0.683\n",
      "Train on 14640 samples, validate on 2440 samples\n",
      "Epoch 1/5000\n",
      "14640/14640 [==============================] - 2s 107us/step - loss: 4.9876 - soft_acc: 0.1553 - val_loss: 4.4867 - val_soft_acc: 0.1568\n",
      "Epoch 2/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 4.2142 - soft_acc: 0.1639 - val_loss: 4.0364 - val_soft_acc: 0.1629\n",
      "Epoch 3/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 4.1425 - soft_acc: 0.1680 - val_loss: 3.9365 - val_soft_acc: 0.1750\n",
      "Epoch 4/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 4.0069 - soft_acc: 0.1687 - val_loss: 4.0106 - val_soft_acc: 0.1785\n",
      "Epoch 5/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.9765 - soft_acc: 0.1685 - val_loss: 3.7818 - val_soft_acc: 0.1615\n",
      "Epoch 6/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 3.8970 - soft_acc: 0.1741 - val_loss: 3.6982 - val_soft_acc: 0.1818\n",
      "Epoch 7/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.8624 - soft_acc: 0.1777 - val_loss: 3.8189 - val_soft_acc: 0.1633\n",
      "Epoch 8/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.8813 - soft_acc: 0.1751 - val_loss: 3.9352 - val_soft_acc: 0.1682\n",
      "Epoch 9/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.8025 - soft_acc: 0.1781 - val_loss: 3.7315 - val_soft_acc: 0.1973\n",
      "Epoch 10/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.7971 - soft_acc: 0.1787 - val_loss: 3.8239 - val_soft_acc: 0.1658\n",
      "Epoch 11/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.7654 - soft_acc: 0.1791 - val_loss: 3.6709 - val_soft_acc: 0.1879\n",
      "Epoch 12/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.7335 - soft_acc: 0.1820 - val_loss: 3.6074 - val_soft_acc: 0.1855\n",
      "Epoch 13/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.7284 - soft_acc: 0.1819 - val_loss: 3.6902 - val_soft_acc: 0.1770\n",
      "Epoch 14/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.6794 - soft_acc: 0.1879 - val_loss: 3.6158 - val_soft_acc: 0.1809\n",
      "Epoch 15/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.6842 - soft_acc: 0.1862 - val_loss: 3.5802 - val_soft_acc: 0.1852\n",
      "Epoch 16/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.6471 - soft_acc: 0.1857 - val_loss: 3.7792 - val_soft_acc: 0.1994\n",
      "Epoch 17/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.6480 - soft_acc: 0.1874 - val_loss: 3.7189 - val_soft_acc: 0.1928\n",
      "Epoch 18/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.6380 - soft_acc: 0.1880 - val_loss: 3.6940 - val_soft_acc: 0.1824\n",
      "Epoch 19/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.5902 - soft_acc: 0.1893 - val_loss: 3.7197 - val_soft_acc: 0.1951\n",
      "Epoch 20/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 3.5609 - soft_acc: 0.1916 - val_loss: 3.4692 - val_soft_acc: 0.2109\n",
      "Epoch 21/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.5633 - soft_acc: 0.1904 - val_loss: 3.4596 - val_soft_acc: 0.1895\n",
      "Epoch 22/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.5383 - soft_acc: 0.1894 - val_loss: 3.4505 - val_soft_acc: 0.1887\n",
      "Epoch 23/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.5420 - soft_acc: 0.1927 - val_loss: 3.4574 - val_soft_acc: 0.1857\n",
      "Epoch 24/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.5042 - soft_acc: 0.1963 - val_loss: 3.4260 - val_soft_acc: 0.1992\n",
      "Epoch 25/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.5016 - soft_acc: 0.1975 - val_loss: 3.4831 - val_soft_acc: 0.1820\n",
      "Epoch 26/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 3.4604 - soft_acc: 0.1977 - val_loss: 3.4356 - val_soft_acc: 0.2113\n",
      "Epoch 27/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.4557 - soft_acc: 0.1993 - val_loss: 3.3449 - val_soft_acc: 0.1939\n",
      "Epoch 28/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.4310 - soft_acc: 0.2009 - val_loss: 3.4286 - val_soft_acc: 0.2047\n",
      "Epoch 29/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.4093 - soft_acc: 0.1997 - val_loss: 3.3687 - val_soft_acc: 0.2006\n",
      "Epoch 30/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.4036 - soft_acc: 0.2023 - val_loss: 3.2973 - val_soft_acc: 0.2068\n",
      "Epoch 31/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.3695 - soft_acc: 0.2052 - val_loss: 3.3301 - val_soft_acc: 0.2086\n",
      "Epoch 32/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.3498 - soft_acc: 0.2031 - val_loss: 3.2944 - val_soft_acc: 0.2207\n",
      "Epoch 33/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.3266 - soft_acc: 0.2051 - val_loss: 3.3401 - val_soft_acc: 0.2037\n",
      "Epoch 34/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.3256 - soft_acc: 0.2082 - val_loss: 3.3018 - val_soft_acc: 0.2014\n",
      "Epoch 35/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.2976 - soft_acc: 0.2059 - val_loss: 3.2873 - val_soft_acc: 0.2141\n",
      "Epoch 36/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.2799 - soft_acc: 0.2097 - val_loss: 3.2994 - val_soft_acc: 0.2061\n",
      "Epoch 37/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.2664 - soft_acc: 0.2107 - val_loss: 3.2078 - val_soft_acc: 0.2172\n",
      "Epoch 38/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.2343 - soft_acc: 0.2151 - val_loss: 3.2055 - val_soft_acc: 0.2133\n",
      "Epoch 39/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.2106 - soft_acc: 0.2131 - val_loss: 3.1817 - val_soft_acc: 0.2102\n",
      "Epoch 40/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.1953 - soft_acc: 0.2164 - val_loss: 3.2638 - val_soft_acc: 0.2189\n",
      "Epoch 41/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.1912 - soft_acc: 0.2185 - val_loss: 3.1369 - val_soft_acc: 0.2201\n",
      "Epoch 42/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.1681 - soft_acc: 0.2183 - val_loss: 3.1309 - val_soft_acc: 0.2273\n",
      "Epoch 43/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.1593 - soft_acc: 0.2179 - val_loss: 3.1347 - val_soft_acc: 0.2391\n",
      "Epoch 44/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.1269 - soft_acc: 0.2205 - val_loss: 3.2225 - val_soft_acc: 0.2270\n",
      "Epoch 45/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.1089 - soft_acc: 0.2239 - val_loss: 3.0563 - val_soft_acc: 0.2256\n",
      "Epoch 46/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.0877 - soft_acc: 0.2245 - val_loss: 3.0594 - val_soft_acc: 0.2285\n",
      "Epoch 47/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.0821 - soft_acc: 0.2285 - val_loss: 3.0193 - val_soft_acc: 0.2225\n",
      "Epoch 48/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.0189 - soft_acc: 0.2296 - val_loss: 3.0274 - val_soft_acc: 0.2150\n",
      "Epoch 49/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 3.0176 - soft_acc: 0.2282 - val_loss: 3.0533 - val_soft_acc: 0.2193\n",
      "Epoch 50/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.0073 - soft_acc: 0.2314 - val_loss: 3.1778 - val_soft_acc: 0.2215\n",
      "Epoch 51/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.9788 - soft_acc: 0.2359 - val_loss: 3.1544 - val_soft_acc: 0.2184\n",
      "Epoch 52/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.0022 - soft_acc: 0.2323 - val_loss: 2.9979 - val_soft_acc: 0.2174\n",
      "Epoch 53/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.9291 - soft_acc: 0.2369 - val_loss: 3.0310 - val_soft_acc: 0.2369\n",
      "Epoch 54/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.9304 - soft_acc: 0.2380 - val_loss: 3.0158 - val_soft_acc: 0.2443\n",
      "Epoch 55/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.9055 - soft_acc: 0.2422 - val_loss: 3.0426 - val_soft_acc: 0.2441\n",
      "Epoch 56/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.8866 - soft_acc: 0.2435 - val_loss: 2.9093 - val_soft_acc: 0.2416\n",
      "Epoch 57/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.8523 - soft_acc: 0.2467 - val_loss: 3.0636 - val_soft_acc: 0.2242\n",
      "Epoch 58/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.8548 - soft_acc: 0.2428 - val_loss: 2.8468 - val_soft_acc: 0.2510\n",
      "Epoch 59/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.8127 - soft_acc: 0.2487 - val_loss: 2.8183 - val_soft_acc: 0.2568\n",
      "Epoch 60/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.7748 - soft_acc: 0.2480 - val_loss: 2.8666 - val_soft_acc: 0.2471\n",
      "Epoch 61/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.7616 - soft_acc: 0.2540 - val_loss: 2.7992 - val_soft_acc: 0.2473\n",
      "Epoch 62/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.7074 - soft_acc: 0.2583 - val_loss: 2.7846 - val_soft_acc: 0.2561\n",
      "Epoch 63/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.7387 - soft_acc: 0.2543 - val_loss: 2.7259 - val_soft_acc: 0.2600\n",
      "Epoch 64/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 2.6933 - soft_acc: 0.2585 - val_loss: 2.9084 - val_soft_acc: 0.2291\n",
      "Epoch 65/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.6751 - soft_acc: 0.2606 - val_loss: 2.7459 - val_soft_acc: 0.2598\n",
      "Epoch 66/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.6334 - soft_acc: 0.2651 - val_loss: 2.6768 - val_soft_acc: 0.2717\n",
      "Epoch 67/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.5993 - soft_acc: 0.2638 - val_loss: 2.8187 - val_soft_acc: 0.2490\n",
      "Epoch 68/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.6195 - soft_acc: 0.2649 - val_loss: 2.8876 - val_soft_acc: 0.2588\n",
      "Epoch 69/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.5933 - soft_acc: 0.2675 - val_loss: 2.7345 - val_soft_acc: 0.2570\n",
      "Epoch 70/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.5884 - soft_acc: 0.2700 - val_loss: 2.6686 - val_soft_acc: 0.2527\n",
      "Epoch 71/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.4914 - soft_acc: 0.2783 - val_loss: 2.6398 - val_soft_acc: 0.2732\n",
      "Epoch 72/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.4816 - soft_acc: 0.2789 - val_loss: 2.6649 - val_soft_acc: 0.2668\n",
      "Epoch 73/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.4602 - soft_acc: 0.2828 - val_loss: 2.5913 - val_soft_acc: 0.2727\n",
      "Epoch 74/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.4402 - soft_acc: 0.2824 - val_loss: 2.5715 - val_soft_acc: 0.2785\n",
      "Epoch 75/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.3901 - soft_acc: 0.2880 - val_loss: 2.6144 - val_soft_acc: 0.2873\n",
      "Epoch 76/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.3711 - soft_acc: 0.2955 - val_loss: 2.5785 - val_soft_acc: 0.2807\n",
      "Epoch 77/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.3466 - soft_acc: 0.2948 - val_loss: 2.6376 - val_soft_acc: 0.2797\n",
      "Epoch 78/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.2813 - soft_acc: 0.2998 - val_loss: 2.5456 - val_soft_acc: 0.2811\n",
      "Epoch 79/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.2617 - soft_acc: 0.3050 - val_loss: 2.4821 - val_soft_acc: 0.2955\n",
      "Epoch 80/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.2239 - soft_acc: 0.3058 - val_loss: 2.4756 - val_soft_acc: 0.2914\n",
      "Epoch 81/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 2.1992 - soft_acc: 0.3113 - val_loss: 2.5176 - val_soft_acc: 0.2914\n",
      "Epoch 82/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.2216 - soft_acc: 0.3071 - val_loss: 2.6250 - val_soft_acc: 0.2891\n",
      "Epoch 83/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.2126 - soft_acc: 0.3117 - val_loss: 2.4462 - val_soft_acc: 0.3049\n",
      "Epoch 84/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.2328 - soft_acc: 0.3062 - val_loss: 2.5206 - val_soft_acc: 0.2920\n",
      "Epoch 85/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.0790 - soft_acc: 0.3247 - val_loss: 2.3981 - val_soft_acc: 0.3113\n",
      "Epoch 86/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.0838 - soft_acc: 0.3278 - val_loss: 2.2641 - val_soft_acc: 0.3209\n",
      "Epoch 87/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.0464 - soft_acc: 0.3224 - val_loss: 2.3470 - val_soft_acc: 0.3121\n",
      "Epoch 88/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.0217 - soft_acc: 0.3321 - val_loss: 2.4784 - val_soft_acc: 0.3109\n",
      "Epoch 89/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.0243 - soft_acc: 0.3304 - val_loss: 2.3427 - val_soft_acc: 0.3254\n",
      "Epoch 90/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.9692 - soft_acc: 0.3391 - val_loss: 2.3731 - val_soft_acc: 0.3059\n",
      "Epoch 91/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.9196 - soft_acc: 0.3473 - val_loss: 2.2646 - val_soft_acc: 0.3248\n",
      "Epoch 92/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 1.8673 - soft_acc: 0.3517 - val_loss: 2.2914 - val_soft_acc: 0.3379\n",
      "Epoch 93/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.8515 - soft_acc: 0.3593 - val_loss: 2.4125 - val_soft_acc: 0.3166\n",
      "Epoch 94/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.7867 - soft_acc: 0.3618 - val_loss: 2.1817 - val_soft_acc: 0.3266\n",
      "Epoch 95/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.8087 - soft_acc: 0.3585 - val_loss: 2.2091 - val_soft_acc: 0.3367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.7715 - soft_acc: 0.3668 - val_loss: 2.2162 - val_soft_acc: 0.3373\n",
      "Epoch 97/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.7449 - soft_acc: 0.3661 - val_loss: 2.2605 - val_soft_acc: 0.3408\n",
      "Epoch 98/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.7075 - soft_acc: 0.3786 - val_loss: 2.1964 - val_soft_acc: 0.3455\n",
      "Epoch 99/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.6800 - soft_acc: 0.3777 - val_loss: 2.2916 - val_soft_acc: 0.3328\n",
      "Epoch 100/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.6622 - soft_acc: 0.3811 - val_loss: 2.1229 - val_soft_acc: 0.3619\n",
      "Epoch 101/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.6091 - soft_acc: 0.3902 - val_loss: 2.0863 - val_soft_acc: 0.3574\n",
      "Epoch 102/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.6145 - soft_acc: 0.3883 - val_loss: 2.0373 - val_soft_acc: 0.3701\n",
      "Epoch 103/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.5416 - soft_acc: 0.4032 - val_loss: 2.1483 - val_soft_acc: 0.3580\n",
      "Epoch 104/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.4986 - soft_acc: 0.4099 - val_loss: 1.9899 - val_soft_acc: 0.3721\n",
      "Epoch 105/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.4951 - soft_acc: 0.4091 - val_loss: 2.1001 - val_soft_acc: 0.3840\n",
      "Epoch 106/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.5440 - soft_acc: 0.4068 - val_loss: 2.0660 - val_soft_acc: 0.3654\n",
      "Epoch 107/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.4570 - soft_acc: 0.4153 - val_loss: 2.1872 - val_soft_acc: 0.3533\n",
      "Epoch 108/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.4198 - soft_acc: 0.4211 - val_loss: 2.1744 - val_soft_acc: 0.3623\n",
      "Epoch 109/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.4525 - soft_acc: 0.4132 - val_loss: 2.0972 - val_soft_acc: 0.3707\n",
      "Epoch 110/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.4599 - soft_acc: 0.4173 - val_loss: 2.0210 - val_soft_acc: 0.3727\n",
      "Epoch 111/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.4105 - soft_acc: 0.4268 - val_loss: 1.9120 - val_soft_acc: 0.3939\n",
      "Epoch 112/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.3095 - soft_acc: 0.4492 - val_loss: 1.8824 - val_soft_acc: 0.3891\n",
      "Epoch 113/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.3100 - soft_acc: 0.4451 - val_loss: 1.8938 - val_soft_acc: 0.3926\n",
      "Epoch 114/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.2494 - soft_acc: 0.4526 - val_loss: 1.8532 - val_soft_acc: 0.3982\n",
      "Epoch 115/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.2394 - soft_acc: 0.4639 - val_loss: 1.8870 - val_soft_acc: 0.4061\n",
      "Epoch 116/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.2154 - soft_acc: 0.4643 - val_loss: 1.8545 - val_soft_acc: 0.4174\n",
      "Epoch 117/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.3230 - soft_acc: 0.4389 - val_loss: 2.0298 - val_soft_acc: 0.3799\n",
      "Epoch 118/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.2470 - soft_acc: 0.4554 - val_loss: 1.8897 - val_soft_acc: 0.4020\n",
      "Epoch 119/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.1944 - soft_acc: 0.4750 - val_loss: 1.9660 - val_soft_acc: 0.3873\n",
      "Epoch 120/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.1739 - soft_acc: 0.4776 - val_loss: 1.9176 - val_soft_acc: 0.4023\n",
      "Epoch 121/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 1.1719 - soft_acc: 0.4755 - val_loss: 1.9200 - val_soft_acc: 0.4105\n",
      "Epoch 122/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.2215 - soft_acc: 0.4656 - val_loss: 1.8386 - val_soft_acc: 0.4109\n",
      "Epoch 123/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.0987 - soft_acc: 0.4914 - val_loss: 1.7702 - val_soft_acc: 0.4248\n",
      "Epoch 124/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.1833 - soft_acc: 0.4737 - val_loss: 1.7928 - val_soft_acc: 0.4213\n",
      "Epoch 125/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.0478 - soft_acc: 0.5030 - val_loss: 1.8630 - val_soft_acc: 0.4166\n",
      "Epoch 126/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.1492 - soft_acc: 0.4832 - val_loss: 1.8455 - val_soft_acc: 0.4227\n",
      "Epoch 127/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.0709 - soft_acc: 0.4994 - val_loss: 1.7876 - val_soft_acc: 0.4332\n",
      "Epoch 128/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.9593 - soft_acc: 0.5248 - val_loss: 1.8422 - val_soft_acc: 0.4172\n",
      "Epoch 129/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.0447 - soft_acc: 0.5052 - val_loss: 1.8671 - val_soft_acc: 0.4275\n",
      "Epoch 130/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.0887 - soft_acc: 0.4974 - val_loss: 1.7748 - val_soft_acc: 0.4414\n",
      "Epoch 131/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.9970 - soft_acc: 0.5180 - val_loss: 1.7567 - val_soft_acc: 0.4541\n",
      "Epoch 132/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.9567 - soft_acc: 0.5255 - val_loss: 1.7054 - val_soft_acc: 0.4461\n",
      "Epoch 133/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.9697 - soft_acc: 0.5254 - val_loss: 1.7891 - val_soft_acc: 0.4123\n",
      "Epoch 134/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.9246 - soft_acc: 0.5311 - val_loss: 1.6833 - val_soft_acc: 0.4451\n",
      "Epoch 135/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.8399 - soft_acc: 0.5646 - val_loss: 1.7737 - val_soft_acc: 0.4555\n",
      "Epoch 136/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8723 - soft_acc: 0.5463 - val_loss: 1.7753 - val_soft_acc: 0.4424\n",
      "Epoch 137/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8484 - soft_acc: 0.5598 - val_loss: 1.8820 - val_soft_acc: 0.4332\n",
      "Epoch 138/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.9382 - soft_acc: 0.5266 - val_loss: 1.7832 - val_soft_acc: 0.4457\n",
      "Epoch 139/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8953 - soft_acc: 0.5459 - val_loss: 1.7201 - val_soft_acc: 0.4643\n",
      "Epoch 140/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8629 - soft_acc: 0.5527 - val_loss: 1.6758 - val_soft_acc: 0.4656\n",
      "Epoch 141/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8872 - soft_acc: 0.5481 - val_loss: 1.7585 - val_soft_acc: 0.4447\n",
      "Epoch 142/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8601 - soft_acc: 0.5519 - val_loss: 1.6752 - val_soft_acc: 0.4719\n",
      "Epoch 143/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8023 - soft_acc: 0.5707 - val_loss: 1.6991 - val_soft_acc: 0.4578\n",
      "Epoch 144/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7877 - soft_acc: 0.5752 - val_loss: 1.7253 - val_soft_acc: 0.4551\n",
      "Epoch 145/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8160 - soft_acc: 0.5625 - val_loss: 1.6877 - val_soft_acc: 0.4541\n",
      "Epoch 146/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.7683 - soft_acc: 0.5770 - val_loss: 1.6607 - val_soft_acc: 0.4777\n",
      "Epoch 147/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7936 - soft_acc: 0.5718 - val_loss: 1.8772 - val_soft_acc: 0.4324\n",
      "Epoch 148/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.9258 - soft_acc: 0.5326 - val_loss: 1.8251 - val_soft_acc: 0.4611\n",
      "Epoch 149/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8893 - soft_acc: 0.5417 - val_loss: 1.6832 - val_soft_acc: 0.4615\n",
      "Epoch 150/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.8395 - soft_acc: 0.5651 - val_loss: 1.8070 - val_soft_acc: 0.4637\n",
      "Epoch 151/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7249 - soft_acc: 0.5914 - val_loss: 1.5548 - val_soft_acc: 0.4951\n",
      "Epoch 152/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7706 - soft_acc: 0.5798 - val_loss: 1.6370 - val_soft_acc: 0.4807\n",
      "Epoch 153/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6634 - soft_acc: 0.6129 - val_loss: 1.6487 - val_soft_acc: 0.4824\n",
      "Epoch 154/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6429 - soft_acc: 0.6258 - val_loss: 1.6595 - val_soft_acc: 0.4842\n",
      "Epoch 155/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6697 - soft_acc: 0.6140 - val_loss: 1.6407 - val_soft_acc: 0.4842\n",
      "Epoch 156/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6778 - soft_acc: 0.6086 - val_loss: 1.6160 - val_soft_acc: 0.4924\n",
      "Epoch 157/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6418 - soft_acc: 0.6176 - val_loss: 1.5957 - val_soft_acc: 0.4984\n",
      "Epoch 158/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6204 - soft_acc: 0.6268 - val_loss: 1.5944 - val_soft_acc: 0.4984\n",
      "Epoch 159/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6186 - soft_acc: 0.6290 - val_loss: 1.6675 - val_soft_acc: 0.4914\n",
      "Epoch 160/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7666 - soft_acc: 0.5830 - val_loss: 1.6834 - val_soft_acc: 0.4861\n",
      "Epoch 161/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6832 - soft_acc: 0.6083 - val_loss: 1.6849 - val_soft_acc: 0.4902\n",
      "Epoch 162/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7080 - soft_acc: 0.6012 - val_loss: 1.5556 - val_soft_acc: 0.5037\n",
      "Epoch 163/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6443 - soft_acc: 0.6187 - val_loss: 1.5324 - val_soft_acc: 0.5070\n",
      "Epoch 164/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.6208 - soft_acc: 0.6267 - val_loss: 1.7363 - val_soft_acc: 0.4957\n",
      "Epoch 165/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6315 - soft_acc: 0.6154 - val_loss: 1.6635 - val_soft_acc: 0.4953\n",
      "Epoch 166/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6668 - soft_acc: 0.6090 - val_loss: 1.7056 - val_soft_acc: 0.4893\n",
      "Epoch 167/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5742 - soft_acc: 0.6417 - val_loss: 1.5994 - val_soft_acc: 0.5141\n",
      "Epoch 168/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5547 - soft_acc: 0.6511 - val_loss: 1.7002 - val_soft_acc: 0.5029\n",
      "Epoch 169/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6355 - soft_acc: 0.6190 - val_loss: 1.7762 - val_soft_acc: 0.4693\n",
      "Epoch 170/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5917 - soft_acc: 0.6378 - val_loss: 1.7134 - val_soft_acc: 0.4836\n",
      "Epoch 171/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6685 - soft_acc: 0.6152 - val_loss: 1.7990 - val_soft_acc: 0.4889\n",
      "Epoch 172/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8138 - soft_acc: 0.5688 - val_loss: 1.7391 - val_soft_acc: 0.4689\n",
      "Epoch 173/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6841 - soft_acc: 0.6063 - val_loss: 1.5586 - val_soft_acc: 0.5195\n",
      "Epoch 174/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4938 - soft_acc: 0.6777 - val_loss: 1.5589 - val_soft_acc: 0.5369\n",
      "Epoch 175/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6324 - soft_acc: 0.6336 - val_loss: 1.6730 - val_soft_acc: 0.5010\n",
      "Epoch 176/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6366 - soft_acc: 0.6188 - val_loss: 1.6169 - val_soft_acc: 0.5193\n",
      "Epoch 177/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4625 - soft_acc: 0.6960 - val_loss: 1.5504 - val_soft_acc: 0.5383\n",
      "Epoch 178/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4571 - soft_acc: 0.7014 - val_loss: 1.4418 - val_soft_acc: 0.5441\n",
      "Epoch 179/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4336 - soft_acc: 0.7128 - val_loss: 1.5802 - val_soft_acc: 0.5371\n",
      "Epoch 180/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5159 - soft_acc: 0.6734 - val_loss: 1.5187 - val_soft_acc: 0.5369\n",
      "Epoch 181/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5765 - soft_acc: 0.6513 - val_loss: 1.5298 - val_soft_acc: 0.5191\n",
      "Epoch 182/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5495 - soft_acc: 0.6655 - val_loss: 1.6309 - val_soft_acc: 0.5248\n",
      "Epoch 183/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5487 - soft_acc: 0.6582 - val_loss: 1.6152 - val_soft_acc: 0.4883\n",
      "Epoch 184/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5994 - soft_acc: 0.6289 - val_loss: 1.7203 - val_soft_acc: 0.5111\n",
      "Epoch 185/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4692 - soft_acc: 0.6888 - val_loss: 1.4807 - val_soft_acc: 0.5393\n",
      "Epoch 186/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5141 - soft_acc: 0.6725 - val_loss: 1.5693 - val_soft_acc: 0.5246\n",
      "Epoch 187/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4822 - soft_acc: 0.6829 - val_loss: 1.5421 - val_soft_acc: 0.5270\n",
      "Epoch 188/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5612 - soft_acc: 0.6595 - val_loss: 1.7075 - val_soft_acc: 0.5057\n",
      "Epoch 189/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.4901 - soft_acc: 0.6780 - val_loss: 1.5539 - val_soft_acc: 0.5447\n",
      "Epoch 190/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4174 - soft_acc: 0.7138 - val_loss: 1.4846 - val_soft_acc: 0.5383\n",
      "Epoch 191/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4342 - soft_acc: 0.7033 - val_loss: 1.4526 - val_soft_acc: 0.5539\n",
      "Epoch 192/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3704 - soft_acc: 0.7343 - val_loss: 1.4289 - val_soft_acc: 0.5594\n",
      "Epoch 193/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.4342 - soft_acc: 0.7099 - val_loss: 1.4927 - val_soft_acc: 0.5621\n",
      "Epoch 194/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5681 - soft_acc: 0.6576 - val_loss: 1.6897 - val_soft_acc: 0.4861\n",
      "Epoch 195/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.7711 - soft_acc: 0.5890 - val_loss: 1.8121 - val_soft_acc: 0.4807\n",
      "Epoch 196/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5382 - soft_acc: 0.6483 - val_loss: 1.5090 - val_soft_acc: 0.5387\n",
      "Epoch 197/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3493 - soft_acc: 0.7440 - val_loss: 1.3595 - val_soft_acc: 0.5742\n",
      "Epoch 198/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2939 - soft_acc: 0.7751 - val_loss: 1.3909 - val_soft_acc: 0.5713\n",
      "Epoch 199/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3627 - soft_acc: 0.7462 - val_loss: 1.4574 - val_soft_acc: 0.5434\n",
      "Epoch 200/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4728 - soft_acc: 0.6878 - val_loss: 1.7311 - val_soft_acc: 0.5160\n",
      "Epoch 201/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5450 - soft_acc: 0.6522 - val_loss: 1.5962 - val_soft_acc: 0.5219\n",
      "Epoch 202/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4203 - soft_acc: 0.7012 - val_loss: 1.5957 - val_soft_acc: 0.5406\n",
      "Epoch 203/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3590 - soft_acc: 0.7409 - val_loss: 1.6290 - val_soft_acc: 0.5404\n",
      "Epoch 204/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3459 - soft_acc: 0.7479 - val_loss: 1.4202 - val_soft_acc: 0.5768\n",
      "Epoch 205/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3694 - soft_acc: 0.7430 - val_loss: 1.4582 - val_soft_acc: 0.5525\n",
      "Epoch 206/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4990 - soft_acc: 0.6968 - val_loss: 1.4532 - val_soft_acc: 0.5523\n",
      "Epoch 207/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4035 - soft_acc: 0.7184 - val_loss: 1.5343 - val_soft_acc: 0.5508\n",
      "Epoch 208/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.6364 - soft_acc: 0.6296 - val_loss: 1.6996 - val_soft_acc: 0.5268\n",
      "Epoch 209/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4094 - soft_acc: 0.7018 - val_loss: 1.4670 - val_soft_acc: 0.5607\n",
      "Epoch 210/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4593 - soft_acc: 0.6903 - val_loss: 1.4633 - val_soft_acc: 0.5619\n",
      "Epoch 211/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3547 - soft_acc: 0.7400 - val_loss: 1.4015 - val_soft_acc: 0.5658\n",
      "Epoch 212/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4485 - soft_acc: 0.7232 - val_loss: 1.5974 - val_soft_acc: 0.5252\n",
      "Epoch 213/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5229 - soft_acc: 0.6644 - val_loss: 1.5594 - val_soft_acc: 0.5330\n",
      "Epoch 214/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3772 - soft_acc: 0.7311 - val_loss: 1.3959 - val_soft_acc: 0.5732\n",
      "Epoch 215/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3191 - soft_acc: 0.7627 - val_loss: 1.3448 - val_soft_acc: 0.5787\n",
      "Epoch 216/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2710 - soft_acc: 0.7864 - val_loss: 1.3138 - val_soft_acc: 0.5941\n",
      "Epoch 217/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2809 - soft_acc: 0.7888 - val_loss: 1.4556 - val_soft_acc: 0.5855\n",
      "Epoch 218/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2970 - soft_acc: 0.7779 - val_loss: 1.3904 - val_soft_acc: 0.5908\n",
      "Epoch 219/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3379 - soft_acc: 0.7536 - val_loss: 1.4625 - val_soft_acc: 0.5613\n",
      "Epoch 220/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3805 - soft_acc: 0.7274 - val_loss: 1.6297 - val_soft_acc: 0.5213\n",
      "Epoch 221/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6524 - soft_acc: 0.6300 - val_loss: 1.7624 - val_soft_acc: 0.5209\n",
      "Epoch 222/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.4990 - soft_acc: 0.6785 - val_loss: 1.4860 - val_soft_acc: 0.5465\n",
      "Epoch 223/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3235 - soft_acc: 0.7537 - val_loss: 1.4732 - val_soft_acc: 0.5660\n",
      "Epoch 224/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3209 - soft_acc: 0.7645 - val_loss: 1.5032 - val_soft_acc: 0.5699\n",
      "Epoch 225/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3536 - soft_acc: 0.7457 - val_loss: 1.4562 - val_soft_acc: 0.5549\n",
      "Epoch 226/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3496 - soft_acc: 0.7390 - val_loss: 1.3618 - val_soft_acc: 0.5836\n",
      "Epoch 227/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3122 - soft_acc: 0.7718 - val_loss: 1.3926 - val_soft_acc: 0.5680\n",
      "Epoch 228/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3456 - soft_acc: 0.7440 - val_loss: 1.4157 - val_soft_acc: 0.5744\n",
      "Epoch 229/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3148 - soft_acc: 0.7648 - val_loss: 1.5548 - val_soft_acc: 0.5424\n",
      "Epoch 230/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6026 - soft_acc: 0.6509 - val_loss: 1.6522 - val_soft_acc: 0.5203\n",
      "Epoch 231/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5250 - soft_acc: 0.6626 - val_loss: 1.4208 - val_soft_acc: 0.5664\n",
      "Epoch 232/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2536 - soft_acc: 0.7844 - val_loss: 1.2940 - val_soft_acc: 0.6125\n",
      "Epoch 233/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1764 - soft_acc: 0.8424 - val_loss: 1.2421 - val_soft_acc: 0.6236\n",
      "Epoch 234/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2135 - soft_acc: 0.8308 - val_loss: 1.3207 - val_soft_acc: 0.6107\n",
      "Epoch 235/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4098 - soft_acc: 0.7260 - val_loss: 1.4167 - val_soft_acc: 0.5605\n",
      "Epoch 236/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.2873 - soft_acc: 0.7791 - val_loss: 1.3777 - val_soft_acc: 0.6006\n",
      "Epoch 237/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2227 - soft_acc: 0.8147 - val_loss: 1.3479 - val_soft_acc: 0.6031\n",
      "Epoch 238/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4547 - soft_acc: 0.7122 - val_loss: 1.7501 - val_soft_acc: 0.5139\n",
      "Epoch 239/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6876 - soft_acc: 0.6190 - val_loss: 1.8162 - val_soft_acc: 0.4980\n",
      "Epoch 240/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4688 - soft_acc: 0.6896 - val_loss: 1.4180 - val_soft_acc: 0.5650\n",
      "Epoch 241/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2291 - soft_acc: 0.7973 - val_loss: 1.3315 - val_soft_acc: 0.6053\n",
      "Epoch 242/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1721 - soft_acc: 0.8431 - val_loss: 1.3109 - val_soft_acc: 0.6215\n",
      "Epoch 243/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.1471 - soft_acc: 0.8672 - val_loss: 1.2547 - val_soft_acc: 0.6270\n",
      "Epoch 244/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1606 - soft_acc: 0.8693 - val_loss: 1.2388 - val_soft_acc: 0.6373\n",
      "Epoch 245/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1936 - soft_acc: 0.8514 - val_loss: 1.4178 - val_soft_acc: 0.5963\n",
      "Epoch 246/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3888 - soft_acc: 0.7385 - val_loss: 1.5064 - val_soft_acc: 0.5607\n",
      "Epoch 247/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5772 - soft_acc: 0.6587 - val_loss: 1.7235 - val_soft_acc: 0.4947\n",
      "Epoch 248/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6391 - soft_acc: 0.6377 - val_loss: 1.4844 - val_soft_acc: 0.5641\n",
      "Epoch 249/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3417 - soft_acc: 0.7344 - val_loss: 1.3601 - val_soft_acc: 0.6000\n",
      "Epoch 250/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3058 - soft_acc: 0.7688 - val_loss: 1.2953 - val_soft_acc: 0.5965\n",
      "Epoch 251/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2526 - soft_acc: 0.7989 - val_loss: 1.3448 - val_soft_acc: 0.6164\n",
      "Epoch 252/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1909 - soft_acc: 0.8407 - val_loss: 1.2278 - val_soft_acc: 0.6451\n",
      "Epoch 253/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1448 - soft_acc: 0.8694 - val_loss: 1.2774 - val_soft_acc: 0.6416\n",
      "Epoch 254/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1737 - soft_acc: 0.8598 - val_loss: 1.4676 - val_soft_acc: 0.5982\n",
      "Epoch 255/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3586 - soft_acc: 0.7554 - val_loss: 1.8635 - val_soft_acc: 0.4977\n",
      "Epoch 256/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5139 - soft_acc: 0.6777 - val_loss: 1.6246 - val_soft_acc: 0.5451\n",
      "Epoch 257/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3870 - soft_acc: 0.7192 - val_loss: 1.4046 - val_soft_acc: 0.5908\n",
      "Epoch 258/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3311 - soft_acc: 0.7566 - val_loss: 1.5413 - val_soft_acc: 0.5395\n",
      "Epoch 259/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2795 - soft_acc: 0.7769 - val_loss: 1.2901 - val_soft_acc: 0.6189\n",
      "Epoch 260/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1787 - soft_acc: 0.8401 - val_loss: 1.3011 - val_soft_acc: 0.6324\n",
      "Epoch 261/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1901 - soft_acc: 0.8466 - val_loss: 1.3560 - val_soft_acc: 0.6244\n",
      "Epoch 262/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1677 - soft_acc: 0.8635 - val_loss: 1.2786 - val_soft_acc: 0.6410\n",
      "Epoch 263/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1685 - soft_acc: 0.8620 - val_loss: 1.3749 - val_soft_acc: 0.5975\n",
      "Epoch 264/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2641 - soft_acc: 0.8167 - val_loss: 1.8037 - val_soft_acc: 0.5051\n",
      "Epoch 265/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.6935 - soft_acc: 0.6206 - val_loss: 1.5921 - val_soft_acc: 0.5203\n",
      "Epoch 266/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6269 - soft_acc: 0.6331 - val_loss: 1.6859 - val_soft_acc: 0.5211\n",
      "Epoch 267/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3461 - soft_acc: 0.7305 - val_loss: 1.3782 - val_soft_acc: 0.5928\n",
      "Epoch 268/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1703 - soft_acc: 0.8363 - val_loss: 1.2333 - val_soft_acc: 0.6350\n",
      "Epoch 269/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1133 - soft_acc: 0.8822 - val_loss: 1.1803 - val_soft_acc: 0.6605\n",
      "Epoch 270/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1203 - soft_acc: 0.8888 - val_loss: 1.2491 - val_soft_acc: 0.6404\n",
      "Epoch 271/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1820 - soft_acc: 0.8592 - val_loss: 1.3775 - val_soft_acc: 0.6150\n",
      "Epoch 272/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2602 - soft_acc: 0.8069 - val_loss: 1.5331 - val_soft_acc: 0.5559\n",
      "Epoch 273/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3584 - soft_acc: 0.7420 - val_loss: 1.5488 - val_soft_acc: 0.5496\n",
      "Epoch 274/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3740 - soft_acc: 0.7266 - val_loss: 1.4604 - val_soft_acc: 0.5777\n",
      "Epoch 275/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1839 - soft_acc: 0.8223 - val_loss: 1.3843 - val_soft_acc: 0.6006\n",
      "Epoch 276/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2878 - soft_acc: 0.7997 - val_loss: 1.5795 - val_soft_acc: 0.5500\n",
      "Epoch 277/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4416 - soft_acc: 0.7070 - val_loss: 1.5087 - val_soft_acc: 0.5434\n",
      "Epoch 278/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2732 - soft_acc: 0.7714 - val_loss: 1.4673 - val_soft_acc: 0.5795\n",
      "Epoch 279/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1898 - soft_acc: 0.8231 - val_loss: 1.3094 - val_soft_acc: 0.6225\n",
      "Epoch 280/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1384 - soft_acc: 0.8685 - val_loss: 1.2933 - val_soft_acc: 0.6404\n",
      "Epoch 281/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1683 - soft_acc: 0.8628 - val_loss: 1.4080 - val_soft_acc: 0.5906\n",
      "Epoch 282/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2707 - soft_acc: 0.8082 - val_loss: 1.4750 - val_soft_acc: 0.5740\n",
      "Epoch 283/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2691 - soft_acc: 0.7775 - val_loss: 1.4004 - val_soft_acc: 0.5949\n",
      "Epoch 284/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3632 - soft_acc: 0.7402 - val_loss: 1.3157 - val_soft_acc: 0.5988\n",
      "Epoch 285/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2019 - soft_acc: 0.8191 - val_loss: 1.5844 - val_soft_acc: 0.5658\n",
      "Epoch 286/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2659 - soft_acc: 0.7872 - val_loss: 1.2866 - val_soft_acc: 0.6160\n",
      "Epoch 287/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1451 - soft_acc: 0.8514 - val_loss: 1.2463 - val_soft_acc: 0.6416\n",
      "Epoch 288/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1187 - soft_acc: 0.8850 - val_loss: 1.1547 - val_soft_acc: 0.6502\n",
      "Epoch 289/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1095 - soft_acc: 0.8885 - val_loss: 1.1727 - val_soft_acc: 0.6434\n",
      "Epoch 290/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1994 - soft_acc: 0.8460 - val_loss: 1.5151 - val_soft_acc: 0.5736\n",
      "Epoch 291/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5319 - soft_acc: 0.6779 - val_loss: 1.6733 - val_soft_acc: 0.5002\n",
      "Epoch 292/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4882 - soft_acc: 0.6788 - val_loss: 1.6264 - val_soft_acc: 0.5389\n",
      "Epoch 293/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4741 - soft_acc: 0.6935 - val_loss: 1.4971 - val_soft_acc: 0.5686\n",
      "Epoch 294/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.2115 - soft_acc: 0.8121 - val_loss: 1.3338 - val_soft_acc: 0.6184\n",
      "Epoch 295/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1590 - soft_acc: 0.8485 - val_loss: 1.3292 - val_soft_acc: 0.6309\n",
      "Epoch 296/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1234 - soft_acc: 0.8836 - val_loss: 1.3073 - val_soft_acc: 0.6451\n",
      "Epoch 297/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.1339 - soft_acc: 0.8820 - val_loss: 1.2333 - val_soft_acc: 0.6447\n",
      "Epoch 298/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1024 - soft_acc: 0.9018 - val_loss: 1.1731 - val_soft_acc: 0.6668\n",
      "Epoch 299/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0828 - soft_acc: 0.9169 - val_loss: 1.2351 - val_soft_acc: 0.6543\n",
      "Epoch 300/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1827 - soft_acc: 0.8469 - val_loss: 1.4034 - val_soft_acc: 0.6201\n",
      "Epoch 301/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1632 - soft_acc: 0.8592 - val_loss: 1.4851 - val_soft_acc: 0.5508\n",
      "Epoch 302/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4900 - soft_acc: 0.7093 - val_loss: 1.7723 - val_soft_acc: 0.5209\n",
      "Epoch 303/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4635 - soft_acc: 0.6884 - val_loss: 1.5920 - val_soft_acc: 0.5652\n",
      "Epoch 304/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3995 - soft_acc: 0.7289 - val_loss: 1.6765 - val_soft_acc: 0.5512\n",
      "Epoch 305/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3383 - soft_acc: 0.7594 - val_loss: 1.2805 - val_soft_acc: 0.6279\n",
      "Epoch 306/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1100 - soft_acc: 0.8804 - val_loss: 1.2428 - val_soft_acc: 0.6588\n",
      "Epoch 307/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0815 - soft_acc: 0.9101 - val_loss: 1.1644 - val_soft_acc: 0.6586\n",
      "Epoch 308/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.0761 - soft_acc: 0.9158 - val_loss: 1.2268 - val_soft_acc: 0.6574\n",
      "Epoch 309/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0578 - soft_acc: 0.9317 - val_loss: 1.1912 - val_soft_acc: 0.6766\n",
      "Epoch 310/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0604 - soft_acc: 0.9324 - val_loss: 1.2125 - val_soft_acc: 0.6719\n",
      "Epoch 311/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1966 - soft_acc: 0.8626 - val_loss: 1.5566 - val_soft_acc: 0.5732\n",
      "Epoch 312/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6549 - soft_acc: 0.6475 - val_loss: 1.7177 - val_soft_acc: 0.4943\n",
      "Epoch 313/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4198 - soft_acc: 0.7003 - val_loss: 1.4381 - val_soft_acc: 0.5711\n",
      "Epoch 314/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1632 - soft_acc: 0.8308 - val_loss: 1.2574 - val_soft_acc: 0.6383\n",
      "Epoch 315/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0874 - soft_acc: 0.8983 - val_loss: 1.2218 - val_soft_acc: 0.6561\n",
      "Epoch 316/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0580 - soft_acc: 0.9277 - val_loss: 1.1550 - val_soft_acc: 0.6775\n",
      "Epoch 317/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0634 - soft_acc: 0.9308 - val_loss: 1.1688 - val_soft_acc: 0.6709\n",
      "Epoch 318/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1121 - soft_acc: 0.9090 - val_loss: 1.2474 - val_soft_acc: 0.6496\n",
      "Epoch 319/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1608 - soft_acc: 0.8713 - val_loss: 1.3587 - val_soft_acc: 0.6041\n",
      "Epoch 320/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6305 - soft_acc: 0.6684 - val_loss: 1.6599 - val_soft_acc: 0.5100\n",
      "Epoch 321/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3979 - soft_acc: 0.7167 - val_loss: 1.4681 - val_soft_acc: 0.5828\n",
      "Epoch 322/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1396 - soft_acc: 0.8510 - val_loss: 1.2089 - val_soft_acc: 0.6465\n",
      "Epoch 323/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0607 - soft_acc: 0.9170 - val_loss: 1.1610 - val_soft_acc: 0.6697\n",
      "Epoch 324/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0538 - soft_acc: 0.9360 - val_loss: 1.1949 - val_soft_acc: 0.6615\n",
      "Epoch 325/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2045 - soft_acc: 0.8350 - val_loss: 1.5261 - val_soft_acc: 0.5607\n",
      "Epoch 326/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4011 - soft_acc: 0.7362 - val_loss: 1.6061 - val_soft_acc: 0.5689\n",
      "Epoch 327/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3661 - soft_acc: 0.7455 - val_loss: 1.3967 - val_soft_acc: 0.5980\n",
      "Epoch 328/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1454 - soft_acc: 0.8492 - val_loss: 1.2103 - val_soft_acc: 0.6426\n",
      "Epoch 329/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0791 - soft_acc: 0.9038 - val_loss: 1.1558 - val_soft_acc: 0.6717\n",
      "Epoch 330/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0597 - soft_acc: 0.9264 - val_loss: 1.1765 - val_soft_acc: 0.6781\n",
      "Epoch 331/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0505 - soft_acc: 0.9369 - val_loss: 1.1339 - val_soft_acc: 0.6848\n",
      "Epoch 332/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0694 - soft_acc: 0.9302 - val_loss: 1.2044 - val_soft_acc: 0.6518\n",
      "Epoch 333/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2647 - soft_acc: 0.8324 - val_loss: 1.8181 - val_soft_acc: 0.4781\n",
      "Epoch 334/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6608 - soft_acc: 0.6311 - val_loss: 1.6730 - val_soft_acc: 0.5383\n",
      "Epoch 335/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3658 - soft_acc: 0.7411 - val_loss: 1.2814 - val_soft_acc: 0.6281\n",
      "Epoch 336/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1099 - soft_acc: 0.8704 - val_loss: 1.2341 - val_soft_acc: 0.6441\n",
      "Epoch 337/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.1004 - soft_acc: 0.8945 - val_loss: 1.1370 - val_soft_acc: 0.6602\n",
      "Epoch 338/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1214 - soft_acc: 0.8976 - val_loss: 1.2449 - val_soft_acc: 0.6592\n",
      "Epoch 339/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0617 - soft_acc: 0.9283 - val_loss: 1.1556 - val_soft_acc: 0.6760\n",
      "Epoch 340/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0527 - soft_acc: 0.9389 - val_loss: 1.1289 - val_soft_acc: 0.6762\n",
      "Epoch 341/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0771 - soft_acc: 0.9284 - val_loss: 1.3646 - val_soft_acc: 0.6217\n",
      "Epoch 342/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2475 - soft_acc: 0.7992 - val_loss: 1.4746 - val_soft_acc: 0.6102\n",
      "Epoch 343/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3722 - soft_acc: 0.7469 - val_loss: 1.7265 - val_soft_acc: 0.5471\n",
      "Epoch 344/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3433 - soft_acc: 0.7456 - val_loss: 1.3582 - val_soft_acc: 0.5992\n",
      "Epoch 345/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1729 - soft_acc: 0.8296 - val_loss: 1.2948 - val_soft_acc: 0.6383\n",
      "Epoch 346/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1118 - soft_acc: 0.8743 - val_loss: 1.2532 - val_soft_acc: 0.6547\n",
      "Epoch 347/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0997 - soft_acc: 0.8961 - val_loss: 1.2686 - val_soft_acc: 0.6518\n",
      "Epoch 348/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1629 - soft_acc: 0.8664 - val_loss: 1.2125 - val_soft_acc: 0.6590\n",
      "Epoch 349/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1195 - soft_acc: 0.8836 - val_loss: 1.2656 - val_soft_acc: 0.6594\n",
      "Epoch 350/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1085 - soft_acc: 0.8819 - val_loss: 1.2024 - val_soft_acc: 0.6672\n",
      "Epoch 351/5000\n",
      "14640/14640 [==============================] - 1s 80us/step - loss: 0.0813 - soft_acc: 0.9094 - val_loss: 1.1243 - val_soft_acc: 0.6672\n",
      "Epoch 352/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.0927 - soft_acc: 0.9067 - val_loss: 1.2663 - val_soft_acc: 0.6344\n",
      "Epoch 353/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6843 - soft_acc: 0.6554 - val_loss: 1.7928 - val_soft_acc: 0.5072\n",
      "Epoch 354/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5223 - soft_acc: 0.6722 - val_loss: 1.3982 - val_soft_acc: 0.5701\n",
      "Epoch 355/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1992 - soft_acc: 0.8141 - val_loss: 1.2531 - val_soft_acc: 0.6412\n",
      "Epoch 356/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1054 - soft_acc: 0.8867 - val_loss: 1.1528 - val_soft_acc: 0.6635\n",
      "Epoch 357/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0565 - soft_acc: 0.9283 - val_loss: 1.0920 - val_soft_acc: 0.6924\n",
      "Epoch 358/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0339 - soft_acc: 0.9500 - val_loss: 1.0914 - val_soft_acc: 0.6916\n",
      "Epoch 359/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0464 - soft_acc: 0.9473 - val_loss: 1.0822 - val_soft_acc: 0.6951\n",
      "Epoch 360/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0481 - soft_acc: 0.9555 - val_loss: 1.1853 - val_soft_acc: 0.6633\n",
      "Epoch 361/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1156 - soft_acc: 0.9011 - val_loss: 1.4184 - val_soft_acc: 0.5980\n",
      "Epoch 362/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5379 - soft_acc: 0.6928 - val_loss: 1.5652 - val_soft_acc: 0.5289\n",
      "Epoch 363/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2727 - soft_acc: 0.7718 - val_loss: 1.2603 - val_soft_acc: 0.6250\n",
      "Epoch 364/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1230 - soft_acc: 0.8643 - val_loss: 1.1602 - val_soft_acc: 0.6590\n",
      "Epoch 365/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0784 - soft_acc: 0.9082 - val_loss: 1.0738 - val_soft_acc: 0.6799\n",
      "Epoch 366/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0496 - soft_acc: 0.9354 - val_loss: 1.0547 - val_soft_acc: 0.6881\n",
      "Epoch 367/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1634 - soft_acc: 0.8616 - val_loss: 1.4713 - val_soft_acc: 0.6043\n",
      "Epoch 368/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6350 - soft_acc: 0.6533 - val_loss: 1.5263 - val_soft_acc: 0.5703\n",
      "Epoch 369/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2786 - soft_acc: 0.7773 - val_loss: 1.2318 - val_soft_acc: 0.6275\n",
      "Epoch 370/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1053 - soft_acc: 0.8835 - val_loss: 1.2068 - val_soft_acc: 0.6477\n",
      "Epoch 371/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0780 - soft_acc: 0.9167 - val_loss: 1.0825 - val_soft_acc: 0.6820\n",
      "Epoch 372/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0709 - soft_acc: 0.9252 - val_loss: 1.1400 - val_soft_acc: 0.6775\n",
      "Epoch 373/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0821 - soft_acc: 0.9203 - val_loss: 1.1584 - val_soft_acc: 0.6699\n",
      "Epoch 374/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0517 - soft_acc: 0.9362 - val_loss: 1.1034 - val_soft_acc: 0.6850\n",
      "Epoch 375/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0392 - soft_acc: 0.9482 - val_loss: 1.1955 - val_soft_acc: 0.6561\n",
      "Epoch 376/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7494 - soft_acc: 0.6155 - val_loss: 1.6743 - val_soft_acc: 0.5232\n",
      "Epoch 377/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2925 - soft_acc: 0.7640 - val_loss: 1.2910 - val_soft_acc: 0.6191\n",
      "Epoch 378/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1246 - soft_acc: 0.8674 - val_loss: 1.2067 - val_soft_acc: 0.6613\n",
      "Epoch 379/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0536 - soft_acc: 0.9297 - val_loss: 1.1466 - val_soft_acc: 0.6736\n",
      "Epoch 380/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.0649 - soft_acc: 0.9305 - val_loss: 1.2457 - val_soft_acc: 0.6508\n",
      "Epoch 381/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1033 - soft_acc: 0.9126 - val_loss: 1.2005 - val_soft_acc: 0.6643\n",
      "Epoch 382/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0746 - soft_acc: 0.9207 - val_loss: 1.1649 - val_soft_acc: 0.6732\n",
      "Epoch 383/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0512 - soft_acc: 0.9387 - val_loss: 1.1426 - val_soft_acc: 0.6906\n",
      "Epoch 384/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0270 - soft_acc: 0.9553 - val_loss: 1.1527 - val_soft_acc: 0.6912\n",
      "Epoch 385/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0508 - soft_acc: 0.9450 - val_loss: 1.2144 - val_soft_acc: 0.6609\n",
      "Epoch 386/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1284 - soft_acc: 0.8818 - val_loss: 1.3750 - val_soft_acc: 0.6318\n",
      "Epoch 387/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5817 - soft_acc: 0.6900 - val_loss: 1.9562 - val_soft_acc: 0.4695\n",
      "Epoch 388/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4871 - soft_acc: 0.6854 - val_loss: 1.4132 - val_soft_acc: 0.5848\n",
      "Epoch 389/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1639 - soft_acc: 0.8345 - val_loss: 1.2863 - val_soft_acc: 0.6289\n",
      "Epoch 390/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1316 - soft_acc: 0.8731 - val_loss: 1.1504 - val_soft_acc: 0.6748\n",
      "Epoch 391/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1211 - soft_acc: 0.8897 - val_loss: 1.1887 - val_soft_acc: 0.6689\n",
      "Epoch 392/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0451 - soft_acc: 0.9316 - val_loss: 1.0652 - val_soft_acc: 0.6898\n",
      "Epoch 393/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0257 - soft_acc: 0.9567 - val_loss: 1.0676 - val_soft_acc: 0.7066\n",
      "Epoch 394/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0149 - soft_acc: 0.9678 - val_loss: 1.0612 - val_soft_acc: 0.7074\n",
      "Epoch 395/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0105 - soft_acc: 0.9746 - val_loss: 1.0503 - val_soft_acc: 0.7129\n",
      "Epoch 396/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0079 - soft_acc: 0.9790 - val_loss: 1.0494 - val_soft_acc: 0.7129\n",
      "Epoch 397/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0160 - soft_acc: 0.9712 - val_loss: 1.0781 - val_soft_acc: 0.6971\n",
      "Epoch 398/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0899 - soft_acc: 0.9187 - val_loss: 1.3629 - val_soft_acc: 0.5984\n",
      "Epoch 399/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8618 - soft_acc: 0.6146 - val_loss: 1.9362 - val_soft_acc: 0.4543\n",
      "Epoch 400/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4932 - soft_acc: 0.6733 - val_loss: 1.4713 - val_soft_acc: 0.5875\n",
      "Epoch 401/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1811 - soft_acc: 0.8193 - val_loss: 1.2560 - val_soft_acc: 0.6326\n",
      "Epoch 402/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0815 - soft_acc: 0.8944 - val_loss: 1.1575 - val_soft_acc: 0.6766\n",
      "Epoch 403/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0518 - soft_acc: 0.9289 - val_loss: 1.0908 - val_soft_acc: 0.6873\n",
      "Epoch 404/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.0226 - soft_acc: 0.9566 - val_loss: 1.0649 - val_soft_acc: 0.6965\n",
      "Epoch 405/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0137 - soft_acc: 0.9694 - val_loss: 1.0425 - val_soft_acc: 0.6990\n",
      "Epoch 406/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0095 - soft_acc: 0.9748 - val_loss: 1.0383 - val_soft_acc: 0.7025\n",
      "Epoch 407/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0086 - soft_acc: 0.9772 - val_loss: 1.0380 - val_soft_acc: 0.7014\n",
      "Epoch 408/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0104 - soft_acc: 0.9775 - val_loss: 1.0309 - val_soft_acc: 0.7012\n",
      "Epoch 409/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.1564 - soft_acc: 0.8974 - val_loss: 1.7232 - val_soft_acc: 0.5096\n",
      "Epoch 410/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.8577 - soft_acc: 0.5932 - val_loss: 1.5928 - val_soft_acc: 0.5451\n",
      "Epoch 411/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3385 - soft_acc: 0.7409 - val_loss: 1.2768 - val_soft_acc: 0.6084\n",
      "Epoch 412/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1431 - soft_acc: 0.8532 - val_loss: 1.1882 - val_soft_acc: 0.6574\n",
      "Epoch 413/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0924 - soft_acc: 0.9044 - val_loss: 1.2167 - val_soft_acc: 0.6484\n",
      "Epoch 414/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0619 - soft_acc: 0.9257 - val_loss: 1.0617 - val_soft_acc: 0.6898\n",
      "Epoch 415/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0233 - soft_acc: 0.9578 - val_loss: 1.0332 - val_soft_acc: 0.7018\n",
      "Epoch 416/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0128 - soft_acc: 0.9713 - val_loss: 1.0399 - val_soft_acc: 0.7045\n",
      "Epoch 417/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0079 - soft_acc: 0.9764 - val_loss: 1.0350 - val_soft_acc: 0.7066\n",
      "Epoch 418/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0057 - soft_acc: 0.9809 - val_loss: 1.0321 - val_soft_acc: 0.7064\n",
      "Epoch 419/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0051 - soft_acc: 0.9834 - val_loss: 1.0326 - val_soft_acc: 0.7068\n",
      "Epoch 420/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0091 - soft_acc: 0.9779 - val_loss: 1.0437 - val_soft_acc: 0.7033\n",
      "Epoch 421/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0073 - soft_acc: 0.9788 - val_loss: 1.0335 - val_soft_acc: 0.7020\n",
      "Epoch 422/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1319 - soft_acc: 0.9131 - val_loss: 2.0683 - val_soft_acc: 0.4799\n",
      "Epoch 423/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 1.1303 - soft_acc: 0.52 - 1s 77us/step - loss: 1.1228 - soft_acc: 0.5257 - val_loss: 1.6804 - val_soft_acc: 0.5053\n",
      "Epoch 424/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4187 - soft_acc: 0.6945 - val_loss: 1.3866 - val_soft_acc: 0.5822\n",
      "Epoch 425/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1583 - soft_acc: 0.8347 - val_loss: 1.2545 - val_soft_acc: 0.6377\n",
      "Epoch 426/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0784 - soft_acc: 0.9040 - val_loss: 1.1060 - val_soft_acc: 0.6746\n",
      "Epoch 427/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0351 - soft_acc: 0.9442 - val_loss: 1.0602 - val_soft_acc: 0.6898\n",
      "Epoch 428/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0167 - soft_acc: 0.9640 - val_loss: 1.0548 - val_soft_acc: 0.6973\n",
      "Epoch 429/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0103 - soft_acc: 0.9730 - val_loss: 1.0535 - val_soft_acc: 0.6996\n",
      "Epoch 430/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0071 - soft_acc: 0.9775 - val_loss: 1.0524 - val_soft_acc: 0.7020\n",
      "Epoch 431/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0085 - soft_acc: 0.9774 - val_loss: 1.0498 - val_soft_acc: 0.7000\n",
      "Epoch 432/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0222 - soft_acc: 0.9647 - val_loss: 1.1157 - val_soft_acc: 0.6734\n",
      "Epoch 433/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7478 - soft_acc: 0.6526 - val_loss: 1.8149 - val_soft_acc: 0.4775\n",
      "Epoch 434/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3873 - soft_acc: 0.7211 - val_loss: 1.3003 - val_soft_acc: 0.6258\n",
      "Epoch 435/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1299 - soft_acc: 0.8544 - val_loss: 1.1595 - val_soft_acc: 0.6574\n",
      "Epoch 436/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0592 - soft_acc: 0.9178 - val_loss: 1.1333 - val_soft_acc: 0.6852\n",
      "Epoch 437/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0420 - soft_acc: 0.9424 - val_loss: 1.1206 - val_soft_acc: 0.6871\n",
      "Epoch 438/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.0238 - soft_acc: 0.9574 - val_loss: 1.0918 - val_soft_acc: 0.6943\n",
      "Epoch 439/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0127 - soft_acc: 0.9710 - val_loss: 1.0924 - val_soft_acc: 0.7004\n",
      "Epoch 440/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0072 - soft_acc: 0.9788 - val_loss: 1.0814 - val_soft_acc: 0.7006\n",
      "Epoch 441/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0053 - soft_acc: 0.9812 - val_loss: 1.0848 - val_soft_acc: 0.7035\n",
      "Epoch 442/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0048 - soft_acc: 0.9843 - val_loss: 1.0784 - val_soft_acc: 0.7051\n",
      "Epoch 443/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0308 - soft_acc: 0.9594 - val_loss: 1.1857 - val_soft_acc: 0.6551\n",
      "Epoch 444/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7534 - soft_acc: 0.6471 - val_loss: 1.9051 - val_soft_acc: 0.4773\n",
      "Epoch 445/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5364 - soft_acc: 0.6675 - val_loss: 1.3938 - val_soft_acc: 0.5857\n",
      "Epoch 446/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2308 - soft_acc: 0.7999 - val_loss: 1.2188 - val_soft_acc: 0.6393\n",
      "Epoch 447/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0918 - soft_acc: 0.8860 - val_loss: 1.1529 - val_soft_acc: 0.6723\n",
      "Epoch 448/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0434 - soft_acc: 0.9321 - val_loss: 1.1670 - val_soft_acc: 0.6836\n",
      "Epoch 449/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0538 - soft_acc: 0.9343 - val_loss: 1.1022 - val_soft_acc: 0.6930\n",
      "Epoch 450/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0453 - soft_acc: 0.9445 - val_loss: 1.0796 - val_soft_acc: 0.6949\n",
      "Epoch 451/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0226 - soft_acc: 0.9597 - val_loss: 1.0906 - val_soft_acc: 0.6969\n",
      "Epoch 452/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.0665 - soft_acc: 0.9415 - val_loss: 1.2788 - val_soft_acc: 0.6549\n",
      "Epoch 453/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1361 - soft_acc: 0.8877 - val_loss: 1.2604 - val_soft_acc: 0.6406\n",
      "Epoch 454/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2861 - soft_acc: 0.8106 - val_loss: 1.6035 - val_soft_acc: 0.5568\n",
      "Epoch 455/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4355 - soft_acc: 0.7123 - val_loss: 1.5929 - val_soft_acc: 0.5557\n",
      "Epoch 456/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1553 - soft_acc: 0.8386 - val_loss: 1.1792 - val_soft_acc: 0.6682\n",
      "Epoch 457/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0456 - soft_acc: 0.9269 - val_loss: 1.1215 - val_soft_acc: 0.7020\n",
      "Epoch 458/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0294 - soft_acc: 0.9518 - val_loss: 1.1103 - val_soft_acc: 0.7016\n",
      "Epoch 00458: early stopping\n",
      ">0.702\n",
      "Train on 14640 samples, validate on 2440 samples\n",
      "Epoch 1/5000\n",
      "14640/14640 [==============================] - 2s 109us/step - loss: 4.8741 - soft_acc: 0.1552 - val_loss: 4.4822 - val_soft_acc: 0.1670\n",
      "Epoch 2/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 4.2290 - soft_acc: 0.1626 - val_loss: 3.9973 - val_soft_acc: 0.1816\n",
      "Epoch 3/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 4.1363 - soft_acc: 0.1671 - val_loss: 3.8826 - val_soft_acc: 0.1686\n",
      "Epoch 4/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 4.0474 - soft_acc: 0.1703 - val_loss: 4.3744 - val_soft_acc: 0.1805\n",
      "Epoch 5/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.9688 - soft_acc: 0.1690 - val_loss: 3.8707 - val_soft_acc: 0.1781\n",
      "Epoch 6/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.9112 - soft_acc: 0.1736 - val_loss: 3.7931 - val_soft_acc: 0.1773\n",
      "Epoch 7/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.8695 - soft_acc: 0.1746 - val_loss: 3.7234 - val_soft_acc: 0.1811\n",
      "Epoch 8/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.8550 - soft_acc: 0.1718 - val_loss: 3.7527 - val_soft_acc: 0.1766\n",
      "Epoch 9/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.8425 - soft_acc: 0.1790 - val_loss: 3.6533 - val_soft_acc: 0.1889\n",
      "Epoch 10/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.7839 - soft_acc: 0.1785 - val_loss: 3.6987 - val_soft_acc: 0.1828\n",
      "Epoch 11/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.7699 - soft_acc: 0.1799 - val_loss: 3.7206 - val_soft_acc: 0.1793\n",
      "Epoch 12/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.7701 - soft_acc: 0.1776 - val_loss: 3.7336 - val_soft_acc: 0.1947\n",
      "Epoch 13/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.7325 - soft_acc: 0.1807 - val_loss: 3.6351 - val_soft_acc: 0.1998\n",
      "Epoch 14/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.7108 - soft_acc: 0.1799 - val_loss: 3.5852 - val_soft_acc: 0.1828\n",
      "Epoch 15/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.7050 - soft_acc: 0.1805 - val_loss: 3.6273 - val_soft_acc: 0.1713\n",
      "Epoch 16/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.6559 - soft_acc: 0.1849 - val_loss: 3.5304 - val_soft_acc: 0.1779\n",
      "Epoch 17/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.6340 - soft_acc: 0.1878 - val_loss: 3.5339 - val_soft_acc: 0.1863\n",
      "Epoch 18/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.6236 - soft_acc: 0.1860 - val_loss: 3.6431 - val_soft_acc: 0.1766\n",
      "Epoch 19/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.6395 - soft_acc: 0.1821 - val_loss: 3.4674 - val_soft_acc: 0.1830\n",
      "Epoch 20/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.6173 - soft_acc: 0.1854 - val_loss: 3.4963 - val_soft_acc: 0.1996\n",
      "Epoch 21/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.5539 - soft_acc: 0.1919 - val_loss: 3.4225 - val_soft_acc: 0.2010\n",
      "Epoch 22/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 3.5425 - soft_acc: 0.1912 - val_loss: 3.4591 - val_soft_acc: 0.1926\n",
      "Epoch 23/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 3.5395 - soft_acc: 0.1919 - val_loss: 3.5140 - val_soft_acc: 0.1828\n",
      "Epoch 24/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.5042 - soft_acc: 0.1948 - val_loss: 3.3403 - val_soft_acc: 0.1951\n",
      "Epoch 25/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.4870 - soft_acc: 0.1948 - val_loss: 3.4983 - val_soft_acc: 0.1932\n",
      "Epoch 26/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.4511 - soft_acc: 0.1973 - val_loss: 3.3469 - val_soft_acc: 0.1930\n",
      "Epoch 27/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.4465 - soft_acc: 0.1972 - val_loss: 3.4325 - val_soft_acc: 0.1930\n",
      "Epoch 28/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.4268 - soft_acc: 0.1957 - val_loss: 3.3860 - val_soft_acc: 0.2014\n",
      "Epoch 29/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.3875 - soft_acc: 0.2033 - val_loss: 3.3785 - val_soft_acc: 0.1895\n",
      "Epoch 30/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.4012 - soft_acc: 0.2008 - val_loss: 3.2899 - val_soft_acc: 0.2082\n",
      "Epoch 31/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.3519 - soft_acc: 0.2028 - val_loss: 3.3261 - val_soft_acc: 0.2197\n",
      "Epoch 32/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.3656 - soft_acc: 0.2053 - val_loss: 3.2899 - val_soft_acc: 0.2102\n",
      "Epoch 33/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.3357 - soft_acc: 0.2052 - val_loss: 3.1914 - val_soft_acc: 0.2166\n",
      "Epoch 34/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.3074 - soft_acc: 0.2081 - val_loss: 3.1421 - val_soft_acc: 0.2092\n",
      "Epoch 35/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.3283 - soft_acc: 0.2057 - val_loss: 3.2703 - val_soft_acc: 0.1945\n",
      "Epoch 36/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 3.2790 - soft_acc: 0.2095 - val_loss: 3.2430 - val_soft_acc: 0.2172\n",
      "Epoch 37/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.2651 - soft_acc: 0.2101 - val_loss: 3.2842 - val_soft_acc: 0.2195\n",
      "Epoch 38/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.2151 - soft_acc: 0.2147 - val_loss: 3.2336 - val_soft_acc: 0.2334\n",
      "Epoch 39/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.2044 - soft_acc: 0.2166 - val_loss: 3.1361 - val_soft_acc: 0.2225\n",
      "Epoch 40/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.1977 - soft_acc: 0.2184 - val_loss: 3.1740 - val_soft_acc: 0.2178\n",
      "Epoch 41/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.1828 - soft_acc: 0.2153 - val_loss: 3.2682 - val_soft_acc: 0.1992\n",
      "Epoch 42/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.1477 - soft_acc: 0.2190 - val_loss: 3.0833 - val_soft_acc: 0.2236\n",
      "Epoch 43/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.1450 - soft_acc: 0.2190 - val_loss: 3.1785 - val_soft_acc: 0.2209\n",
      "Epoch 44/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.1320 - soft_acc: 0.2201 - val_loss: 3.1203 - val_soft_acc: 0.2346\n",
      "Epoch 45/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.0968 - soft_acc: 0.2225 - val_loss: 3.0862 - val_soft_acc: 0.2166\n",
      "Epoch 46/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.0773 - soft_acc: 0.2237 - val_loss: 3.0567 - val_soft_acc: 0.2311\n",
      "Epoch 47/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.0619 - soft_acc: 0.2237 - val_loss: 3.0689 - val_soft_acc: 0.2346\n",
      "Epoch 48/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.0661 - soft_acc: 0.2277 - val_loss: 3.0401 - val_soft_acc: 0.2361\n",
      "Epoch 49/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.0204 - soft_acc: 0.2298 - val_loss: 3.0282 - val_soft_acc: 0.2195\n",
      "Epoch 50/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.0089 - soft_acc: 0.2303 - val_loss: 2.9966 - val_soft_acc: 0.2395\n",
      "Epoch 51/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.9822 - soft_acc: 0.2315 - val_loss: 2.9435 - val_soft_acc: 0.2279\n",
      "Epoch 52/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.9402 - soft_acc: 0.2363 - val_loss: 2.9900 - val_soft_acc: 0.2404\n",
      "Epoch 53/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 2.9080 - soft_acc: 0.2406 - val_loss: 2.9078 - val_soft_acc: 0.2473\n",
      "Epoch 54/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.9249 - soft_acc: 0.2411 - val_loss: 2.9771 - val_soft_acc: 0.2465\n",
      "Epoch 55/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.8753 - soft_acc: 0.2435 - val_loss: 2.9661 - val_soft_acc: 0.2508\n",
      "Epoch 56/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.8780 - soft_acc: 0.2409 - val_loss: 2.8954 - val_soft_acc: 0.2344\n",
      "Epoch 57/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.8416 - soft_acc: 0.2417 - val_loss: 2.8910 - val_soft_acc: 0.2480\n",
      "Epoch 58/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.8246 - soft_acc: 0.2473 - val_loss: 2.8490 - val_soft_acc: 0.2541\n",
      "Epoch 59/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 2.7874 - soft_acc: 0.25 - 1s 76us/step - loss: 2.7872 - soft_acc: 0.2515 - val_loss: 2.8896 - val_soft_acc: 0.2510\n",
      "Epoch 60/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.7460 - soft_acc: 0.2511 - val_loss: 2.8708 - val_soft_acc: 0.2529\n",
      "Epoch 61/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.7571 - soft_acc: 0.2518 - val_loss: 2.8169 - val_soft_acc: 0.2533\n",
      "Epoch 62/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.7133 - soft_acc: 0.2561 - val_loss: 2.7821 - val_soft_acc: 0.2656\n",
      "Epoch 63/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.7118 - soft_acc: 0.2558 - val_loss: 2.7524 - val_soft_acc: 0.2631\n",
      "Epoch 64/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.6993 - soft_acc: 0.2564 - val_loss: 2.7592 - val_soft_acc: 0.2629\n",
      "Epoch 65/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 2.6529 - soft_acc: 0.2637 - val_loss: 2.8019 - val_soft_acc: 0.2600\n",
      "Epoch 66/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.6486 - soft_acc: 0.2633 - val_loss: 2.6978 - val_soft_acc: 0.2637\n",
      "Epoch 67/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.5654 - soft_acc: 0.2709 - val_loss: 2.7150 - val_soft_acc: 0.2662\n",
      "Epoch 68/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.5686 - soft_acc: 0.2731 - val_loss: 2.6644 - val_soft_acc: 0.2611\n",
      "Epoch 69/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.5633 - soft_acc: 0.2730 - val_loss: 2.6644 - val_soft_acc: 0.2730\n",
      "Epoch 70/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.5138 - soft_acc: 0.2765 - val_loss: 2.6675 - val_soft_acc: 0.2777\n",
      "Epoch 71/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.5099 - soft_acc: 0.2804 - val_loss: 2.6779 - val_soft_acc: 0.2689\n",
      "Epoch 72/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.4457 - soft_acc: 0.2846 - val_loss: 2.6841 - val_soft_acc: 0.2750\n",
      "Epoch 73/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.4543 - soft_acc: 0.2842 - val_loss: 2.7041 - val_soft_acc: 0.2828\n",
      "Epoch 74/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.3962 - soft_acc: 0.2908 - val_loss: 2.6219 - val_soft_acc: 0.2701\n",
      "Epoch 75/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.4097 - soft_acc: 0.2889 - val_loss: 2.4802 - val_soft_acc: 0.2914\n",
      "Epoch 76/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.3430 - soft_acc: 0.2981 - val_loss: 2.6220 - val_soft_acc: 0.2824\n",
      "Epoch 77/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.3169 - soft_acc: 0.2972 - val_loss: 2.5284 - val_soft_acc: 0.2852\n",
      "Epoch 78/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.3058 - soft_acc: 0.3015 - val_loss: 2.5355 - val_soft_acc: 0.2908\n",
      "Epoch 79/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.2410 - soft_acc: 0.3068 - val_loss: 2.4362 - val_soft_acc: 0.3184\n",
      "Epoch 80/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.2379 - soft_acc: 0.3069 - val_loss: 2.3673 - val_soft_acc: 0.2977\n",
      "Epoch 81/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.1506 - soft_acc: 0.3144 - val_loss: 2.4596 - val_soft_acc: 0.3051\n",
      "Epoch 82/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 2.1640 - soft_acc: 0.3155 - val_loss: 2.4104 - val_soft_acc: 0.2994\n",
      "Epoch 83/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.1144 - soft_acc: 0.3206 - val_loss: 2.4859 - val_soft_acc: 0.2994\n",
      "Epoch 84/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.1015 - soft_acc: 0.3217 - val_loss: 2.3588 - val_soft_acc: 0.2977\n",
      "Epoch 85/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.0568 - soft_acc: 0.3297 - val_loss: 2.3333 - val_soft_acc: 0.3137\n",
      "Epoch 86/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.0414 - soft_acc: 0.3317 - val_loss: 2.3161 - val_soft_acc: 0.3162\n",
      "Epoch 87/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.9898 - soft_acc: 0.3361 - val_loss: 2.3485 - val_soft_acc: 0.3084\n",
      "Epoch 88/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.9552 - soft_acc: 0.3400 - val_loss: 2.3684 - val_soft_acc: 0.3236\n",
      "Epoch 89/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.9374 - soft_acc: 0.3375 - val_loss: 2.2000 - val_soft_acc: 0.3242\n",
      "Epoch 90/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.9393 - soft_acc: 0.3467 - val_loss: 2.2648 - val_soft_acc: 0.3244\n",
      "Epoch 91/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.9035 - soft_acc: 0.3457 - val_loss: 2.2639 - val_soft_acc: 0.3225\n",
      "Epoch 92/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.8440 - soft_acc: 0.3550 - val_loss: 2.3452 - val_soft_acc: 0.3252\n",
      "Epoch 93/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.8141 - soft_acc: 0.3554 - val_loss: 2.2357 - val_soft_acc: 0.3342\n",
      "Epoch 94/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 1.7957 - soft_acc: 0.36 - 1s 79us/step - loss: 1.7952 - soft_acc: 0.3645 - val_loss: 2.2029 - val_soft_acc: 0.3301\n",
      "Epoch 95/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.7375 - soft_acc: 0.3700 - val_loss: 2.1832 - val_soft_acc: 0.3439\n",
      "Epoch 96/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.7081 - soft_acc: 0.3751 - val_loss: 2.1015 - val_soft_acc: 0.3465\n",
      "Epoch 97/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.6865 - soft_acc: 0.3776 - val_loss: 2.0582 - val_soft_acc: 0.3449\n",
      "Epoch 98/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.6506 - soft_acc: 0.3820 - val_loss: 2.1433 - val_soft_acc: 0.3352\n",
      "Epoch 99/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.7333 - soft_acc: 0.3692 - val_loss: 2.1000 - val_soft_acc: 0.3451\n",
      "Epoch 100/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.5966 - soft_acc: 0.3966 - val_loss: 2.0817 - val_soft_acc: 0.3365\n",
      "Epoch 101/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.5602 - soft_acc: 0.3973 - val_loss: 2.0961 - val_soft_acc: 0.3607\n",
      "Epoch 102/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.5529 - soft_acc: 0.3979 - val_loss: 1.9812 - val_soft_acc: 0.3693\n",
      "Epoch 103/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.4553 - soft_acc: 0.4076 - val_loss: 1.9993 - val_soft_acc: 0.3662\n",
      "Epoch 104/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.4565 - soft_acc: 0.4163 - val_loss: 1.9814 - val_soft_acc: 0.3732\n",
      "Epoch 105/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.5212 - soft_acc: 0.4072 - val_loss: 2.0519 - val_soft_acc: 0.3645\n",
      "Epoch 106/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.5010 - soft_acc: 0.4083 - val_loss: 2.0553 - val_soft_acc: 0.3703\n",
      "Epoch 107/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.5047 - soft_acc: 0.4147 - val_loss: 1.9741 - val_soft_acc: 0.3773\n",
      "Epoch 108/5000\n",
      "14640/14640 [==============================] - 1s 80us/step - loss: 1.3774 - soft_acc: 0.4290 - val_loss: 1.9257 - val_soft_acc: 0.3820\n",
      "Epoch 109/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.3014 - soft_acc: 0.4451 - val_loss: 1.9432 - val_soft_acc: 0.3734\n",
      "Epoch 110/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.3533 - soft_acc: 0.4324 - val_loss: 2.0067 - val_soft_acc: 0.3865\n",
      "Epoch 111/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.3201 - soft_acc: 0.4402 - val_loss: 1.9177 - val_soft_acc: 0.3836\n",
      "Epoch 112/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.2886 - soft_acc: 0.4470 - val_loss: 1.8445 - val_soft_acc: 0.3898\n",
      "Epoch 113/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.2289 - soft_acc: 0.4543 - val_loss: 1.8419 - val_soft_acc: 0.3975\n",
      "Epoch 114/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.1455 - soft_acc: 0.4801 - val_loss: 1.7969 - val_soft_acc: 0.4020\n",
      "Epoch 115/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.2192 - soft_acc: 0.4618 - val_loss: 1.8688 - val_soft_acc: 0.3885\n",
      "Epoch 116/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.2106 - soft_acc: 0.4646 - val_loss: 1.8892 - val_soft_acc: 0.4008\n",
      "Epoch 117/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.1739 - soft_acc: 0.4695 - val_loss: 1.7898 - val_soft_acc: 0.4219\n",
      "Epoch 118/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.2371 - soft_acc: 0.4642 - val_loss: 1.9309 - val_soft_acc: 0.4086\n",
      "Epoch 119/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.2484 - soft_acc: 0.4616 - val_loss: 1.8014 - val_soft_acc: 0.4170\n",
      "Epoch 120/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.0962 - soft_acc: 0.4851 - val_loss: 1.8336 - val_soft_acc: 0.4059\n",
      "Epoch 121/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.1064 - soft_acc: 0.4851 - val_loss: 1.8492 - val_soft_acc: 0.4238\n",
      "Epoch 122/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.0028 - soft_acc: 0.5069 - val_loss: 1.8539 - val_soft_acc: 0.4180\n",
      "Epoch 123/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.0888 - soft_acc: 0.4924 - val_loss: 1.7332 - val_soft_acc: 0.4191\n",
      "Epoch 124/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.0512 - soft_acc: 0.4976 - val_loss: 1.7239 - val_soft_acc: 0.4273\n",
      "Epoch 125/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.0448 - soft_acc: 0.5027 - val_loss: 1.6865 - val_soft_acc: 0.4381\n",
      "Epoch 126/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.9281 - soft_acc: 0.5305 - val_loss: 1.8194 - val_soft_acc: 0.4205\n",
      "Epoch 127/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.0268 - soft_acc: 0.5074 - val_loss: 1.7373 - val_soft_acc: 0.4258\n",
      "Epoch 128/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.9301 - soft_acc: 0.5287 - val_loss: 1.7465 - val_soft_acc: 0.4256\n",
      "Epoch 129/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.9852 - soft_acc: 0.5125 - val_loss: 1.8112 - val_soft_acc: 0.4311\n",
      "Epoch 130/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.9724 - soft_acc: 0.5157 - val_loss: 1.7446 - val_soft_acc: 0.4389\n",
      "Epoch 131/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 1.0453 - soft_acc: 0.5028 - val_loss: 1.7350 - val_soft_acc: 0.4363\n",
      "Epoch 132/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.9017 - soft_acc: 0.5374 - val_loss: 1.6674 - val_soft_acc: 0.4439\n",
      "Epoch 133/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.9115 - soft_acc: 0.5336 - val_loss: 1.6501 - val_soft_acc: 0.4586\n",
      "Epoch 134/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.8562 - soft_acc: 0.5525 - val_loss: 1.6362 - val_soft_acc: 0.4426\n",
      "Epoch 135/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8495 - soft_acc: 0.5493 - val_loss: 1.6704 - val_soft_acc: 0.4529\n",
      "Epoch 136/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8349 - soft_acc: 0.5529 - val_loss: 1.6416 - val_soft_acc: 0.4576\n",
      "Epoch 137/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.7993 - soft_acc: 0.5593 - val_loss: 1.7122 - val_soft_acc: 0.4527\n",
      "Epoch 138/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.9622 - soft_acc: 0.5231 - val_loss: 1.7447 - val_soft_acc: 0.4410\n",
      "Epoch 139/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.9948 - soft_acc: 0.5144 - val_loss: 1.7183 - val_soft_acc: 0.4516\n",
      "Epoch 140/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8031 - soft_acc: 0.5581 - val_loss: 1.6350 - val_soft_acc: 0.4594\n",
      "Epoch 141/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.7369 - soft_acc: 0.5806 - val_loss: 1.6975 - val_soft_acc: 0.4795\n",
      "Epoch 142/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8293 - soft_acc: 0.5601 - val_loss: 1.7346 - val_soft_acc: 0.4525\n",
      "Epoch 143/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.7588 - soft_acc: 0.5774 - val_loss: 1.6644 - val_soft_acc: 0.4693\n",
      "Epoch 144/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.7695 - soft_acc: 0.5780 - val_loss: 1.5965 - val_soft_acc: 0.4721\n",
      "Epoch 145/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.7292 - soft_acc: 0.5907 - val_loss: 1.5479 - val_soft_acc: 0.4787\n",
      "Epoch 146/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.7422 - soft_acc: 0.5878 - val_loss: 1.7907 - val_soft_acc: 0.4623\n",
      "Epoch 147/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.7331 - soft_acc: 0.5787 - val_loss: 1.6374 - val_soft_acc: 0.4717\n",
      "Epoch 148/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.7304 - soft_acc: 0.5844 - val_loss: 1.6006 - val_soft_acc: 0.4621\n",
      "Epoch 149/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.7789 - soft_acc: 0.5662 - val_loss: 1.7273 - val_soft_acc: 0.4553\n",
      "Epoch 150/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.7888 - soft_acc: 0.5722 - val_loss: 1.6725 - val_soft_acc: 0.4711\n",
      "Epoch 151/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.7136 - soft_acc: 0.5880 - val_loss: 1.5782 - val_soft_acc: 0.4707\n",
      "Epoch 152/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6846 - soft_acc: 0.6063 - val_loss: 1.6704 - val_soft_acc: 0.4705\n",
      "Epoch 153/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6792 - soft_acc: 0.6062 - val_loss: 1.5102 - val_soft_acc: 0.5037\n",
      "Epoch 154/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5371 - soft_acc: 0.6499 - val_loss: 1.5750 - val_soft_acc: 0.4951\n",
      "Epoch 155/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6576 - soft_acc: 0.6128 - val_loss: 1.6920 - val_soft_acc: 0.4635\n",
      "Epoch 156/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.7489 - soft_acc: 0.5841 - val_loss: 1.5382 - val_soft_acc: 0.4928\n",
      "Epoch 157/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5913 - soft_acc: 0.6339 - val_loss: 1.5808 - val_soft_acc: 0.4973\n",
      "Epoch 158/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5803 - soft_acc: 0.6446 - val_loss: 1.4390 - val_soft_acc: 0.5119\n",
      "Epoch 159/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5527 - soft_acc: 0.6574 - val_loss: 1.4123 - val_soft_acc: 0.5084\n",
      "Epoch 160/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5422 - soft_acc: 0.6531 - val_loss: 1.7093 - val_soft_acc: 0.4746\n",
      "Epoch 161/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.6702 - soft_acc: 0.6066 - val_loss: 1.7860 - val_soft_acc: 0.4512\n",
      "Epoch 162/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.0077 - soft_acc: 0.5177 - val_loss: 1.5793 - val_soft_acc: 0.4684\n",
      "Epoch 163/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6579 - soft_acc: 0.6024 - val_loss: 1.5722 - val_soft_acc: 0.5010\n",
      "Epoch 164/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5112 - soft_acc: 0.6602 - val_loss: 1.5276 - val_soft_acc: 0.5105\n",
      "Epoch 165/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8731 - soft_acc: 0.5555 - val_loss: 1.5929 - val_soft_acc: 0.4879\n",
      "Epoch 166/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5913 - soft_acc: 0.6323 - val_loss: 1.5980 - val_soft_acc: 0.4939\n",
      "Epoch 167/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5256 - soft_acc: 0.6590 - val_loss: 1.4364 - val_soft_acc: 0.5322\n",
      "Epoch 168/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5625 - soft_acc: 0.6515 - val_loss: 1.4775 - val_soft_acc: 0.5150\n",
      "Epoch 169/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5155 - soft_acc: 0.6716 - val_loss: 1.3944 - val_soft_acc: 0.5285\n",
      "Epoch 170/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5060 - soft_acc: 0.6775 - val_loss: 1.4964 - val_soft_acc: 0.5295\n",
      "Epoch 171/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5041 - soft_acc: 0.6716 - val_loss: 1.6245 - val_soft_acc: 0.5047\n",
      "Epoch 172/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5408 - soft_acc: 0.6615 - val_loss: 1.4387 - val_soft_acc: 0.5330\n",
      "Epoch 173/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4415 - soft_acc: 0.6986 - val_loss: 1.4025 - val_soft_acc: 0.5432\n",
      "Epoch 174/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5180 - soft_acc: 0.6700 - val_loss: 1.4330 - val_soft_acc: 0.5148\n",
      "Epoch 175/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5023 - soft_acc: 0.6743 - val_loss: 1.6142 - val_soft_acc: 0.4924\n",
      "Epoch 176/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6312 - soft_acc: 0.6180 - val_loss: 1.5306 - val_soft_acc: 0.5078\n",
      "Epoch 177/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5004 - soft_acc: 0.6619 - val_loss: 1.4886 - val_soft_acc: 0.5168\n",
      "Epoch 178/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5247 - soft_acc: 0.6570 - val_loss: 1.3997 - val_soft_acc: 0.5385\n",
      "Epoch 179/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6059 - soft_acc: 0.6325 - val_loss: 1.4822 - val_soft_acc: 0.5125\n",
      "Epoch 180/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.4812 - soft_acc: 0.6723 - val_loss: 1.3802 - val_soft_acc: 0.5336\n",
      "Epoch 181/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4507 - soft_acc: 0.6953 - val_loss: 1.4458 - val_soft_acc: 0.5408\n",
      "Epoch 182/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4306 - soft_acc: 0.7033 - val_loss: 1.4378 - val_soft_acc: 0.5406\n",
      "Epoch 183/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5560 - soft_acc: 0.6531 - val_loss: 1.4926 - val_soft_acc: 0.5145\n",
      "Epoch 184/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4724 - soft_acc: 0.6781 - val_loss: 1.4238 - val_soft_acc: 0.5490\n",
      "Epoch 185/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4441 - soft_acc: 0.6960 - val_loss: 1.5155 - val_soft_acc: 0.5211\n",
      "Epoch 186/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4575 - soft_acc: 0.6925 - val_loss: 1.5138 - val_soft_acc: 0.5191\n",
      "Epoch 187/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4571 - soft_acc: 0.6794 - val_loss: 1.3743 - val_soft_acc: 0.5355\n",
      "Epoch 188/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4088 - soft_acc: 0.7037 - val_loss: 1.4285 - val_soft_acc: 0.5457\n",
      "Epoch 189/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3756 - soft_acc: 0.7314 - val_loss: 1.3298 - val_soft_acc: 0.5652\n",
      "Epoch 190/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3921 - soft_acc: 0.7196 - val_loss: 1.4474 - val_soft_acc: 0.5301\n",
      "Epoch 191/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4270 - soft_acc: 0.7033 - val_loss: 1.5271 - val_soft_acc: 0.5152\n",
      "Epoch 192/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4722 - soft_acc: 0.6788 - val_loss: 1.4420 - val_soft_acc: 0.5486\n",
      "Epoch 193/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5021 - soft_acc: 0.6865 - val_loss: 1.5898 - val_soft_acc: 0.5053\n",
      "Epoch 194/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4698 - soft_acc: 0.6768 - val_loss: 1.4902 - val_soft_acc: 0.5172\n",
      "Epoch 195/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4028 - soft_acc: 0.7035 - val_loss: 1.4796 - val_soft_acc: 0.5379\n",
      "Epoch 196/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6128 - soft_acc: 0.6345 - val_loss: 1.5554 - val_soft_acc: 0.5111\n",
      "Epoch 197/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4824 - soft_acc: 0.6811 - val_loss: 1.3596 - val_soft_acc: 0.5633\n",
      "Epoch 198/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3951 - soft_acc: 0.7157 - val_loss: 1.3716 - val_soft_acc: 0.5617\n",
      "Epoch 199/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3118 - soft_acc: 0.7647 - val_loss: 1.4206 - val_soft_acc: 0.5445\n",
      "Epoch 200/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3367 - soft_acc: 0.7469 - val_loss: 1.3156 - val_soft_acc: 0.5893\n",
      "Epoch 201/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3172 - soft_acc: 0.7621 - val_loss: 1.4333 - val_soft_acc: 0.5490\n",
      "Epoch 202/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5337 - soft_acc: 0.6723 - val_loss: 1.5715 - val_soft_acc: 0.5066\n",
      "Epoch 203/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.7253 - soft_acc: 0.6058 - val_loss: 1.5467 - val_soft_acc: 0.5117\n",
      "Epoch 204/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4571 - soft_acc: 0.6801 - val_loss: 1.3744 - val_soft_acc: 0.5568\n",
      "Epoch 205/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2999 - soft_acc: 0.7585 - val_loss: 1.2792 - val_soft_acc: 0.5920\n",
      "Epoch 206/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2876 - soft_acc: 0.7735 - val_loss: 1.2760 - val_soft_acc: 0.5932\n",
      "Epoch 207/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3172 - soft_acc: 0.7673 - val_loss: 1.2618 - val_soft_acc: 0.5805\n",
      "Epoch 208/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3232 - soft_acc: 0.7584 - val_loss: 1.2925 - val_soft_acc: 0.5818\n",
      "Epoch 209/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3189 - soft_acc: 0.7505 - val_loss: 1.4679 - val_soft_acc: 0.5578\n",
      "Epoch 210/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3494 - soft_acc: 0.7393 - val_loss: 1.6339 - val_soft_acc: 0.5219\n",
      "Epoch 211/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4157 - soft_acc: 0.7026 - val_loss: 1.3465 - val_soft_acc: 0.5754\n",
      "Epoch 212/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5488 - soft_acc: 0.6593 - val_loss: 1.6781 - val_soft_acc: 0.5170\n",
      "Epoch 213/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4313 - soft_acc: 0.6899 - val_loss: 1.4361 - val_soft_acc: 0.5537\n",
      "Epoch 214/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5202 - soft_acc: 0.6644 - val_loss: 1.5081 - val_soft_acc: 0.5355\n",
      "Epoch 215/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.4891 - soft_acc: 0.6817 - val_loss: 1.4790 - val_soft_acc: 0.5529\n",
      "Epoch 216/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2882 - soft_acc: 0.7648 - val_loss: 1.3008 - val_soft_acc: 0.5891\n",
      "Epoch 217/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2948 - soft_acc: 0.7791 - val_loss: 1.3156 - val_soft_acc: 0.5939\n",
      "Epoch 218/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2011 - soft_acc: 0.8213 - val_loss: 1.1701 - val_soft_acc: 0.6252\n",
      "Epoch 219/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2161 - soft_acc: 0.8323 - val_loss: 1.3419 - val_soft_acc: 0.5988\n",
      "Epoch 220/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2753 - soft_acc: 0.7853 - val_loss: 1.3278 - val_soft_acc: 0.5910\n",
      "Epoch 221/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3146 - soft_acc: 0.7621 - val_loss: 1.4810 - val_soft_acc: 0.5496\n",
      "Epoch 222/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.7112 - soft_acc: 0.61 - 1s 76us/step - loss: 0.7233 - soft_acc: 0.6146 - val_loss: 1.6841 - val_soft_acc: 0.4709\n",
      "Epoch 223/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.4967 - soft_acc: 0.6577 - val_loss: 1.3187 - val_soft_acc: 0.5830\n",
      "Epoch 224/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2434 - soft_acc: 0.7816 - val_loss: 1.2618 - val_soft_acc: 0.5889\n",
      "Epoch 225/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2499 - soft_acc: 0.7951 - val_loss: 1.2853 - val_soft_acc: 0.5992\n",
      "Epoch 226/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1955 - soft_acc: 0.8283 - val_loss: 1.1675 - val_soft_acc: 0.6266\n",
      "Epoch 227/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1889 - soft_acc: 0.8343 - val_loss: 1.3325 - val_soft_acc: 0.5941\n",
      "Epoch 228/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3178 - soft_acc: 0.7619 - val_loss: 1.4749 - val_soft_acc: 0.5381\n",
      "Epoch 229/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5761 - soft_acc: 0.6693 - val_loss: 1.8076 - val_soft_acc: 0.4746\n",
      "Epoch 230/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5495 - soft_acc: 0.6501 - val_loss: 1.3238 - val_soft_acc: 0.5793\n",
      "Epoch 231/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2810 - soft_acc: 0.7641 - val_loss: 1.2459 - val_soft_acc: 0.5910\n",
      "Epoch 232/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1941 - soft_acc: 0.8172 - val_loss: 1.3404 - val_soft_acc: 0.5891\n",
      "Epoch 233/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1678 - soft_acc: 0.8425 - val_loss: 1.1834 - val_soft_acc: 0.6318\n",
      "Epoch 234/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2450 - soft_acc: 0.8010 - val_loss: 1.5230 - val_soft_acc: 0.5570\n",
      "Epoch 235/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4975 - soft_acc: 0.6813 - val_loss: 1.4879 - val_soft_acc: 0.5496\n",
      "Epoch 236/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3710 - soft_acc: 0.7253 - val_loss: 1.5043 - val_soft_acc: 0.5502\n",
      "Epoch 237/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.3131 - soft_acc: 0.7554 - val_loss: 1.3899 - val_soft_acc: 0.5689\n",
      "Epoch 238/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3228 - soft_acc: 0.7600 - val_loss: 1.3815 - val_soft_acc: 0.5732\n",
      "Epoch 239/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2275 - soft_acc: 0.8030 - val_loss: 1.2394 - val_soft_acc: 0.6174\n",
      "Epoch 240/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1834 - soft_acc: 0.8323 - val_loss: 1.1546 - val_soft_acc: 0.6336\n",
      "Epoch 241/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1639 - soft_acc: 0.8554 - val_loss: 1.2807 - val_soft_acc: 0.6012\n",
      "Epoch 242/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2276 - soft_acc: 0.8113 - val_loss: 1.3017 - val_soft_acc: 0.6125\n",
      "Epoch 243/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.1884 - soft_acc: 0.83 - 1s 76us/step - loss: 0.1885 - soft_acc: 0.8336 - val_loss: 1.2459 - val_soft_acc: 0.6182\n",
      "Epoch 244/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3456 - soft_acc: 0.7645 - val_loss: 1.7949 - val_soft_acc: 0.5145\n",
      "Epoch 245/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6895 - soft_acc: 0.6189 - val_loss: 1.6136 - val_soft_acc: 0.5031\n",
      "Epoch 246/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3768 - soft_acc: 0.7046 - val_loss: 1.3217 - val_soft_acc: 0.5889\n",
      "Epoch 247/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2204 - soft_acc: 0.7945 - val_loss: 1.2537 - val_soft_acc: 0.6178\n",
      "Epoch 248/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.1591 - soft_acc: 0.8424 - val_loss: 1.1543 - val_soft_acc: 0.6279\n",
      "Epoch 249/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1419 - soft_acc: 0.8584 - val_loss: 1.1493 - val_soft_acc: 0.6391\n",
      "Epoch 250/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2417 - soft_acc: 0.8138 - val_loss: 1.3597 - val_soft_acc: 0.5662\n",
      "Epoch 251/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5437 - soft_acc: 0.6853 - val_loss: 1.7032 - val_soft_acc: 0.5111\n",
      "Epoch 252/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.4812 - soft_acc: 0.6870 - val_loss: 1.3965 - val_soft_acc: 0.5674\n",
      "Epoch 253/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2419 - soft_acc: 0.7922 - val_loss: 1.1702 - val_soft_acc: 0.6148\n",
      "Epoch 254/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1639 - soft_acc: 0.8381 - val_loss: 1.1452 - val_soft_acc: 0.6387\n",
      "Epoch 255/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1279 - soft_acc: 0.8708 - val_loss: 1.1695 - val_soft_acc: 0.6430\n",
      "Epoch 256/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1232 - soft_acc: 0.8793 - val_loss: 1.1422 - val_soft_acc: 0.6357\n",
      "Epoch 257/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1892 - soft_acc: 0.8416 - val_loss: 1.1888 - val_soft_acc: 0.6232\n",
      "Epoch 258/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1847 - soft_acc: 0.8300 - val_loss: 1.2312 - val_soft_acc: 0.6227\n",
      "Epoch 259/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2105 - soft_acc: 0.8191 - val_loss: 1.3539 - val_soft_acc: 0.5832\n",
      "Epoch 260/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.7245 - soft_acc: 0.6317 - val_loss: 1.7263 - val_soft_acc: 0.4883\n",
      "Epoch 261/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4606 - soft_acc: 0.6804 - val_loss: 1.4169 - val_soft_acc: 0.5723\n",
      "Epoch 262/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2980 - soft_acc: 0.7637 - val_loss: 1.2581 - val_soft_acc: 0.5998\n",
      "Epoch 263/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1855 - soft_acc: 0.8289 - val_loss: 1.2033 - val_soft_acc: 0.6154\n",
      "Epoch 264/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1426 - soft_acc: 0.8611 - val_loss: 1.1385 - val_soft_acc: 0.6502\n",
      "Epoch 265/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1148 - soft_acc: 0.8869 - val_loss: 1.2321 - val_soft_acc: 0.6156\n",
      "Epoch 266/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.1850 - soft_acc: 0.8470 - val_loss: 1.4828 - val_soft_acc: 0.5584\n",
      "Epoch 267/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4819 - soft_acc: 0.6896 - val_loss: 1.3846 - val_soft_acc: 0.5678\n",
      "Epoch 268/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3610 - soft_acc: 0.7330 - val_loss: 1.3987 - val_soft_acc: 0.5656\n",
      "Epoch 269/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.1813 - soft_acc: 0.8145 - val_loss: 1.1157 - val_soft_acc: 0.6512\n",
      "Epoch 270/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1008 - soft_acc: 0.8874 - val_loss: 1.1697 - val_soft_acc: 0.6492\n",
      "Epoch 271/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2452 - soft_acc: 0.8251 - val_loss: 1.3372 - val_soft_acc: 0.5910\n",
      "Epoch 272/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2348 - soft_acc: 0.8069 - val_loss: 1.3432 - val_soft_acc: 0.6156\n",
      "Epoch 273/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4274 - soft_acc: 0.7197 - val_loss: 1.6232 - val_soft_acc: 0.5270\n",
      "Epoch 274/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2881 - soft_acc: 0.7628 - val_loss: 1.2480 - val_soft_acc: 0.6133\n",
      "Epoch 275/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1701 - soft_acc: 0.8264 - val_loss: 1.0874 - val_soft_acc: 0.6506\n",
      "Epoch 276/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1072 - soft_acc: 0.8836 - val_loss: 1.1700 - val_soft_acc: 0.6539\n",
      "Epoch 277/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1340 - soft_acc: 0.8791 - val_loss: 1.1252 - val_soft_acc: 0.6539\n",
      "Epoch 278/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1302 - soft_acc: 0.8800 - val_loss: 1.1106 - val_soft_acc: 0.6623\n",
      "Epoch 279/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1103 - soft_acc: 0.8916 - val_loss: 1.0979 - val_soft_acc: 0.6736\n",
      "Epoch 280/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.1245 - soft_acc: 0.8829 - val_loss: 1.1740 - val_soft_acc: 0.6297\n",
      "Epoch 281/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4919 - soft_acc: 0.6931 - val_loss: 1.8820 - val_soft_acc: 0.4492\n",
      "Epoch 282/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.7319 - soft_acc: 0.6032 - val_loss: 1.6430 - val_soft_acc: 0.5162\n",
      "Epoch 283/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5243 - soft_acc: 0.6667 - val_loss: 1.4762 - val_soft_acc: 0.5555\n",
      "Epoch 284/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2236 - soft_acc: 0.7916 - val_loss: 1.1657 - val_soft_acc: 0.6457\n",
      "Epoch 285/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1014 - soft_acc: 0.8831 - val_loss: 1.1216 - val_soft_acc: 0.6541\n",
      "Epoch 286/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1531 - soft_acc: 0.8650 - val_loss: 1.2387 - val_soft_acc: 0.6268\n",
      "Epoch 287/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1610 - soft_acc: 0.8587 - val_loss: 1.2718 - val_soft_acc: 0.6182\n",
      "Epoch 288/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2215 - soft_acc: 0.8223 - val_loss: 1.3330 - val_soft_acc: 0.5781\n",
      "Epoch 289/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2754 - soft_acc: 0.7779 - val_loss: 1.1975 - val_soft_acc: 0.6230\n",
      "Epoch 290/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1171 - soft_acc: 0.8666 - val_loss: 1.1266 - val_soft_acc: 0.6398\n",
      "Epoch 291/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0874 - soft_acc: 0.8946 - val_loss: 1.0768 - val_soft_acc: 0.6713\n",
      "Epoch 292/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0522 - soft_acc: 0.9282 - val_loss: 1.0341 - val_soft_acc: 0.6783\n",
      "Epoch 293/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.0482 - soft_acc: 0.93 - 1s 76us/step - loss: 0.0487 - soft_acc: 0.9356 - val_loss: 1.0339 - val_soft_acc: 0.6826\n",
      "Epoch 294/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1466 - soft_acc: 0.8824 - val_loss: 1.1850 - val_soft_acc: 0.6285\n",
      "Epoch 295/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.2734 - soft_acc: 0.7954 - val_loss: 1.5808 - val_soft_acc: 0.5141\n",
      "Epoch 296/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.7967 - soft_acc: 0.5905 - val_loss: 1.7352 - val_soft_acc: 0.4926\n",
      "Epoch 297/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4941 - soft_acc: 0.6840 - val_loss: 1.3095 - val_soft_acc: 0.5871\n",
      "Epoch 298/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1603 - soft_acc: 0.8254 - val_loss: 1.1494 - val_soft_acc: 0.6381\n",
      "Epoch 299/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0804 - soft_acc: 0.8933 - val_loss: 1.0726 - val_soft_acc: 0.6799\n",
      "Epoch 300/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0453 - soft_acc: 0.9319 - val_loss: 1.0384 - val_soft_acc: 0.6893\n",
      "Epoch 301/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0485 - soft_acc: 0.9399 - val_loss: 1.0999 - val_soft_acc: 0.6809\n",
      "Epoch 302/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0614 - soft_acc: 0.9327 - val_loss: 1.0832 - val_soft_acc: 0.6668\n",
      "Epoch 303/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.7050 - soft_acc: 0.6295 - val_loss: 1.6020 - val_soft_acc: 0.5184\n",
      "Epoch 304/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4029 - soft_acc: 0.7145 - val_loss: 1.2816 - val_soft_acc: 0.6080\n",
      "Epoch 305/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1215 - soft_acc: 0.8585 - val_loss: 1.1368 - val_soft_acc: 0.6650\n",
      "Epoch 306/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0799 - soft_acc: 0.9129 - val_loss: 1.3124 - val_soft_acc: 0.6270\n",
      "Epoch 307/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1980 - soft_acc: 0.8297 - val_loss: 1.1896 - val_soft_acc: 0.6541\n",
      "Epoch 308/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0856 - soft_acc: 0.9065 - val_loss: 1.1374 - val_soft_acc: 0.6756\n",
      "Epoch 309/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.0576 - soft_acc: 0.9264 - val_loss: 1.0852 - val_soft_acc: 0.6869\n",
      "Epoch 310/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0462 - soft_acc: 0.9384 - val_loss: 1.0835 - val_soft_acc: 0.6928\n",
      "Epoch 311/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0835 - soft_acc: 0.9257 - val_loss: 1.4281 - val_soft_acc: 0.6012\n",
      "Epoch 312/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.7833 - soft_acc: 0.6261 - val_loss: 1.7564 - val_soft_acc: 0.5041\n",
      "Epoch 313/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5304 - soft_acc: 0.6626 - val_loss: 1.4019 - val_soft_acc: 0.5746\n",
      "Epoch 314/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2261 - soft_acc: 0.7940 - val_loss: 1.3080 - val_soft_acc: 0.6162\n",
      "Epoch 315/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1178 - soft_acc: 0.8692 - val_loss: 1.1189 - val_soft_acc: 0.6680\n",
      "Epoch 316/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1741 - soft_acc: 0.8464 - val_loss: 1.4279 - val_soft_acc: 0.5736\n",
      "Epoch 317/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2720 - soft_acc: 0.7925 - val_loss: 1.2868 - val_soft_acc: 0.6191\n",
      "Epoch 318/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1764 - soft_acc: 0.8411 - val_loss: 1.1489 - val_soft_acc: 0.6566\n",
      "Epoch 319/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1055 - soft_acc: 0.8836 - val_loss: 1.1747 - val_soft_acc: 0.6607\n",
      "Epoch 320/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0715 - soft_acc: 0.9146 - val_loss: 1.0358 - val_soft_acc: 0.6906\n",
      "Epoch 321/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0987 - soft_acc: 0.9066 - val_loss: 1.2421 - val_soft_acc: 0.6172\n",
      "Epoch 322/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.0859 - soft_acc: 0.8945 - val_loss: 1.0196 - val_soft_acc: 0.6980\n",
      "Epoch 323/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.0464 - soft_acc: 0.9378 - val_loss: 1.0559 - val_soft_acc: 0.6936\n",
      "Epoch 324/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0631 - soft_acc: 0.9333 - val_loss: 1.0643 - val_soft_acc: 0.6783\n",
      "Epoch 325/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5987 - soft_acc: 0.6722 - val_loss: 1.8023 - val_soft_acc: 0.4787\n",
      "Epoch 326/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4759 - soft_acc: 0.6805 - val_loss: 1.5704 - val_soft_acc: 0.5328\n",
      "Epoch 327/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2581 - soft_acc: 0.7788 - val_loss: 1.2572 - val_soft_acc: 0.6254\n",
      "Epoch 328/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1327 - soft_acc: 0.8613 - val_loss: 1.1431 - val_soft_acc: 0.6434\n",
      "Epoch 329/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1383 - soft_acc: 0.8617 - val_loss: 1.1068 - val_soft_acc: 0.6625\n",
      "Epoch 330/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1085 - soft_acc: 0.8859 - val_loss: 1.1178 - val_soft_acc: 0.6652\n",
      "Epoch 331/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0620 - soft_acc: 0.9177 - val_loss: 1.0761 - val_soft_acc: 0.6723\n",
      "Epoch 332/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0955 - soft_acc: 0.9142 - val_loss: 1.2603 - val_soft_acc: 0.6566\n",
      "Epoch 333/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2090 - soft_acc: 0.8380 - val_loss: 1.2033 - val_soft_acc: 0.6291\n",
      "Epoch 334/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3028 - soft_acc: 0.7781 - val_loss: 1.5665 - val_soft_acc: 0.5514\n",
      "Epoch 335/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3132 - soft_acc: 0.7632 - val_loss: 1.2044 - val_soft_acc: 0.6508\n",
      "Epoch 336/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1161 - soft_acc: 0.8780 - val_loss: 1.0827 - val_soft_acc: 0.6787\n",
      "Epoch 337/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0979 - soft_acc: 0.8997 - val_loss: 1.0956 - val_soft_acc: 0.6619\n",
      "Epoch 338/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.0813 - soft_acc: 0.9074 - val_loss: 1.0537 - val_soft_acc: 0.6852\n",
      "Epoch 339/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0496 - soft_acc: 0.9341 - val_loss: 1.0765 - val_soft_acc: 0.6752\n",
      "Epoch 340/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0333 - soft_acc: 0.9493 - val_loss: 1.0136 - val_soft_acc: 0.7051\n",
      "Epoch 341/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0266 - soft_acc: 0.9572 - val_loss: 1.0007 - val_soft_acc: 0.7049\n",
      "Epoch 342/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0308 - soft_acc: 0.9586 - val_loss: 1.0050 - val_soft_acc: 0.6998\n",
      "Epoch 343/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0258 - soft_acc: 0.9599 - val_loss: 1.0201 - val_soft_acc: 0.6918\n",
      "Epoch 344/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0630 - soft_acc: 0.9287 - val_loss: 1.1202 - val_soft_acc: 0.6703\n",
      "Epoch 345/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5603 - soft_acc: 0.7241 - val_loss: 2.1284 - val_soft_acc: 0.4197\n",
      "Epoch 346/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.9708 - soft_acc: 0.5477 - val_loss: 1.6094 - val_soft_acc: 0.5139\n",
      "Epoch 347/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3323 - soft_acc: 0.7287 - val_loss: 1.2569 - val_soft_acc: 0.6096\n",
      "Epoch 348/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1107 - soft_acc: 0.8586 - val_loss: 1.1169 - val_soft_acc: 0.6676\n",
      "Epoch 349/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0443 - soft_acc: 0.9280 - val_loss: 1.0508 - val_soft_acc: 0.6908\n",
      "Epoch 350/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0369 - soft_acc: 0.9483 - val_loss: 1.0841 - val_soft_acc: 0.6873\n",
      "Epoch 351/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0315 - soft_acc: 0.9520 - val_loss: 1.0296 - val_soft_acc: 0.7096\n",
      "Epoch 352/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.0158 - soft_acc: 0.9650 - val_loss: 1.0100 - val_soft_acc: 0.7061\n",
      "Epoch 353/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0175 - soft_acc: 0.9680 - val_loss: 1.0218 - val_soft_acc: 0.7066\n",
      "Epoch 354/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0203 - soft_acc: 0.9673 - val_loss: 1.0247 - val_soft_acc: 0.7018\n",
      "Epoch 355/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1930 - soft_acc: 0.8617 - val_loss: 1.8883 - val_soft_acc: 0.4797\n",
      "Epoch 356/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8857 - soft_acc: 0.5717 - val_loss: 1.6755 - val_soft_acc: 0.4943\n",
      "Epoch 357/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3902 - soft_acc: 0.7132 - val_loss: 1.2588 - val_soft_acc: 0.6166\n",
      "Epoch 358/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2491 - soft_acc: 0.7986 - val_loss: 1.1487 - val_soft_acc: 0.6391\n",
      "Epoch 359/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0828 - soft_acc: 0.8902 - val_loss: 1.0663 - val_soft_acc: 0.6676\n",
      "Epoch 360/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0408 - soft_acc: 0.9366 - val_loss: 1.0373 - val_soft_acc: 0.6973\n",
      "Epoch 361/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0281 - soft_acc: 0.9540 - val_loss: 1.0361 - val_soft_acc: 0.7047\n",
      "Epoch 362/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0657 - soft_acc: 0.9258 - val_loss: 1.0873 - val_soft_acc: 0.6805\n",
      "Epoch 363/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0341 - soft_acc: 0.9461 - val_loss: 1.0162 - val_soft_acc: 0.7035\n",
      "Epoch 364/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0165 - soft_acc: 0.9656 - val_loss: 1.0061 - val_soft_acc: 0.7158\n",
      "Epoch 365/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0177 - soft_acc: 0.9693 - val_loss: 1.0491 - val_soft_acc: 0.6949\n",
      "Epoch 366/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.0414 - soft_acc: 0.9486 - val_loss: 1.1561 - val_soft_acc: 0.6824\n",
      "Epoch 367/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4558 - soft_acc: 0.7560 - val_loss: 2.0391 - val_soft_acc: 0.4621\n",
      "Epoch 368/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8343 - soft_acc: 0.5832 - val_loss: 1.7291 - val_soft_acc: 0.5262\n",
      "Epoch 369/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2917 - soft_acc: 0.7634 - val_loss: 1.2393 - val_soft_acc: 0.6340\n",
      "Epoch 370/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1163 - soft_acc: 0.8637 - val_loss: 1.1312 - val_soft_acc: 0.6734\n",
      "Epoch 371/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0526 - soft_acc: 0.9210 - val_loss: 1.0498 - val_soft_acc: 0.6936\n",
      "Epoch 372/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0352 - soft_acc: 0.9472 - val_loss: 1.0064 - val_soft_acc: 0.7041\n",
      "Epoch 373/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0169 - soft_acc: 0.9635 - val_loss: 0.9938 - val_soft_acc: 0.7074\n",
      "Epoch 374/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0196 - soft_acc: 0.9676 - val_loss: 1.0017 - val_soft_acc: 0.7080\n",
      "Epoch 375/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0163 - soft_acc: 0.9695 - val_loss: 1.0052 - val_soft_acc: 0.7084\n",
      "Epoch 376/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.0176 - soft_acc: 0.9675 - val_loss: 1.0803 - val_soft_acc: 0.6859\n",
      "Epoch 377/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.0682 - soft_acc: 0.9233 - val_loss: 1.1093 - val_soft_acc: 0.6697\n",
      "Epoch 378/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5209 - soft_acc: 0.7110 - val_loss: 1.7959 - val_soft_acc: 0.5082\n",
      "Epoch 379/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6539 - soft_acc: 0.6365 - val_loss: 1.5631 - val_soft_acc: 0.5336\n",
      "Epoch 380/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3290 - soft_acc: 0.7433 - val_loss: 1.2767 - val_soft_acc: 0.6133\n",
      "Epoch 381/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1081 - soft_acc: 0.8675 - val_loss: 1.0826 - val_soft_acc: 0.6857\n",
      "Epoch 382/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0481 - soft_acc: 0.9296 - val_loss: 1.0926 - val_soft_acc: 0.6857\n",
      "Epoch 383/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0365 - soft_acc: 0.9463 - val_loss: 1.0307 - val_soft_acc: 0.7053\n",
      "Epoch 384/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0176 - soft_acc: 0.9636 - val_loss: 1.0237 - val_soft_acc: 0.7070\n",
      "Epoch 385/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0153 - soft_acc: 0.9712 - val_loss: 1.0244 - val_soft_acc: 0.7066\n",
      "Epoch 386/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0097 - soft_acc: 0.9758 - val_loss: 1.0154 - val_soft_acc: 0.7098\n",
      "Epoch 387/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0190 - soft_acc: 0.9704 - val_loss: 1.0744 - val_soft_acc: 0.6967\n",
      "Epoch 388/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0135 - soft_acc: 0.9741 - val_loss: 1.0086 - val_soft_acc: 0.7082\n",
      "Epoch 389/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0084 - soft_acc: 0.9784 - val_loss: 1.0303 - val_soft_acc: 0.7111\n",
      "Epoch 390/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0663 - soft_acc: 0.9351 - val_loss: 1.2576 - val_soft_acc: 0.6461\n",
      "Epoch 391/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.9859 - soft_acc: 0.5620 - val_loss: 1.7963 - val_soft_acc: 0.4580\n",
      "Epoch 392/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.4798 - soft_acc: 0.6719 - val_loss: 1.4930 - val_soft_acc: 0.5598\n",
      "Epoch 393/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3016 - soft_acc: 0.7614 - val_loss: 1.2748 - val_soft_acc: 0.6141\n",
      "Epoch 394/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1759 - soft_acc: 0.8326 - val_loss: 1.2018 - val_soft_acc: 0.6381\n",
      "Epoch 395/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.0729 - soft_acc: 0.9045 - val_loss: 1.0386 - val_soft_acc: 0.6836\n",
      "Epoch 396/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0391 - soft_acc: 0.9416 - val_loss: 1.0202 - val_soft_acc: 0.6936\n",
      "Epoch 397/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0310 - soft_acc: 0.9524 - val_loss: 1.0027 - val_soft_acc: 0.6982\n",
      "Epoch 398/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0146 - soft_acc: 0.9675 - val_loss: 0.9706 - val_soft_acc: 0.7111\n",
      "Epoch 399/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0089 - soft_acc: 0.9750 - val_loss: 0.9637 - val_soft_acc: 0.7076\n",
      "Epoch 400/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.0162 - soft_acc: 0.9723 - val_loss: 0.9728 - val_soft_acc: 0.7064\n",
      "Epoch 401/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0289 - soft_acc: 0.9584 - val_loss: 1.0606 - val_soft_acc: 0.6879\n",
      "Epoch 402/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1581 - soft_acc: 0.8769 - val_loss: 1.5762 - val_soft_acc: 0.5365\n",
      "Epoch 403/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.6303 - soft_acc: 0.6447 - val_loss: 1.4887 - val_soft_acc: 0.5471\n",
      "Epoch 404/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3324 - soft_acc: 0.7511 - val_loss: 1.2050 - val_soft_acc: 0.6270\n",
      "Epoch 405/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1043 - soft_acc: 0.8765 - val_loss: 1.1272 - val_soft_acc: 0.6648\n",
      "Epoch 406/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0754 - soft_acc: 0.9105 - val_loss: 1.0452 - val_soft_acc: 0.6867\n",
      "Epoch 407/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0638 - soft_acc: 0.9255 - val_loss: 1.0290 - val_soft_acc: 0.6932\n",
      "Epoch 408/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0205 - soft_acc: 0.9583 - val_loss: 0.9932 - val_soft_acc: 0.7043\n",
      "Epoch 409/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0107 - soft_acc: 0.9725 - val_loss: 0.9757 - val_soft_acc: 0.7168\n",
      "Epoch 410/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0064 - soft_acc: 0.9783 - val_loss: 0.9777 - val_soft_acc: 0.7127\n",
      "Epoch 411/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0049 - soft_acc: 0.9818 - val_loss: 0.9791 - val_soft_acc: 0.7152\n",
      "Epoch 412/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0039 - soft_acc: 0.9839 - val_loss: 0.9732 - val_soft_acc: 0.7166\n",
      "Epoch 413/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0042 - soft_acc: 0.9844 - val_loss: 0.9915 - val_soft_acc: 0.7072\n",
      "Epoch 414/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8482 - soft_acc: 0.6630 - val_loss: 2.0158 - val_soft_acc: 0.4293\n",
      "Epoch 415/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.6877 - soft_acc: 0.6156 - val_loss: 1.3822 - val_soft_acc: 0.5473\n",
      "Epoch 416/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2168 - soft_acc: 0.7897 - val_loss: 1.1284 - val_soft_acc: 0.6357\n",
      "Epoch 417/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1362 - soft_acc: 0.8638 - val_loss: 1.0851 - val_soft_acc: 0.6615\n",
      "Epoch 418/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0518 - soft_acc: 0.9229 - val_loss: 1.0183 - val_soft_acc: 0.6936\n",
      "Epoch 419/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0248 - soft_acc: 0.9543 - val_loss: 1.0001 - val_soft_acc: 0.7031\n",
      "Epoch 420/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0142 - soft_acc: 0.9666 - val_loss: 1.0012 - val_soft_acc: 0.7049\n",
      "Epoch 421/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0096 - soft_acc: 0.9751 - val_loss: 0.9918 - val_soft_acc: 0.7113\n",
      "Epoch 422/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0112 - soft_acc: 0.9770 - val_loss: 0.9941 - val_soft_acc: 0.7098\n",
      "Epoch 423/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0065 - soft_acc: 0.9801 - val_loss: 0.9910 - val_soft_acc: 0.7170\n",
      "Epoch 424/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.0052 - soft_acc: 0.9814 - val_loss: 0.9871 - val_soft_acc: 0.7117\n",
      "Epoch 425/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0044 - soft_acc: 0.9838 - val_loss: 0.9934 - val_soft_acc: 0.7158\n",
      "Epoch 426/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0062 - soft_acc: 0.9817 - val_loss: 1.0529 - val_soft_acc: 0.6971\n",
      "Epoch 427/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5765 - soft_acc: 0.7334 - val_loss: 1.9200 - val_soft_acc: 0.4465\n",
      "Epoch 428/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6973 - soft_acc: 0.6145 - val_loss: 1.3328 - val_soft_acc: 0.5939\n",
      "Epoch 429/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1707 - soft_acc: 0.8185 - val_loss: 1.1290 - val_soft_acc: 0.6469\n",
      "Epoch 430/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.1229 - soft_acc: 0.8688 - val_loss: 1.0868 - val_soft_acc: 0.6654\n",
      "Epoch 431/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0608 - soft_acc: 0.9197 - val_loss: 0.9918 - val_soft_acc: 0.6926\n",
      "Epoch 432/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0274 - soft_acc: 0.9509 - val_loss: 0.9878 - val_soft_acc: 0.6996\n",
      "Epoch 433/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0149 - soft_acc: 0.9671 - val_loss: 0.9647 - val_soft_acc: 0.7055\n",
      "Epoch 434/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0083 - soft_acc: 0.9761 - val_loss: 0.9502 - val_soft_acc: 0.7092\n",
      "Epoch 435/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0054 - soft_acc: 0.9808 - val_loss: 0.9603 - val_soft_acc: 0.7105\n",
      "Epoch 436/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0037 - soft_acc: 0.9838 - val_loss: 0.9535 - val_soft_acc: 0.7092\n",
      "Epoch 437/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0029 - soft_acc: 0.9863 - val_loss: 0.9536 - val_soft_acc: 0.7133\n",
      "Epoch 438/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.0027 - soft_acc: 0.9875 - val_loss: 0.9585 - val_soft_acc: 0.7127\n",
      "Epoch 439/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0870 - soft_acc: 0.9329 - val_loss: 1.6105 - val_soft_acc: 0.5309\n",
      "Epoch 440/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.1396 - soft_acc: 0.5228 - val_loss: 1.5590 - val_soft_acc: 0.5205\n",
      "Epoch 441/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.3236 - soft_acc: 0.7309 - val_loss: 1.2141 - val_soft_acc: 0.6162\n",
      "Epoch 442/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.2008 - soft_acc: 0.8194 - val_loss: 1.2248 - val_soft_acc: 0.6334\n",
      "Epoch 443/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.1193 - soft_acc: 0.8684 - val_loss: 1.1945 - val_soft_acc: 0.6402\n",
      "Epoch 444/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0747 - soft_acc: 0.9109 - val_loss: 1.0899 - val_soft_acc: 0.6895\n",
      "Epoch 445/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0350 - soft_acc: 0.9472 - val_loss: 0.9993 - val_soft_acc: 0.7072\n",
      "Epoch 446/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0140 - soft_acc: 0.9683 - val_loss: 0.9806 - val_soft_acc: 0.7055\n",
      "Epoch 447/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0083 - soft_acc: 0.9758 - val_loss: 0.9774 - val_soft_acc: 0.7109\n",
      "Epoch 448/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0054 - soft_acc: 0.9813 - val_loss: 0.9776 - val_soft_acc: 0.7182\n",
      "Epoch 449/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0040 - soft_acc: 0.9838 - val_loss: 0.9805 - val_soft_acc: 0.7182\n",
      "Epoch 450/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0036 - soft_acc: 0.9850 - val_loss: 0.9763 - val_soft_acc: 0.7156\n",
      "Epoch 451/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0031 - soft_acc: 0.9865 - val_loss: 0.9766 - val_soft_acc: 0.7154\n",
      "Epoch 452/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0029 - soft_acc: 0.9860 - val_loss: 0.9778 - val_soft_acc: 0.7129\n",
      "Epoch 453/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0034 - soft_acc: 0.9858 - val_loss: 0.9768 - val_soft_acc: 0.7160\n",
      "Epoch 454/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0044 - soft_acc: 0.9831 - val_loss: 0.9731 - val_soft_acc: 0.7137\n",
      "Epoch 455/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0082 - soft_acc: 0.9760 - val_loss: 0.9929 - val_soft_acc: 0.7125\n",
      "Epoch 456/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6165 - soft_acc: 0.7530 - val_loss: 2.5007 - val_soft_acc: 0.3432\n",
      "Epoch 457/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.1867 - soft_acc: 0.4978 - val_loss: 1.5434 - val_soft_acc: 0.5408\n",
      "Epoch 458/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2476 - soft_acc: 0.7615 - val_loss: 1.2524 - val_soft_acc: 0.6184\n",
      "Epoch 459/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1509 - soft_acc: 0.8386 - val_loss: 1.2178 - val_soft_acc: 0.6408\n",
      "Epoch 460/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0810 - soft_acc: 0.9000 - val_loss: 1.0486 - val_soft_acc: 0.6885\n",
      "Epoch 461/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0397 - soft_acc: 0.9363 - val_loss: 1.0127 - val_soft_acc: 0.6998\n",
      "Epoch 462/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0214 - soft_acc: 0.9625 - val_loss: 1.0253 - val_soft_acc: 0.6982\n",
      "Epoch 463/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0217 - soft_acc: 0.9625 - val_loss: 1.0094 - val_soft_acc: 0.7053\n",
      "Epoch 464/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0440 - soft_acc: 0.9507 - val_loss: 1.0537 - val_soft_acc: 0.6840\n",
      "Epoch 465/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1412 - soft_acc: 0.8893 - val_loss: 1.3925 - val_soft_acc: 0.5945\n",
      "Epoch 466/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3841 - soft_acc: 0.7465 - val_loss: 1.4694 - val_soft_acc: 0.5674\n",
      "Epoch 467/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.2141 - soft_acc: 0.81 - 1s 78us/step - loss: 0.2127 - soft_acc: 0.8121 - val_loss: 1.2242 - val_soft_acc: 0.6449\n",
      "Epoch 468/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0864 - soft_acc: 0.8978 - val_loss: 1.1075 - val_soft_acc: 0.6904\n",
      "Epoch 469/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0333 - soft_acc: 0.9421 - val_loss: 1.0333 - val_soft_acc: 0.7057\n",
      "Epoch 470/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0149 - soft_acc: 0.9676 - val_loss: 1.0155 - val_soft_acc: 0.7115\n",
      "Epoch 471/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0085 - soft_acc: 0.9778 - val_loss: 1.0012 - val_soft_acc: 0.7174\n",
      "Epoch 472/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0052 - soft_acc: 0.9821 - val_loss: 1.0046 - val_soft_acc: 0.7152\n",
      "Epoch 473/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0039 - soft_acc: 0.9848 - val_loss: 1.0067 - val_soft_acc: 0.7156\n",
      "Epoch 474/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0029 - soft_acc: 0.9852 - val_loss: 0.9981 - val_soft_acc: 0.7121\n",
      "Epoch 475/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0024 - soft_acc: 0.9878 - val_loss: 0.9966 - val_soft_acc: 0.7164\n",
      "Epoch 476/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0022 - soft_acc: 0.9882 - val_loss: 1.0016 - val_soft_acc: 0.7154\n",
      "Epoch 477/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0031 - soft_acc: 0.9855 - val_loss: 1.0041 - val_soft_acc: 0.7156\n",
      "Epoch 478/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0042 - soft_acc: 0.9831 - val_loss: 1.0011 - val_soft_acc: 0.7135\n",
      "Epoch 479/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0072 - soft_acc: 0.9794 - val_loss: 1.0017 - val_soft_acc: 0.7102\n",
      "Epoch 480/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.9867 - soft_acc: 0.6431 - val_loss: 2.1272 - val_soft_acc: 0.3916\n",
      "Epoch 481/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.8343 - soft_acc: 0.5775 - val_loss: 1.3472 - val_soft_acc: 0.5611\n",
      "Epoch 482/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2358 - soft_acc: 0.7747 - val_loss: 1.2152 - val_soft_acc: 0.6238\n",
      "Epoch 483/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0916 - soft_acc: 0.8817 - val_loss: 1.0766 - val_soft_acc: 0.6748\n",
      "Epoch 484/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.0473 - soft_acc: 0.9295 - val_loss: 1.0119 - val_soft_acc: 0.6982\n",
      "Epoch 00484: early stopping\n",
      ">0.698\n",
      "Train on 14640 samples, validate on 2440 samples\n",
      "Epoch 1/5000\n",
      "14640/14640 [==============================] - 2s 118us/step - loss: 5.1442 - soft_acc: 0.1515 - val_loss: 4.1820 - val_soft_acc: 0.1623\n",
      "Epoch 2/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 4.2419 - soft_acc: 0.1568 - val_loss: 4.3592 - val_soft_acc: 0.1588\n",
      "Epoch 3/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 4.1256 - soft_acc: 0.1636 - val_loss: 3.9510 - val_soft_acc: 0.1469\n",
      "Epoch 4/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 4.0715 - soft_acc: 0.1675 - val_loss: 3.9167 - val_soft_acc: 0.1850\n",
      "Epoch 5/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.9896 - soft_acc: 0.1713 - val_loss: 3.9749 - val_soft_acc: 0.1768\n",
      "Epoch 6/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.9207 - soft_acc: 0.1716 - val_loss: 3.7664 - val_soft_acc: 0.1703\n",
      "Epoch 7/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.9030 - soft_acc: 0.1711 - val_loss: 4.3889 - val_soft_acc: 0.2000\n",
      "Epoch 8/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.8697 - soft_acc: 0.1734 - val_loss: 3.7102 - val_soft_acc: 0.1830\n",
      "Epoch 9/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.8265 - soft_acc: 0.1766 - val_loss: 3.8035 - val_soft_acc: 0.1709\n",
      "Epoch 10/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.8062 - soft_acc: 0.1771 - val_loss: 4.0692 - val_soft_acc: 0.1912\n",
      "Epoch 11/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.8189 - soft_acc: 0.1751 - val_loss: 3.8743 - val_soft_acc: 0.1963\n",
      "Epoch 12/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.7507 - soft_acc: 0.1781 - val_loss: 3.7219 - val_soft_acc: 0.1885\n",
      "Epoch 13/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.7387 - soft_acc: 0.1821 - val_loss: 3.6116 - val_soft_acc: 0.1918\n",
      "Epoch 14/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.6891 - soft_acc: 0.1804 - val_loss: 3.6739 - val_soft_acc: 0.1836\n",
      "Epoch 15/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.6986 - soft_acc: 0.1809 - val_loss: 3.5596 - val_soft_acc: 0.1998\n",
      "Epoch 16/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.6605 - soft_acc: 0.1850 - val_loss: 3.5907 - val_soft_acc: 0.1775\n",
      "Epoch 17/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.6846 - soft_acc: 0.1835 - val_loss: 3.7798 - val_soft_acc: 0.2012\n",
      "Epoch 18/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.6341 - soft_acc: 0.1879 - val_loss: 3.6589 - val_soft_acc: 0.2055\n",
      "Epoch 19/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.6147 - soft_acc: 0.1834 - val_loss: 3.5840 - val_soft_acc: 0.1797\n",
      "Epoch 20/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.5876 - soft_acc: 0.1858 - val_loss: 3.5939 - val_soft_acc: 0.1918\n",
      "Epoch 21/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.5876 - soft_acc: 0.1869 - val_loss: 3.4717 - val_soft_acc: 0.1951\n",
      "Epoch 22/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.5521 - soft_acc: 0.1889 - val_loss: 3.5553 - val_soft_acc: 0.2049\n",
      "Epoch 23/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.5409 - soft_acc: 0.1902 - val_loss: 3.4937 - val_soft_acc: 0.1875\n",
      "Epoch 24/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.5100 - soft_acc: 0.1911 - val_loss: 3.4263 - val_soft_acc: 0.1883\n",
      "Epoch 25/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 3.4958 - soft_acc: 0.1936 - val_loss: 3.4151 - val_soft_acc: 0.2125\n",
      "Epoch 26/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.4730 - soft_acc: 0.1939 - val_loss: 3.3987 - val_soft_acc: 0.1963\n",
      "Epoch 27/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.4548 - soft_acc: 0.1960 - val_loss: 3.4382 - val_soft_acc: 0.2143\n",
      "Epoch 28/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.4661 - soft_acc: 0.1966 - val_loss: 3.4081 - val_soft_acc: 0.2010\n",
      "Epoch 29/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.4389 - soft_acc: 0.1971 - val_loss: 3.3871 - val_soft_acc: 0.1957\n",
      "Epoch 30/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.3967 - soft_acc: 0.1988 - val_loss: 3.3924 - val_soft_acc: 0.2078\n",
      "Epoch 31/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.3841 - soft_acc: 0.2016 - val_loss: 3.4452 - val_soft_acc: 0.1930\n",
      "Epoch 32/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.3688 - soft_acc: 0.2046 - val_loss: 3.3029 - val_soft_acc: 0.2123\n",
      "Epoch 33/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.3209 - soft_acc: 0.2048 - val_loss: 3.3084 - val_soft_acc: 0.1982\n",
      "Epoch 34/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.3538 - soft_acc: 0.2034 - val_loss: 3.4477 - val_soft_acc: 0.2287\n",
      "Epoch 35/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.3008 - soft_acc: 0.2093 - val_loss: 3.2672 - val_soft_acc: 0.2135\n",
      "Epoch 36/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.3071 - soft_acc: 0.2064 - val_loss: 3.3134 - val_soft_acc: 0.1949\n",
      "Epoch 37/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.2786 - soft_acc: 0.2081 - val_loss: 3.2031 - val_soft_acc: 0.2174\n",
      "Epoch 38/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.2412 - soft_acc: 0.2139 - val_loss: 3.3929 - val_soft_acc: 0.2131\n",
      "Epoch 39/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 3.2396 - soft_acc: 0.2146 - val_loss: 3.2244 - val_soft_acc: 0.2184\n",
      "Epoch 40/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.2128 - soft_acc: 0.2156 - val_loss: 3.4181 - val_soft_acc: 0.2117\n",
      "Epoch 41/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.1989 - soft_acc: 0.2176 - val_loss: 3.1664 - val_soft_acc: 0.2115\n",
      "Epoch 42/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.1552 - soft_acc: 0.2202 - val_loss: 3.1307 - val_soft_acc: 0.2250\n",
      "Epoch 43/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.1720 - soft_acc: 0.2190 - val_loss: 3.1425 - val_soft_acc: 0.2246\n",
      "Epoch 44/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.1376 - soft_acc: 0.2209 - val_loss: 3.1517 - val_soft_acc: 0.2332\n",
      "Epoch 45/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.1543 - soft_acc: 0.2183 - val_loss: 3.0934 - val_soft_acc: 0.2287\n",
      "Epoch 46/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.0697 - soft_acc: 0.2267 - val_loss: 3.0290 - val_soft_acc: 0.2338\n",
      "Epoch 47/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.0670 - soft_acc: 0.2242 - val_loss: 3.0927 - val_soft_acc: 0.2162\n",
      "Epoch 48/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.0539 - soft_acc: 0.2270 - val_loss: 3.0424 - val_soft_acc: 0.2418\n",
      "Epoch 49/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.0367 - soft_acc: 0.2305 - val_loss: 3.0199 - val_soft_acc: 0.2510\n",
      "Epoch 50/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.9982 - soft_acc: 0.2314 - val_loss: 3.0446 - val_soft_acc: 0.2316\n",
      "Epoch 51/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.9913 - soft_acc: 0.2321 - val_loss: 2.9850 - val_soft_acc: 0.2365\n",
      "Epoch 52/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 2.9805 - soft_acc: 0.2370 - val_loss: 2.9459 - val_soft_acc: 0.2395\n",
      "Epoch 53/5000\n",
      "14640/14640 [==============================] - 1s 81us/step - loss: 2.9459 - soft_acc: 0.2371 - val_loss: 2.9464 - val_soft_acc: 0.2424\n",
      "Epoch 54/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.9285 - soft_acc: 0.2364 - val_loss: 2.8767 - val_soft_acc: 0.2533\n",
      "Epoch 55/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.8960 - soft_acc: 0.2431 - val_loss: 2.9615 - val_soft_acc: 0.2398\n",
      "Epoch 56/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.9065 - soft_acc: 0.2391 - val_loss: 2.8749 - val_soft_acc: 0.2381\n",
      "Epoch 57/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.8323 - soft_acc: 0.2429 - val_loss: 2.9099 - val_soft_acc: 0.2629\n",
      "Epoch 58/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.8292 - soft_acc: 0.2444 - val_loss: 2.8723 - val_soft_acc: 0.2525\n",
      "Epoch 59/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.7903 - soft_acc: 0.2494 - val_loss: 2.8579 - val_soft_acc: 0.2539t\n",
      "Epoch 60/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.7686 - soft_acc: 0.2511 - val_loss: 2.8437 - val_soft_acc: 0.2439\n",
      "Epoch 61/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.7554 - soft_acc: 0.2544 - val_loss: 2.7915 - val_soft_acc: 0.2543\n",
      "Epoch 62/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.7338 - soft_acc: 0.2536 - val_loss: 2.7283 - val_soft_acc: 0.2623\n",
      "Epoch 63/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.7458 - soft_acc: 0.2589 - val_loss: 2.7731 - val_soft_acc: 0.2389\n",
      "Epoch 64/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.6713 - soft_acc: 0.2634 - val_loss: 2.7563 - val_soft_acc: 0.2668\n",
      "Epoch 65/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.6703 - soft_acc: 0.2607 - val_loss: 2.7306 - val_soft_acc: 0.2654\n",
      "Epoch 66/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.6281 - soft_acc: 0.2663 - val_loss: 2.7419 - val_soft_acc: 0.2598\n",
      "Epoch 67/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.6001 - soft_acc: 0.2670 - val_loss: 2.7153 - val_soft_acc: 0.2680\n",
      "Epoch 68/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 2.5750 - soft_acc: 0.2688 - val_loss: 2.6828 - val_soft_acc: 0.2672\n",
      "Epoch 69/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.5477 - soft_acc: 0.2740 - val_loss: 2.7372 - val_soft_acc: 0.2686\n",
      "Epoch 70/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.5245 - soft_acc: 0.2757 - val_loss: 2.6391 - val_soft_acc: 0.2730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.4710 - soft_acc: 0.2822 - val_loss: 2.6232 - val_soft_acc: 0.2740\n",
      "Epoch 72/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.4251 - soft_acc: 0.2883 - val_loss: 2.6400 - val_soft_acc: 0.2900\n",
      "Epoch 73/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.4245 - soft_acc: 0.2881 - val_loss: 2.6886 - val_soft_acc: 0.2775\n",
      "Epoch 74/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.3860 - soft_acc: 0.2938 - val_loss: 2.5870 - val_soft_acc: 0.2818\n",
      "Epoch 75/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.3754 - soft_acc: 0.2962 - val_loss: 2.5202 - val_soft_acc: 0.2879\n",
      "Epoch 76/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.3594 - soft_acc: 0.2945 - val_loss: 2.4937 - val_soft_acc: 0.2902\n",
      "Epoch 77/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.3214 - soft_acc: 0.2958 - val_loss: 2.4968 - val_soft_acc: 0.2922\n",
      "Epoch 78/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.2889 - soft_acc: 0.3031 - val_loss: 2.5639 - val_soft_acc: 0.2932\n",
      "Epoch 79/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.2571 - soft_acc: 0.3036 - val_loss: 2.4247 - val_soft_acc: 0.2957\n",
      "Epoch 80/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.2486 - soft_acc: 0.3091 - val_loss: 2.4358 - val_soft_acc: 0.3076\n",
      "Epoch 81/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.2175 - soft_acc: 0.3110 - val_loss: 2.4651 - val_soft_acc: 0.2996\n",
      "Epoch 82/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 2.1630 - soft_acc: 0.3161 - val_loss: 2.3526 - val_soft_acc: 0.3107\n",
      "Epoch 83/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 2.1412 - soft_acc: 0.31 - 1s 76us/step - loss: 2.1413 - soft_acc: 0.3186 - val_loss: 2.4499 - val_soft_acc: 0.2895\n",
      "Epoch 84/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.1254 - soft_acc: 0.3194 - val_loss: 2.5435 - val_soft_acc: 0.2986\n",
      "Epoch 85/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.0767 - soft_acc: 0.3252 - val_loss: 2.2498 - val_soft_acc: 0.3221\n",
      "Epoch 86/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.0105 - soft_acc: 0.3364 - val_loss: 2.2983 - val_soft_acc: 0.3186\n",
      "Epoch 87/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.0630 - soft_acc: 0.3299 - val_loss: 2.3089 - val_soft_acc: 0.3268\n",
      "Epoch 88/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.9929 - soft_acc: 0.3371 - val_loss: 2.2959 - val_soft_acc: 0.3176\n",
      "Epoch 89/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.9946 - soft_acc: 0.3373 - val_loss: 2.2206 - val_soft_acc: 0.3264\n",
      "Epoch 90/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.9109 - soft_acc: 0.3503 - val_loss: 2.3000 - val_soft_acc: 0.3172\n",
      "Epoch 91/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.8714 - soft_acc: 0.3523 - val_loss: 2.1114 - val_soft_acc: 0.3391\n",
      "Epoch 92/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.8372 - soft_acc: 0.3596 - val_loss: 2.2899 - val_soft_acc: 0.3336\n",
      "Epoch 93/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.8293 - soft_acc: 0.3589 - val_loss: 2.1603 - val_soft_acc: 0.3363\n",
      "Epoch 94/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.7668 - soft_acc: 0.3652 - val_loss: 2.2148 - val_soft_acc: 0.3238\n",
      "Epoch 95/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.7800 - soft_acc: 0.3668 - val_loss: 2.2322 - val_soft_acc: 0.3342\n",
      "Epoch 96/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 1.7769 - soft_acc: 0.3621 - val_loss: 2.2204 - val_soft_acc: 0.3332\n",
      "Epoch 97/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.7710 - soft_acc: 0.3642 - val_loss: 2.2741 - val_soft_acc: 0.3316\n",
      "Epoch 98/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.6864 - soft_acc: 0.3810 - val_loss: 2.2553 - val_soft_acc: 0.3422\n",
      "Epoch 99/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.6862 - soft_acc: 0.3784 - val_loss: 2.0810 - val_soft_acc: 0.3504\n",
      "Epoch 100/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.6412 - soft_acc: 0.3827 - val_loss: 2.1206 - val_soft_acc: 0.3445\n",
      "Epoch 101/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.6087 - soft_acc: 0.3921 - val_loss: 2.2964 - val_soft_acc: 0.3416\n",
      "Epoch 102/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.6457 - soft_acc: 0.3807 - val_loss: 2.1554 - val_soft_acc: 0.3488\n",
      "Epoch 103/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.5542 - soft_acc: 0.3999 - val_loss: 2.0290 - val_soft_acc: 0.3641\n",
      "Epoch 104/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.5037 - soft_acc: 0.4034 - val_loss: 2.0363 - val_soft_acc: 0.3904\n",
      "Epoch 105/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.4700 - soft_acc: 0.4102 - val_loss: 1.9676 - val_soft_acc: 0.3773\n",
      "Epoch 106/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 1.5025 - soft_acc: 0.4085 - val_loss: 1.9512 - val_soft_acc: 0.3924\n",
      "Epoch 107/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.4426 - soft_acc: 0.4165 - val_loss: 1.9255 - val_soft_acc: 0.3760\n",
      "Epoch 108/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.3726 - soft_acc: 0.4290 - val_loss: 1.9773 - val_soft_acc: 0.3926\n",
      "Epoch 109/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 1.3972 - soft_acc: 0.42 - 1s 76us/step - loss: 1.4005 - soft_acc: 0.4233 - val_loss: 1.9443 - val_soft_acc: 0.3904\n",
      "Epoch 110/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.4027 - soft_acc: 0.4233 - val_loss: 2.0325 - val_soft_acc: 0.3797\n",
      "Epoch 111/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 1.3575 - soft_acc: 0.4310 - val_loss: 1.9547 - val_soft_acc: 0.3846\n",
      "Epoch 112/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.3162 - soft_acc: 0.4453 - val_loss: 2.0058 - val_soft_acc: 0.3820\n",
      "Epoch 113/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.3086 - soft_acc: 0.4402 - val_loss: 1.9436 - val_soft_acc: 0.3926\n",
      "Epoch 114/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.3111 - soft_acc: 0.4460 - val_loss: 1.9966 - val_soft_acc: 0.3803\n",
      "Epoch 115/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.2914 - soft_acc: 0.4407 - val_loss: 2.1620 - val_soft_acc: 0.3635\n",
      "Epoch 116/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.2334 - soft_acc: 0.4554 - val_loss: 1.8892 - val_soft_acc: 0.4061\n",
      "Epoch 117/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.2223 - soft_acc: 0.4585 - val_loss: 1.9271 - val_soft_acc: 0.4080\n",
      "Epoch 118/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.1151 - soft_acc: 0.4796 - val_loss: 1.7497 - val_soft_acc: 0.4166\n",
      "Epoch 119/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.1207 - soft_acc: 0.4806 - val_loss: 1.9048 - val_soft_acc: 0.3881\n",
      "Epoch 120/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.1198 - soft_acc: 0.4865 - val_loss: 1.9043 - val_soft_acc: 0.4082\n",
      "Epoch 121/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.1588 - soft_acc: 0.4699 - val_loss: 1.8400 - val_soft_acc: 0.4082\n",
      "Epoch 122/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.1805 - soft_acc: 0.4678 - val_loss: 1.8278 - val_soft_acc: 0.4184\n",
      "Epoch 123/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.1441 - soft_acc: 0.4750 - val_loss: 1.7069 - val_soft_acc: 0.4410\n",
      "Epoch 124/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.1141 - soft_acc: 0.4833 - val_loss: 1.8304 - val_soft_acc: 0.4158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 1.0889 - soft_acc: 0.4861 - val_loss: 1.7651 - val_soft_acc: 0.4336\n",
      "Epoch 126/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.0160 - soft_acc: 0.5018 - val_loss: 1.8385 - val_soft_acc: 0.4199\n",
      "Epoch 127/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.0097 - soft_acc: 0.5059 - val_loss: 1.8208 - val_soft_acc: 0.4283\n",
      "Epoch 128/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.1062 - soft_acc: 0.4859 - val_loss: 1.7179 - val_soft_acc: 0.4365\n",
      "Epoch 129/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.9717 - soft_acc: 0.5090 - val_loss: 1.8334 - val_soft_acc: 0.4289\n",
      "Epoch 130/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.9390 - soft_acc: 0.5222 - val_loss: 1.7769 - val_soft_acc: 0.4379\n",
      "Epoch 131/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.9466 - soft_acc: 0.5220 - val_loss: 1.7485 - val_soft_acc: 0.4461\n",
      "Epoch 132/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.9179 - soft_acc: 0.5294 - val_loss: 1.7016 - val_soft_acc: 0.4350\n",
      "Epoch 133/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.0091 - soft_acc: 0.5060 - val_loss: 1.7947 - val_soft_acc: 0.4238\n",
      "Epoch 134/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8982 - soft_acc: 0.5307 - val_loss: 1.6663 - val_soft_acc: 0.4463\n",
      "Epoch 135/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.9071 - soft_acc: 0.5310 - val_loss: 1.7208 - val_soft_acc: 0.4449\n",
      "Epoch 136/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.9014 - soft_acc: 0.5335 - val_loss: 1.6046 - val_soft_acc: 0.4602\n",
      "Epoch 137/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8847 - soft_acc: 0.5338 - val_loss: 1.6877 - val_soft_acc: 0.4711\n",
      "Epoch 138/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8400 - soft_acc: 0.5423 - val_loss: 1.6777 - val_soft_acc: 0.4635\n",
      "Epoch 139/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.8915 - soft_acc: 0.5372 - val_loss: 1.7750 - val_soft_acc: 0.4301\n",
      "Epoch 140/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8644 - soft_acc: 0.5428 - val_loss: 1.7533 - val_soft_acc: 0.4500\n",
      "Epoch 141/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8865 - soft_acc: 0.5359 - val_loss: 1.6938 - val_soft_acc: 0.4447\n",
      "Epoch 142/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.8930 - soft_acc: 0.5335 - val_loss: 1.6767 - val_soft_acc: 0.4674\n",
      "Epoch 143/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.7581 - soft_acc: 0.5711 - val_loss: 1.5745 - val_soft_acc: 0.4824\n",
      "Epoch 144/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.7448 - soft_acc: 0.5776 - val_loss: 1.6754 - val_soft_acc: 0.4779\n",
      "Epoch 145/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.8028 - soft_acc: 0.5619 - val_loss: 1.6726 - val_soft_acc: 0.4416\n",
      "Epoch 146/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.7706 - soft_acc: 0.5671 - val_loss: 1.6917 - val_soft_acc: 0.4686\n",
      "Epoch 147/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.8482 - soft_acc: 0.5468 - val_loss: 1.7196 - val_soft_acc: 0.4461\n",
      "Epoch 148/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.7467 - soft_acc: 0.5733 - val_loss: 1.6168 - val_soft_acc: 0.4709\n",
      "Epoch 149/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.7075 - soft_acc: 0.5856 - val_loss: 1.6218 - val_soft_acc: 0.4783\n",
      "Epoch 150/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6705 - soft_acc: 0.6004 - val_loss: 1.7205 - val_soft_acc: 0.4531\n",
      "Epoch 151/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.8181 - soft_acc: 0.5569 - val_loss: 1.5397 - val_soft_acc: 0.5014\n",
      "Epoch 152/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.7186 - soft_acc: 0.5864 - val_loss: 1.6805 - val_soft_acc: 0.4641\n",
      "Epoch 153/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.7230 - soft_acc: 0.5813 - val_loss: 1.5467 - val_soft_acc: 0.4920\n",
      "Epoch 154/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.6558 - soft_acc: 0.6121 - val_loss: 1.6634 - val_soft_acc: 0.4822\n",
      "Epoch 155/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8940 - soft_acc: 0.5389 - val_loss: 1.6625 - val_soft_acc: 0.4746\n",
      "Epoch 156/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6174 - soft_acc: 0.6171 - val_loss: 1.6096 - val_soft_acc: 0.4848\n",
      "Epoch 157/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6369 - soft_acc: 0.6114 - val_loss: 1.5301 - val_soft_acc: 0.5080\n",
      "Epoch 158/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.6046 - soft_acc: 0.6284 - val_loss: 1.5285 - val_soft_acc: 0.4941\n",
      "Epoch 159/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.6329 - soft_acc: 0.6168 - val_loss: 1.5890 - val_soft_acc: 0.4988\n",
      "Epoch 160/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.5982 - soft_acc: 0.6252 - val_loss: 1.5942 - val_soft_acc: 0.4998\n",
      "Epoch 161/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.5663 - soft_acc: 0.63 - 1s 76us/step - loss: 0.5706 - soft_acc: 0.6361 - val_loss: 1.5543 - val_soft_acc: 0.5064\n",
      "Epoch 162/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6254 - soft_acc: 0.6176 - val_loss: 1.7305 - val_soft_acc: 0.4664\n",
      "Epoch 163/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8313 - soft_acc: 0.5513 - val_loss: 1.7407 - val_soft_acc: 0.4770\n",
      "Epoch 164/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.6512 - soft_acc: 0.6025 - val_loss: 1.5601 - val_soft_acc: 0.4932\n",
      "Epoch 165/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5496 - soft_acc: 0.6366 - val_loss: 1.5061 - val_soft_acc: 0.5223\n",
      "Epoch 166/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5252 - soft_acc: 0.6568 - val_loss: 1.6083 - val_soft_acc: 0.5057\n",
      "Epoch 167/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5588 - soft_acc: 0.6389 - val_loss: 1.4435 - val_soft_acc: 0.5102\n",
      "Epoch 168/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5553 - soft_acc: 0.6447 - val_loss: 1.5329 - val_soft_acc: 0.5211\n",
      "Epoch 169/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6180 - soft_acc: 0.6157 - val_loss: 1.5466 - val_soft_acc: 0.5068\n",
      "Epoch 170/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5609 - soft_acc: 0.6372 - val_loss: 1.5440 - val_soft_acc: 0.5100\n",
      "Epoch 171/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5732 - soft_acc: 0.6388 - val_loss: 1.5261 - val_soft_acc: 0.5037\n",
      "Epoch 172/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5116 - soft_acc: 0.6589 - val_loss: 1.4393 - val_soft_acc: 0.5295\n",
      "Epoch 173/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5244 - soft_acc: 0.6653 - val_loss: 1.6825 - val_soft_acc: 0.4697\n",
      "Epoch 174/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.7440 - soft_acc: 0.5839 - val_loss: 1.6705 - val_soft_acc: 0.4826\n",
      "Epoch 175/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6987 - soft_acc: 0.5902 - val_loss: 1.5285 - val_soft_acc: 0.5135\n",
      "Epoch 176/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5039 - soft_acc: 0.6575 - val_loss: 1.5401 - val_soft_acc: 0.5186\n",
      "Epoch 177/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4777 - soft_acc: 0.6766 - val_loss: 1.4263 - val_soft_acc: 0.5223\n",
      "Epoch 178/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4261 - soft_acc: 0.6959 - val_loss: 1.4313 - val_soft_acc: 0.5502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4338 - soft_acc: 0.6957 - val_loss: 1.4473 - val_soft_acc: 0.5404\n",
      "Epoch 180/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5254 - soft_acc: 0.6551 - val_loss: 1.5124 - val_soft_acc: 0.5199\n",
      "Epoch 181/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5403 - soft_acc: 0.6473 - val_loss: 1.4421 - val_soft_acc: 0.5350\n",
      "Epoch 182/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.4601 - soft_acc: 0.6814 - val_loss: 1.3977 - val_soft_acc: 0.5436\n",
      "Epoch 183/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4328 - soft_acc: 0.6958 - val_loss: 1.4981 - val_soft_acc: 0.5270\n",
      "Epoch 184/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5524 - soft_acc: 0.6561 - val_loss: 1.7117 - val_soft_acc: 0.4912\n",
      "Epoch 185/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5913 - soft_acc: 0.6398 - val_loss: 1.7816 - val_soft_acc: 0.4518\n",
      "Epoch 186/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6265 - soft_acc: 0.6247 - val_loss: 1.5617 - val_soft_acc: 0.5176\n",
      "Epoch 187/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6703 - soft_acc: 0.6113 - val_loss: 1.5694 - val_soft_acc: 0.5154\n",
      "Epoch 188/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4613 - soft_acc: 0.6711 - val_loss: 1.3635 - val_soft_acc: 0.5672\n",
      "Epoch 189/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3482 - soft_acc: 0.7292 - val_loss: 1.3732 - val_soft_acc: 0.5576\n",
      "Epoch 190/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3879 - soft_acc: 0.7192 - val_loss: 1.4201 - val_soft_acc: 0.5443\n",
      "Epoch 191/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5774 - soft_acc: 0.6515 - val_loss: 1.4414 - val_soft_acc: 0.5160\n",
      "Epoch 192/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4431 - soft_acc: 0.6853 - val_loss: 1.4447 - val_soft_acc: 0.5318\n",
      "Epoch 193/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3732 - soft_acc: 0.7248 - val_loss: 1.3139 - val_soft_acc: 0.5637\n",
      "Epoch 194/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3276 - soft_acc: 0.7472 - val_loss: 1.4370 - val_soft_acc: 0.5520\n",
      "Epoch 195/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3460 - soft_acc: 0.7421 - val_loss: 1.3438 - val_soft_acc: 0.5553\n",
      "Epoch 196/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.4279 - soft_acc: 0.6983 - val_loss: 1.4552 - val_soft_acc: 0.5324\n",
      "Epoch 197/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4720 - soft_acc: 0.6810 - val_loss: 1.6220 - val_soft_acc: 0.5203\n",
      "Epoch 198/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6247 - soft_acc: 0.6186 - val_loss: 1.5860 - val_soft_acc: 0.4910\n",
      "Epoch 199/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5907 - soft_acc: 0.6293 - val_loss: 1.4963 - val_soft_acc: 0.5148\n",
      "Epoch 200/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3799 - soft_acc: 0.7059 - val_loss: 1.3189 - val_soft_acc: 0.5775\n",
      "Epoch 201/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3253 - soft_acc: 0.7534 - val_loss: 1.3114 - val_soft_acc: 0.5721\n",
      "Epoch 202/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3553 - soft_acc: 0.7355 - val_loss: 1.4107 - val_soft_acc: 0.5627\n",
      "Epoch 203/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3413 - soft_acc: 0.7365 - val_loss: 1.3137 - val_soft_acc: 0.5707\n",
      "Epoch 204/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4003 - soft_acc: 0.7087 - val_loss: 1.4667 - val_soft_acc: 0.5598\n",
      "Epoch 205/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.6659 - soft_acc: 0.6280 - val_loss: 1.7243 - val_soft_acc: 0.4750\n",
      "Epoch 206/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5340 - soft_acc: 0.6538 - val_loss: 1.5307 - val_soft_acc: 0.5352\n",
      "Epoch 207/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3739 - soft_acc: 0.7122 - val_loss: 1.2905 - val_soft_acc: 0.5807\n",
      "Epoch 208/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3000 - soft_acc: 0.7583 - val_loss: 1.3094 - val_soft_acc: 0.5906\n",
      "Epoch 209/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2467 - soft_acc: 0.7884 - val_loss: 1.2126 - val_soft_acc: 0.6143\n",
      "Epoch 210/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2416 - soft_acc: 0.7999 - val_loss: 1.2949 - val_soft_acc: 0.6002\n",
      "Epoch 211/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.2766 - soft_acc: 0.7819 - val_loss: 1.2780 - val_soft_acc: 0.5787\n",
      "Epoch 212/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4463 - soft_acc: 0.6964 - val_loss: 1.6330 - val_soft_acc: 0.5145\n",
      "Epoch 213/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.7893 - soft_acc: 0.5781 - val_loss: 1.6922 - val_soft_acc: 0.4986\n",
      "Epoch 214/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4763 - soft_acc: 0.6686 - val_loss: 1.5061 - val_soft_acc: 0.5422\n",
      "Epoch 215/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3587 - soft_acc: 0.7241 - val_loss: 1.2929 - val_soft_acc: 0.5873\n",
      "Epoch 216/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2543 - soft_acc: 0.7819 - val_loss: 1.4166 - val_soft_acc: 0.5660\n",
      "Epoch 217/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5909 - soft_acc: 0.6547 - val_loss: 1.7010 - val_soft_acc: 0.5061\n",
      "Epoch 218/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5234 - soft_acc: 0.6622 - val_loss: 1.3154 - val_soft_acc: 0.5801\n",
      "Epoch 219/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2654 - soft_acc: 0.7748 - val_loss: 1.3407 - val_soft_acc: 0.5850\n",
      "Epoch 220/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2864 - soft_acc: 0.7718 - val_loss: 1.3881 - val_soft_acc: 0.5734\n",
      "Epoch 221/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2620 - soft_acc: 0.7806 - val_loss: 1.2331 - val_soft_acc: 0.6096\n",
      "Epoch 222/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2671 - soft_acc: 0.8001 - val_loss: 1.2239 - val_soft_acc: 0.6166\n",
      "Epoch 223/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2154 - soft_acc: 0.8217 - val_loss: 1.3032 - val_soft_acc: 0.5980\n",
      "Epoch 224/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2807 - soft_acc: 0.7765 - val_loss: 1.4450 - val_soft_acc: 0.5512\n",
      "Epoch 225/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.3745 - soft_acc: 0.7214 - val_loss: 1.4956 - val_soft_acc: 0.5484\n",
      "Epoch 226/5000\n",
      "14640/14640 [==============================] - 1s 75us/step - loss: 0.5067 - soft_acc: 0.6600 - val_loss: 1.5242 - val_soft_acc: 0.5326\n",
      "Epoch 227/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4544 - soft_acc: 0.6803 - val_loss: 1.4216 - val_soft_acc: 0.5465\n",
      "Epoch 228/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.3592 - soft_acc: 0.72 - 1s 76us/step - loss: 0.3574 - soft_acc: 0.7256 - val_loss: 1.4930 - val_soft_acc: 0.5580\n",
      "Epoch 229/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3397 - soft_acc: 0.7304 - val_loss: 1.5026 - val_soft_acc: 0.5549\n",
      "Epoch 230/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3443 - soft_acc: 0.7347 - val_loss: 1.3213 - val_soft_acc: 0.5904\n",
      "Epoch 231/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2423 - soft_acc: 0.7909 - val_loss: 1.4259 - val_soft_acc: 0.5818\n",
      "Epoch 232/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4395 - soft_acc: 0.7031 - val_loss: 1.3707 - val_soft_acc: 0.5586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3074 - soft_acc: 0.7458 - val_loss: 1.3492 - val_soft_acc: 0.5920\n",
      "Epoch 234/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3101 - soft_acc: 0.7659 - val_loss: 1.4716 - val_soft_acc: 0.5551\n",
      "Epoch 235/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3019 - soft_acc: 0.7600 - val_loss: 1.3214 - val_soft_acc: 0.5867\n",
      "Epoch 236/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2468 - soft_acc: 0.7918 - val_loss: 1.2269 - val_soft_acc: 0.6162\n",
      "Epoch 237/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2546 - soft_acc: 0.7843 - val_loss: 1.2638 - val_soft_acc: 0.6047\n",
      "Epoch 238/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1751 - soft_acc: 0.8313 - val_loss: 1.1995 - val_soft_acc: 0.6199\n",
      "Epoch 239/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.1687 - soft_acc: 0.8463 - val_loss: 1.2696 - val_soft_acc: 0.6053\n",
      "Epoch 240/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2010 - soft_acc: 0.8302 - val_loss: 1.2181 - val_soft_acc: 0.6180\n",
      "Epoch 241/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5429 - soft_acc: 0.6742 - val_loss: 1.7995 - val_soft_acc: 0.4795\n",
      "Epoch 242/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5755 - soft_acc: 0.6344 - val_loss: 1.6336 - val_soft_acc: 0.5227\n",
      "Epoch 243/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4311 - soft_acc: 0.6842 - val_loss: 1.3306 - val_soft_acc: 0.5850\n",
      "Epoch 244/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2447 - soft_acc: 0.7806 - val_loss: 1.2118 - val_soft_acc: 0.6178\n",
      "Epoch 245/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1784 - soft_acc: 0.8302 - val_loss: 1.2732 - val_soft_acc: 0.6150\n",
      "Epoch 246/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2354 - soft_acc: 0.8077 - val_loss: 1.3438 - val_soft_acc: 0.5934\n",
      "Epoch 247/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1976 - soft_acc: 0.8168 - val_loss: 1.2524 - val_soft_acc: 0.6039\n",
      "Epoch 248/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2909 - soft_acc: 0.7719 - val_loss: 1.4047 - val_soft_acc: 0.5553\n",
      "Epoch 249/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2624 - soft_acc: 0.7788 - val_loss: 1.2954 - val_soft_acc: 0.6051\n",
      "Epoch 250/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1923 - soft_acc: 0.8207 - val_loss: 1.1520 - val_soft_acc: 0.6350\n",
      "Epoch 251/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2692 - soft_acc: 0.7969 - val_loss: 1.4272 - val_soft_acc: 0.5645\n",
      "Epoch 252/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3170 - soft_acc: 0.7418 - val_loss: 1.5175 - val_soft_acc: 0.5615\n",
      "Epoch 253/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.2651 - soft_acc: 0.7706 - val_loss: 1.2755 - val_soft_acc: 0.6070\n",
      "Epoch 254/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1906 - soft_acc: 0.8209 - val_loss: 1.3547 - val_soft_acc: 0.5803\n",
      "Epoch 255/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3332 - soft_acc: 0.7594 - val_loss: 1.5126 - val_soft_acc: 0.5459\n",
      "Epoch 256/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5152 - soft_acc: 0.6801 - val_loss: 1.6678 - val_soft_acc: 0.5098\n",
      "Epoch 257/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3711 - soft_acc: 0.7167 - val_loss: 1.3107 - val_soft_acc: 0.5920\n",
      "Epoch 258/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1885 - soft_acc: 0.8154 - val_loss: 1.1703 - val_soft_acc: 0.6371\n",
      "Epoch 259/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1244 - soft_acc: 0.8698 - val_loss: 1.1741 - val_soft_acc: 0.6359\n",
      "Epoch 260/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1290 - soft_acc: 0.8734 - val_loss: 1.1830 - val_soft_acc: 0.6447\n",
      "Epoch 261/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1382 - soft_acc: 0.8750 - val_loss: 1.1229 - val_soft_acc: 0.6404\n",
      "Epoch 262/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2139 - soft_acc: 0.8253 - val_loss: 1.3682 - val_soft_acc: 0.5842\n",
      "Epoch 263/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5545 - soft_acc: 0.6727 - val_loss: 1.7546 - val_soft_acc: 0.4781\n",
      "Epoch 264/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.6478 - soft_acc: 0.6273 - val_loss: 1.6212 - val_soft_acc: 0.5018\n",
      "Epoch 265/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3724 - soft_acc: 0.7171 - val_loss: 1.3395 - val_soft_acc: 0.5984\n",
      "Epoch 266/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2390 - soft_acc: 0.7932 - val_loss: 1.2429 - val_soft_acc: 0.6162\n",
      "Epoch 267/5000\n",
      "14640/14640 [==============================] - 1s 80us/step - loss: 0.1281 - soft_acc: 0.8612 - val_loss: 1.1584 - val_soft_acc: 0.6561\n",
      "Epoch 268/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.0837 - soft_acc: 0.9002 - val_loss: 1.1299 - val_soft_acc: 0.6711\n",
      "Epoch 269/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1305 - soft_acc: 0.8841 - val_loss: 1.1790 - val_soft_acc: 0.6439\n",
      "Epoch 270/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1939 - soft_acc: 0.8427 - val_loss: 1.3152 - val_soft_acc: 0.5822\n",
      "Epoch 271/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2540 - soft_acc: 0.7872 - val_loss: 1.3282 - val_soft_acc: 0.5963\n",
      "Epoch 272/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3858 - soft_acc: 0.7234 - val_loss: 1.8069 - val_soft_acc: 0.4777\n",
      "Epoch 273/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.7021 - soft_acc: 0.6130 - val_loss: 1.4139 - val_soft_acc: 0.5650\n",
      "Epoch 274/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2909 - soft_acc: 0.7535 - val_loss: 1.2921 - val_soft_acc: 0.5988\n",
      "Epoch 275/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1185 - soft_acc: 0.8630 - val_loss: 1.1548 - val_soft_acc: 0.6549\n",
      "Epoch 276/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0750 - soft_acc: 0.9104 - val_loss: 1.1252 - val_soft_acc: 0.6691\n",
      "Epoch 277/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0700 - soft_acc: 0.9201 - val_loss: 1.1195 - val_soft_acc: 0.6574\n",
      "Epoch 278/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0909 - soft_acc: 0.9128 - val_loss: 1.1547 - val_soft_acc: 0.6611\n",
      "Epoch 279/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1879 - soft_acc: 0.8468 - val_loss: 1.2437 - val_soft_acc: 0.6277\n",
      "Epoch 280/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2849 - soft_acc: 0.7742 - val_loss: 1.5294 - val_soft_acc: 0.5613\n",
      "Epoch 281/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5925 - soft_acc: 0.6545 - val_loss: 1.5240 - val_soft_acc: 0.5357\n",
      "Epoch 282/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.3702 - soft_acc: 0.7191 - val_loss: 1.4487 - val_soft_acc: 0.5635\n",
      "Epoch 283/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2205 - soft_acc: 0.7930 - val_loss: 1.1559 - val_soft_acc: 0.6324\n",
      "Epoch 284/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1355 - soft_acc: 0.8600 - val_loss: 1.1521 - val_soft_acc: 0.6400\n",
      "Epoch 285/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1902 - soft_acc: 0.8382 - val_loss: 1.1830 - val_soft_acc: 0.6480\n",
      "Epoch 286/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1290 - soft_acc: 0.8654 - val_loss: 1.1358 - val_soft_acc: 0.6535\n",
      "Epoch 287/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0893 - soft_acc: 0.8994 - val_loss: 1.0805 - val_soft_acc: 0.6693\n",
      "Epoch 288/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0653 - soft_acc: 0.9192 - val_loss: 1.1032 - val_soft_acc: 0.6723\n",
      "Epoch 289/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0624 - soft_acc: 0.9254 - val_loss: 1.0535 - val_soft_acc: 0.6898\n",
      "Epoch 290/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1002 - soft_acc: 0.8990 - val_loss: 1.3829 - val_soft_acc: 0.5941\n",
      "Epoch 291/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.9432 - soft_acc: 0.5579 - val_loss: 1.8597 - val_soft_acc: 0.4619\n",
      "Epoch 292/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4525 - soft_acc: 0.6798 - val_loss: 1.3184 - val_soft_acc: 0.6010\n",
      "Epoch 293/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1891 - soft_acc: 0.8125 - val_loss: 1.2987 - val_soft_acc: 0.6250\n",
      "Epoch 294/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1059 - soft_acc: 0.8796 - val_loss: 1.1156 - val_soft_acc: 0.6639\n",
      "Epoch 295/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0609 - soft_acc: 0.9199 - val_loss: 1.1268 - val_soft_acc: 0.6764\n",
      "Epoch 296/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.0514 - soft_acc: 0.9354 - val_loss: 1.1315 - val_soft_acc: 0.6711\n",
      "Epoch 297/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0654 - soft_acc: 0.9224 - val_loss: 1.1747 - val_soft_acc: 0.6561\n",
      "Epoch 298/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1583 - soft_acc: 0.8791 - val_loss: 1.3408 - val_soft_acc: 0.6043\n",
      "Epoch 299/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6278 - soft_acc: 0.6603 - val_loss: 1.6661 - val_soft_acc: 0.5172\n",
      "Epoch 300/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5459 - soft_acc: 0.6603 - val_loss: 1.4847 - val_soft_acc: 0.5410\n",
      "Epoch 301/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3036 - soft_acc: 0.7420 - val_loss: 1.2655 - val_soft_acc: 0.6121\n",
      "Epoch 302/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1261 - soft_acc: 0.8542 - val_loss: 1.1384 - val_soft_acc: 0.6463\n",
      "Epoch 303/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0638 - soft_acc: 0.9121 - val_loss: 1.0778 - val_soft_acc: 0.6768\n",
      "Epoch 304/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0500 - soft_acc: 0.9352 - val_loss: 1.1091 - val_soft_acc: 0.6691\n",
      "Epoch 305/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.1319 - soft_acc: 0.88 - 1s 76us/step - loss: 0.1324 - soft_acc: 0.8890 - val_loss: 1.2842 - val_soft_acc: 0.6357\n",
      "Epoch 306/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2712 - soft_acc: 0.8004 - val_loss: 1.5426 - val_soft_acc: 0.5277\n",
      "Epoch 307/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5612 - soft_acc: 0.6588 - val_loss: 1.5935 - val_soft_acc: 0.5217\n",
      "Epoch 308/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2727 - soft_acc: 0.7650 - val_loss: 1.4766 - val_soft_acc: 0.5803\n",
      "Epoch 309/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1843 - soft_acc: 0.8219 - val_loss: 1.2662 - val_soft_acc: 0.6443\n",
      "Epoch 310/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0814 - soft_acc: 0.8974 - val_loss: 1.1620 - val_soft_acc: 0.6715\n",
      "Epoch 311/5000\n",
      "14640/14640 [==============================] - 1s 81us/step - loss: 0.1119 - soft_acc: 0.8929 - val_loss: 1.1644 - val_soft_acc: 0.6518\n",
      "Epoch 312/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.1439 - soft_acc: 0.8788 - val_loss: 1.2391 - val_soft_acc: 0.6189\n",
      "Epoch 313/5000\n",
      "14640/14640 [==============================] - 1s 81us/step - loss: 0.2077 - soft_acc: 0.8295 - val_loss: 1.3803 - val_soft_acc: 0.5727\n",
      "Epoch 314/5000\n",
      "14640/14640 [==============================] - 1s 81us/step - loss: 0.2630 - soft_acc: 0.7796 - val_loss: 1.3036 - val_soft_acc: 0.6094\n",
      "Epoch 315/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.2279 - soft_acc: 0.8098 - val_loss: 1.2744 - val_soft_acc: 0.6150\n",
      "Epoch 316/5000\n",
      "14640/14640 [==============================] - 1s 80us/step - loss: 0.1549 - soft_acc: 0.8447 - val_loss: 1.1854 - val_soft_acc: 0.6475\n",
      "Epoch 317/5000\n",
      "14640/14640 [==============================] - 1s 80us/step - loss: 0.1781 - soft_acc: 0.8408 - val_loss: 1.3667 - val_soft_acc: 0.5844\n",
      "Epoch 318/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.2649 - soft_acc: 0.7796 - val_loss: 1.4094 - val_soft_acc: 0.5719\n",
      "Epoch 319/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.2422 - soft_acc: 0.7859 - val_loss: 1.3063 - val_soft_acc: 0.6129\n",
      "Epoch 320/5000\n",
      "14640/14640 [==============================] - 1s 82us/step - loss: 0.1134 - soft_acc: 0.8710 - val_loss: 1.1002 - val_soft_acc: 0.6662\n",
      "Epoch 321/5000\n",
      "14640/14640 [==============================] - 1s 83us/step - loss: 0.0456 - soft_acc: 0.9321 - val_loss: 1.0968 - val_soft_acc: 0.6904\n",
      "Epoch 322/5000\n",
      "14640/14640 [==============================] - 1s 82us/step - loss: 0.0406 - soft_acc: 0.9445 - val_loss: 1.1126 - val_soft_acc: 0.6867\n",
      "Epoch 323/5000\n",
      "14640/14640 [==============================] - 1s 80us/step - loss: 0.0722 - soft_acc: 0.9234 - val_loss: 1.1656 - val_soft_acc: 0.6596\n",
      "Epoch 324/5000\n",
      "14640/14640 [==============================] - 1s 82us/step - loss: 0.1405 - soft_acc: 0.8868 - val_loss: 1.3289 - val_soft_acc: 0.6053\n",
      "Epoch 325/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.3884 - soft_acc: 0.7277 - val_loss: 1.5711 - val_soft_acc: 0.5459\n",
      "Epoch 326/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5885 - soft_acc: 0.6481 - val_loss: 1.4919 - val_soft_acc: 0.5482\n",
      "Epoch 327/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2812 - soft_acc: 0.7610 - val_loss: 1.2829 - val_soft_acc: 0.6248\n",
      "Epoch 328/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0950 - soft_acc: 0.8748 - val_loss: 1.1583 - val_soft_acc: 0.6582\n",
      "Epoch 329/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0437 - soft_acc: 0.9292 - val_loss: 1.1120 - val_soft_acc: 0.6832\n",
      "Epoch 330/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0551 - soft_acc: 0.9345 - val_loss: 1.1265 - val_soft_acc: 0.6861\n",
      "Epoch 331/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0460 - soft_acc: 0.9403 - val_loss: 1.1850 - val_soft_acc: 0.6633\n",
      "Epoch 332/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1806 - soft_acc: 0.8496 - val_loss: 1.5522 - val_soft_acc: 0.5762\n",
      "Epoch 333/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3847 - soft_acc: 0.7325 - val_loss: 1.8089 - val_soft_acc: 0.4836\n",
      "Epoch 334/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5455 - soft_acc: 0.6634 - val_loss: 1.7086 - val_soft_acc: 0.5246\n",
      "Epoch 335/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2601 - soft_acc: 0.7765 - val_loss: 1.2286 - val_soft_acc: 0.6432\n",
      "Epoch 336/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0892 - soft_acc: 0.8902 - val_loss: 1.1324 - val_soft_acc: 0.6758\n",
      "Epoch 337/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0440 - soft_acc: 0.9331 - val_loss: 1.1084 - val_soft_acc: 0.6949\n",
      "Epoch 338/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.0264 - soft_acc: 0.9536 - val_loss: 1.1256 - val_soft_acc: 0.6977\n",
      "Epoch 339/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0333 - soft_acc: 0.9516 - val_loss: 1.0862 - val_soft_acc: 0.6959\n",
      "Epoch 00339: early stopping\n",
      ">0.696\n",
      "Train on 14640 samples, validate on 2440 samples\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 2s 114us/step - loss: 4.7947 - soft_acc: 0.1585 - val_loss: 4.1404 - val_soft_acc: 0.1643\n",
      "Epoch 2/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 4.2290 - soft_acc: 0.1585 - val_loss: 3.9575 - val_soft_acc: 0.1650\n",
      "Epoch 3/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 4.0963 - soft_acc: 0.1631 - val_loss: 3.8816 - val_soft_acc: 0.1631\n",
      "Epoch 4/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 4.0082 - soft_acc: 0.1673 - val_loss: 3.9285 - val_soft_acc: 0.1721\n",
      "Epoch 5/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 4.0306 - soft_acc: 0.1728 - val_loss: 3.9150 - val_soft_acc: 0.1586\n",
      "Epoch 6/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.9066 - soft_acc: 0.1713 - val_loss: 3.8034 - val_soft_acc: 0.1584\n",
      "Epoch 7/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.8660 - soft_acc: 0.1772 - val_loss: 3.7344 - val_soft_acc: 0.1857\n",
      "Epoch 8/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.8808 - soft_acc: 0.1740 - val_loss: 3.7536 - val_soft_acc: 0.1799\n",
      "Epoch 9/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.8057 - soft_acc: 0.1804 - val_loss: 3.7003 - val_soft_acc: 0.1666\n",
      "Epoch 10/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.7781 - soft_acc: 0.1796 - val_loss: 3.6584 - val_soft_acc: 0.1664\n",
      "Epoch 11/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.7953 - soft_acc: 0.1798 - val_loss: 3.7114 - val_soft_acc: 0.1703\n",
      "Epoch 12/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.7411 - soft_acc: 0.1792 - val_loss: 3.6931 - val_soft_acc: 0.1885\n",
      "Epoch 13/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 3.7029 - soft_acc: 0.1805 - val_loss: 3.7308 - val_soft_acc: 0.1764\n",
      "Epoch 14/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.7157 - soft_acc: 0.1808 - val_loss: 3.5858 - val_soft_acc: 0.1926\n",
      "Epoch 15/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.7060 - soft_acc: 0.1855 - val_loss: 3.5786 - val_soft_acc: 0.2014\n",
      "Epoch 16/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.6491 - soft_acc: 0.1855 - val_loss: 3.4864 - val_soft_acc: 0.2059\n",
      "Epoch 17/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.6297 - soft_acc: 0.1870 - val_loss: 3.4997 - val_soft_acc: 0.2109\n",
      "Epoch 18/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 3.6010 - soft_acc: 0.1884 - val_loss: 3.6447 - val_soft_acc: 0.2027\n",
      "Epoch 19/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.5667 - soft_acc: 0.1909 - val_loss: 3.5218 - val_soft_acc: 0.1920\n",
      "Epoch 20/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 3.5569 - soft_acc: 0.1909 - val_loss: 3.5336 - val_soft_acc: 0.1895\n",
      "Epoch 21/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.5471 - soft_acc: 0.1912 - val_loss: 3.4086 - val_soft_acc: 0.1955\n",
      "Epoch 22/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.5119 - soft_acc: 0.1944 - val_loss: 3.6009 - val_soft_acc: 0.1957\n",
      "Epoch 23/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.4851 - soft_acc: 0.1964 - val_loss: 3.3733 - val_soft_acc: 0.1912\n",
      "Epoch 24/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.4740 - soft_acc: 0.1957 - val_loss: 3.4569 - val_soft_acc: 0.2131\n",
      "Epoch 25/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.4427 - soft_acc: 0.1966 - val_loss: 3.3700 - val_soft_acc: 0.1998\n",
      "Epoch 26/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 3.4020 - soft_acc: 0.1991 - val_loss: 3.3545 - val_soft_acc: 0.2178\n",
      "Epoch 27/5000\n",
      "14640/14640 [==============================] - 1s 80us/step - loss: 3.3855 - soft_acc: 0.2022 - val_loss: 3.4039 - val_soft_acc: 0.2115\n",
      "Epoch 28/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 3.3870 - soft_acc: 0.20 - 1s 78us/step - loss: 3.3913 - soft_acc: 0.2036 - val_loss: 3.5791 - val_soft_acc: 0.2170\n",
      "Epoch 29/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.4015 - soft_acc: 0.1983 - val_loss: 3.2803 - val_soft_acc: 0.2018\n",
      "Epoch 30/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.3575 - soft_acc: 0.2046 - val_loss: 3.2886 - val_soft_acc: 0.2057\n",
      "Epoch 31/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.3412 - soft_acc: 0.2040 - val_loss: 3.4088 - val_soft_acc: 0.2195\n",
      "Epoch 32/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.3326 - soft_acc: 0.2100 - val_loss: 3.4357 - val_soft_acc: 0.2039\n",
      "Epoch 33/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 3.2902 - soft_acc: 0.2087 - val_loss: 3.2556 - val_soft_acc: 0.2154\n",
      "Epoch 34/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 3.2864 - soft_acc: 0.2064 - val_loss: 3.4289 - val_soft_acc: 0.1842\n",
      "Epoch 35/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.2619 - soft_acc: 0.2121 - val_loss: 3.2748 - val_soft_acc: 0.2184\n",
      "Epoch 36/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.2452 - soft_acc: 0.2122 - val_loss: 3.2104 - val_soft_acc: 0.2006\n",
      "Epoch 37/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.2206 - soft_acc: 0.2111 - val_loss: 3.2643 - val_soft_acc: 0.2156\n",
      "Epoch 38/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.2073 - soft_acc: 0.2156 - val_loss: 3.1669 - val_soft_acc: 0.2320\n",
      "Epoch 39/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.1903 - soft_acc: 0.2176 - val_loss: 3.1371 - val_soft_acc: 0.2164\n",
      "Epoch 40/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.1591 - soft_acc: 0.2185 - val_loss: 3.1466 - val_soft_acc: 0.2320\n",
      "Epoch 41/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 3.1746 - soft_acc: 0.2157 - val_loss: 3.1628 - val_soft_acc: 0.2174\n",
      "Epoch 42/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 3.1350 - soft_acc: 0.2235 - val_loss: 3.2105 - val_soft_acc: 0.2291\n",
      "Epoch 43/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.0897 - soft_acc: 0.2242 - val_loss: 3.1031 - val_soft_acc: 0.2250\n",
      "Epoch 44/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.1188 - soft_acc: 0.2203 - val_loss: 3.0635 - val_soft_acc: 0.2221\n",
      "Epoch 45/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.0849 - soft_acc: 0.2252 - val_loss: 3.1174 - val_soft_acc: 0.2148\n",
      "Epoch 46/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.0533 - soft_acc: 0.2276 - val_loss: 3.0789 - val_soft_acc: 0.2434\n",
      "Epoch 47/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.0662 - soft_acc: 0.2272 - val_loss: 3.1109 - val_soft_acc: 0.2139\n",
      "Epoch 48/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.0009 - soft_acc: 0.2313 - val_loss: 3.0361 - val_soft_acc: 0.2275\n",
      "Epoch 49/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.0022 - soft_acc: 0.2305 - val_loss: 3.2462 - val_soft_acc: 0.2182\n",
      "Epoch 50/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 2.9402 - soft_acc: 0.23 - 1s 77us/step - loss: 2.9509 - soft_acc: 0.2339 - val_loss: 3.0643 - val_soft_acc: 0.2322\n",
      "Epoch 51/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.9537 - soft_acc: 0.2363 - val_loss: 2.9582 - val_soft_acc: 0.2420\n",
      "Epoch 52/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.9241 - soft_acc: 0.2373 - val_loss: 3.0155 - val_soft_acc: 0.2215\n",
      "Epoch 53/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.9203 - soft_acc: 0.2347 - val_loss: 2.9318 - val_soft_acc: 0.2369\n",
      "Epoch 54/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.8771 - soft_acc: 0.2413 - val_loss: 2.9273 - val_soft_acc: 0.2445\n",
      "Epoch 55/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 79us/step - loss: 2.8500 - soft_acc: 0.2438 - val_loss: 2.9843 - val_soft_acc: 0.2461\n",
      "Epoch 56/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.8382 - soft_acc: 0.2471 - val_loss: 2.9807 - val_soft_acc: 0.2424\n",
      "Epoch 57/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.8129 - soft_acc: 0.2485 - val_loss: 2.8236 - val_soft_acc: 0.2516\n",
      "Epoch 58/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 2.7979 - soft_acc: 0.25 - 1s 77us/step - loss: 2.7906 - soft_acc: 0.2498 - val_loss: 2.8909 - val_soft_acc: 0.2402\n",
      "Epoch 59/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.7731 - soft_acc: 0.2508 - val_loss: 2.9114 - val_soft_acc: 0.2564\n",
      "Epoch 60/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.7555 - soft_acc: 0.2545 - val_loss: 2.9766 - val_soft_acc: 0.2459\n",
      "Epoch 61/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.7290 - soft_acc: 0.2567 - val_loss: 2.8202 - val_soft_acc: 0.2572\n",
      "Epoch 62/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.7069 - soft_acc: 0.2592 - val_loss: 2.8311 - val_soft_acc: 0.2639\n",
      "Epoch 63/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.7031 - soft_acc: 0.2591 - val_loss: 2.8258 - val_soft_acc: 0.2340\n",
      "Epoch 64/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.6613 - soft_acc: 0.2651 - val_loss: 2.6993 - val_soft_acc: 0.2611\n",
      "Epoch 65/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.5952 - soft_acc: 0.2704 - val_loss: 2.7415 - val_soft_acc: 0.2629\n",
      "Epoch 66/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 2.6734 - soft_acc: 0.26 - 1s 77us/step - loss: 2.6823 - soft_acc: 0.2612 - val_loss: 2.7441 - val_soft_acc: 0.2643\n",
      "Epoch 67/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.5867 - soft_acc: 0.2701 - val_loss: 2.7190 - val_soft_acc: 0.2650\n",
      "Epoch 68/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.5382 - soft_acc: 0.2782 - val_loss: 2.6841 - val_soft_acc: 0.2723\n",
      "Epoch 69/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 2.5066 - soft_acc: 0.2802 - val_loss: 2.7190 - val_soft_acc: 0.2684\n",
      "Epoch 70/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.5265 - soft_acc: 0.2787 - val_loss: 2.6844 - val_soft_acc: 0.2781\n",
      "Epoch 71/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.4747 - soft_acc: 0.2787 - val_loss: 2.6198 - val_soft_acc: 0.2719\n",
      "Epoch 72/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.5243 - soft_acc: 0.2753 - val_loss: 2.5579 - val_soft_acc: 0.2873\n",
      "Epoch 73/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.4306 - soft_acc: 0.2903 - val_loss: 2.6079 - val_soft_acc: 0.2885\n",
      "Epoch 74/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.3651 - soft_acc: 0.2943 - val_loss: 2.5538 - val_soft_acc: 0.2846\n",
      "Epoch 75/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.3711 - soft_acc: 0.2954 - val_loss: 2.5532 - val_soft_acc: 0.2924\n",
      "Epoch 76/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.3428 - soft_acc: 0.2967 - val_loss: 2.5035 - val_soft_acc: 0.2982\n",
      "Epoch 77/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.3070 - soft_acc: 0.3015 - val_loss: 2.4862 - val_soft_acc: 0.2998\n",
      "Epoch 78/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 2.2554 - soft_acc: 0.30 - 1s 77us/step - loss: 2.2592 - soft_acc: 0.3073 - val_loss: 2.5083 - val_soft_acc: 0.2867\n",
      "Epoch 79/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 2.2238 - soft_acc: 0.30 - 1s 77us/step - loss: 2.2315 - soft_acc: 0.3074 - val_loss: 2.4543 - val_soft_acc: 0.3061\n",
      "Epoch 80/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.1955 - soft_acc: 0.3107 - val_loss: 2.4828 - val_soft_acc: 0.3018\n",
      "Epoch 81/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.1894 - soft_acc: 0.3074 - val_loss: 2.3916 - val_soft_acc: 0.3131\n",
      "Epoch 82/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.1436 - soft_acc: 0.3202 - val_loss: 2.4038 - val_soft_acc: 0.3037\n",
      "Epoch 83/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 2.1489 - soft_acc: 0.3210 - val_loss: 2.3591 - val_soft_acc: 0.2996\n",
      "Epoch 84/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 2.0902 - soft_acc: 0.3278 - val_loss: 2.3438 - val_soft_acc: 0.3096\n",
      "Epoch 85/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.0741 - soft_acc: 0.3348 - val_loss: 2.3351 - val_soft_acc: 0.2986\n",
      "Epoch 86/5000\n",
      "14640/14640 [==============================] - 1s 80us/step - loss: 2.0441 - soft_acc: 0.3313 - val_loss: 2.4080 - val_soft_acc: 0.3125\n",
      "Epoch 87/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.0096 - soft_acc: 0.3355 - val_loss: 2.2715 - val_soft_acc: 0.3180\n",
      "Epoch 88/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.9547 - soft_acc: 0.3421 - val_loss: 2.3951 - val_soft_acc: 0.3119\n",
      "Epoch 89/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.9358 - soft_acc: 0.3492 - val_loss: 2.3005 - val_soft_acc: 0.3174\n",
      "Epoch 90/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.8808 - soft_acc: 0.3554 - val_loss: 2.1849 - val_soft_acc: 0.3424\n",
      "Epoch 91/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.8541 - soft_acc: 0.3587 - val_loss: 2.2630 - val_soft_acc: 0.3219\n",
      "Epoch 92/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.8301 - soft_acc: 0.3585 - val_loss: 2.3048 - val_soft_acc: 0.3285\n",
      "Epoch 93/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.8717 - soft_acc: 0.3527 - val_loss: 2.2580 - val_soft_acc: 0.3328\n",
      "Epoch 94/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.7756 - soft_acc: 0.3651 - val_loss: 2.2867 - val_soft_acc: 0.3334\n",
      "Epoch 95/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.8285 - soft_acc: 0.3573 - val_loss: 2.2007 - val_soft_acc: 0.3279\n",
      "Epoch 96/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.7491 - soft_acc: 0.3692 - val_loss: 2.1004 - val_soft_acc: 0.3328\n",
      "Epoch 97/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.6651 - soft_acc: 0.3843 - val_loss: 2.2253 - val_soft_acc: 0.3346\n",
      "Epoch 98/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 1.6824 - soft_acc: 0.3782 - val_loss: 2.1152 - val_soft_acc: 0.3391\n",
      "Epoch 99/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.6199 - soft_acc: 0.3894 - val_loss: 2.1060 - val_soft_acc: 0.3576\n",
      "Epoch 100/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.5758 - soft_acc: 0.3998 - val_loss: 2.0454 - val_soft_acc: 0.3588\n",
      "Epoch 101/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.6070 - soft_acc: 0.3912 - val_loss: 2.0743 - val_soft_acc: 0.3607\n",
      "Epoch 102/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.5801 - soft_acc: 0.3997 - val_loss: 2.0926 - val_soft_acc: 0.3564\n",
      "Epoch 103/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.5018 - soft_acc: 0.4093 - val_loss: 1.9852 - val_soft_acc: 0.3701\n",
      "Epoch 104/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.4974 - soft_acc: 0.4078 - val_loss: 2.0952 - val_soft_acc: 0.3764\n",
      "Epoch 105/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.4263 - soft_acc: 0.4226 - val_loss: 1.9999 - val_soft_acc: 0.3631\n",
      "Epoch 106/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 1.4371 - soft_acc: 0.41 - 1s 76us/step - loss: 1.4399 - soft_acc: 0.4175 - val_loss: 2.0368 - val_soft_acc: 0.3705\n",
      "Epoch 107/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.4111 - soft_acc: 0.4225 - val_loss: 2.1399 - val_soft_acc: 0.3518\n",
      "Epoch 108/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.4889 - soft_acc: 0.4153 - val_loss: 1.9768 - val_soft_acc: 0.3678\n",
      "Epoch 109/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.4835 - soft_acc: 0.4192 - val_loss: 1.9678 - val_soft_acc: 0.3805\n",
      "Epoch 110/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.3468 - soft_acc: 0.4398 - val_loss: 2.0243 - val_soft_acc: 0.3773\n",
      "Epoch 111/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.3209 - soft_acc: 0.4435 - val_loss: 1.9211 - val_soft_acc: 0.3857\n",
      "Epoch 112/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 1.2963 - soft_acc: 0.4485 - val_loss: 1.9235 - val_soft_acc: 0.3861\n",
      "Epoch 113/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.2318 - soft_acc: 0.4574 - val_loss: 1.9474 - val_soft_acc: 0.4029\n",
      "Epoch 114/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.2840 - soft_acc: 0.4455 - val_loss: 1.8851 - val_soft_acc: 0.4008\n",
      "Epoch 115/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.2230 - soft_acc: 0.4659 - val_loss: 1.9092 - val_soft_acc: 0.3969\n",
      "Epoch 116/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.1789 - soft_acc: 0.4693 - val_loss: 1.8865 - val_soft_acc: 0.4123\n",
      "Epoch 117/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.2113 - soft_acc: 0.4683 - val_loss: 1.8781 - val_soft_acc: 0.4014\n",
      "Epoch 118/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.2250 - soft_acc: 0.4643 - val_loss: 1.8917 - val_soft_acc: 0.4131\n",
      "Epoch 119/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.1188 - soft_acc: 0.4916 - val_loss: 1.7375 - val_soft_acc: 0.4217\n",
      "Epoch 120/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.0789 - soft_acc: 0.4964 - val_loss: 1.8086 - val_soft_acc: 0.4215\n",
      "Epoch 121/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.1349 - soft_acc: 0.4812 - val_loss: 1.8720 - val_soft_acc: 0.3996\n",
      "Epoch 122/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.1346 - soft_acc: 0.4856 - val_loss: 1.9553 - val_soft_acc: 0.3961\n",
      "Epoch 123/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.0753 - soft_acc: 0.4950 - val_loss: 1.8004 - val_soft_acc: 0.4191\n",
      "Epoch 124/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 1.1411 - soft_acc: 0.4834 - val_loss: 1.9294 - val_soft_acc: 0.4051\n",
      "Epoch 125/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.0478 - soft_acc: 0.4976 - val_loss: 1.7985 - val_soft_acc: 0.4164\n",
      "Epoch 126/5000\n",
      "14640/14640 [==============================] - 1s 80us/step - loss: 0.9935 - soft_acc: 0.5167 - val_loss: 1.6742 - val_soft_acc: 0.4494\n",
      "Epoch 127/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.9924 - soft_acc: 0.5172 - val_loss: 1.7490 - val_soft_acc: 0.4432\n",
      "Epoch 128/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.9634 - soft_acc: 0.5206 - val_loss: 1.8285 - val_soft_acc: 0.4330\n",
      "Epoch 129/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.0178 - soft_acc: 0.5110 - val_loss: 1.7226 - val_soft_acc: 0.4334\n",
      "Epoch 130/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.0387 - soft_acc: 0.5046 - val_loss: 1.8128 - val_soft_acc: 0.4234\n",
      "Epoch 131/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.9315 - soft_acc: 0.5240 - val_loss: 1.6552 - val_soft_acc: 0.4432\n",
      "Epoch 132/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.9112 - soft_acc: 0.5302 - val_loss: 1.7042 - val_soft_acc: 0.4451\n",
      "Epoch 133/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.9144 - soft_acc: 0.5353 - val_loss: 1.7205 - val_soft_acc: 0.4428\n",
      "Epoch 134/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.9102 - soft_acc: 0.5391 - val_loss: 1.7867 - val_soft_acc: 0.4252\n",
      "Epoch 135/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.9138 - soft_acc: 0.5364 - val_loss: 1.7544 - val_soft_acc: 0.4297\n",
      "Epoch 136/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.9719 - soft_acc: 0.5179 - val_loss: 1.7441 - val_soft_acc: 0.4465\n",
      "Epoch 137/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.8903 - soft_acc: 0.5464 - val_loss: 1.6403 - val_soft_acc: 0.4709\n",
      "Epoch 138/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.8283 - soft_acc: 0.5552 - val_loss: 1.6285 - val_soft_acc: 0.4682\n",
      "Epoch 139/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.8198 - soft_acc: 0.5622 - val_loss: 1.6236 - val_soft_acc: 0.4602\n",
      "Epoch 140/5000\n",
      "14640/14640 [==============================] - 1s 81us/step - loss: 0.7518 - soft_acc: 0.5792 - val_loss: 1.7206 - val_soft_acc: 0.4422\n",
      "Epoch 141/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.8406 - soft_acc: 0.5564 - val_loss: 1.7652 - val_soft_acc: 0.4457\n",
      "Epoch 142/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.8566 - soft_acc: 0.5492 - val_loss: 1.7217 - val_soft_acc: 0.4453\n",
      "Epoch 143/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.8299 - soft_acc: 0.5597 - val_loss: 1.6216 - val_soft_acc: 0.4605\n",
      "Epoch 144/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8680 - soft_acc: 0.5484 - val_loss: 1.6938 - val_soft_acc: 0.4422\n",
      "Epoch 145/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.7573 - soft_acc: 0.5740 - val_loss: 1.6384 - val_soft_acc: 0.4699\n",
      "Epoch 146/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.7258 - soft_acc: 0.5864 - val_loss: 1.5957 - val_soft_acc: 0.4775\n",
      "Epoch 147/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.7461 - soft_acc: 0.5891 - val_loss: 1.5939 - val_soft_acc: 0.4777\n",
      "Epoch 148/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.7248 - soft_acc: 0.5820 - val_loss: 1.5650 - val_soft_acc: 0.4717\n",
      "Epoch 149/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.7216 - soft_acc: 0.5927 - val_loss: 1.8153 - val_soft_acc: 0.4502\n",
      "Epoch 150/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.7122 - soft_acc: 0.5873 - val_loss: 1.5151 - val_soft_acc: 0.4996\n",
      "Epoch 151/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.8082 - soft_acc: 0.5668 - val_loss: 1.6224 - val_soft_acc: 0.4789\n",
      "Epoch 152/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.6224 - soft_acc: 0.6183 - val_loss: 1.6900 - val_soft_acc: 0.4799\n",
      "Epoch 153/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6551 - soft_acc: 0.6046 - val_loss: 1.6238 - val_soft_acc: 0.4674\n",
      "Epoch 154/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.7388 - soft_acc: 0.5932 - val_loss: 1.7369 - val_soft_acc: 0.4668\n",
      "Epoch 155/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.7430 - soft_acc: 0.5794 - val_loss: 1.6432 - val_soft_acc: 0.4742\n",
      "Epoch 156/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.7491 - soft_acc: 0.5794 - val_loss: 1.7310 - val_soft_acc: 0.4654\n",
      "Epoch 157/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6862 - soft_acc: 0.5928 - val_loss: 1.5376 - val_soft_acc: 0.4809\n",
      "Epoch 158/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.6939 - soft_acc: 0.6027 - val_loss: 1.5884 - val_soft_acc: 0.5094\n",
      "Epoch 159/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.6550 - soft_acc: 0.6105 - val_loss: 1.7352 - val_soft_acc: 0.4727\n",
      "Epoch 160/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.6742 - soft_acc: 0.6050 - val_loss: 1.4290 - val_soft_acc: 0.5201\n",
      "Epoch 161/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5954 - soft_acc: 0.6324 - val_loss: 1.4679 - val_soft_acc: 0.5092\n",
      "Epoch 162/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6262 - soft_acc: 0.6247 - val_loss: 1.6476 - val_soft_acc: 0.4939\n",
      "Epoch 163/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6362 - soft_acc: 0.6173 - val_loss: 1.5253 - val_soft_acc: 0.5041\n",
      "Epoch 164/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5452 - soft_acc: 0.6517 - val_loss: 1.4628 - val_soft_acc: 0.5283\n",
      "Epoch 165/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6843 - soft_acc: 0.6084 - val_loss: 1.5807 - val_soft_acc: 0.4932\n",
      "Epoch 166/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6254 - soft_acc: 0.6207 - val_loss: 1.5488 - val_soft_acc: 0.5127\n",
      "Epoch 167/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.6011 - soft_acc: 0.6302 - val_loss: 1.4984 - val_soft_acc: 0.5068\n",
      "Epoch 168/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4865 - soft_acc: 0.6721 - val_loss: 1.4358 - val_soft_acc: 0.5264\n",
      "Epoch 169/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.6366 - soft_acc: 0.6191 - val_loss: 1.4743 - val_soft_acc: 0.5096\n",
      "Epoch 170/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4904 - soft_acc: 0.6708 - val_loss: 1.4130 - val_soft_acc: 0.5457\n",
      "Epoch 171/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5110 - soft_acc: 0.6705 - val_loss: 1.5788 - val_soft_acc: 0.5197\n",
      "Epoch 172/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.6107 - soft_acc: 0.6278 - val_loss: 1.8569 - val_soft_acc: 0.4652\n",
      "Epoch 173/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.7604 - soft_acc: 0.5807 - val_loss: 1.7602 - val_soft_acc: 0.4572\n",
      "Epoch 174/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.6566 - soft_acc: 0.6173 - val_loss: 1.7355 - val_soft_acc: 0.4750\n",
      "Epoch 175/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5632 - soft_acc: 0.6397 - val_loss: 1.5881 - val_soft_acc: 0.5043\n",
      "Epoch 176/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4951 - soft_acc: 0.6654 - val_loss: 1.4524 - val_soft_acc: 0.5154\n",
      "Epoch 177/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4935 - soft_acc: 0.6744 - val_loss: 1.4147 - val_soft_acc: 0.5389\n",
      "Epoch 178/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4097 - soft_acc: 0.7078 - val_loss: 1.4830 - val_soft_acc: 0.5334\n",
      "Epoch 179/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4738 - soft_acc: 0.6846 - val_loss: 1.4714 - val_soft_acc: 0.5381\n",
      "Epoch 180/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4946 - soft_acc: 0.6655 - val_loss: 1.5741 - val_soft_acc: 0.5072\n",
      "Epoch 181/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5430 - soft_acc: 0.6512 - val_loss: 1.5494 - val_soft_acc: 0.5160\n",
      "Epoch 182/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5257 - soft_acc: 0.6647 - val_loss: 1.5386 - val_soft_acc: 0.5156\n",
      "Epoch 183/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.5077 - soft_acc: 0.6692 - val_loss: 1.6003 - val_soft_acc: 0.5209\n",
      "Epoch 184/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5663 - soft_acc: 0.6462 - val_loss: 1.5671 - val_soft_acc: 0.5090\n",
      "Epoch 185/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5137 - soft_acc: 0.6616 - val_loss: 1.5668 - val_soft_acc: 0.5176\n",
      "Epoch 186/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4590 - soft_acc: 0.6853 - val_loss: 1.4687 - val_soft_acc: 0.5391\n",
      "Epoch 187/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4943 - soft_acc: 0.6716 - val_loss: 1.4755 - val_soft_acc: 0.5469\n",
      "Epoch 188/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5735 - soft_acc: 0.6580 - val_loss: 1.3870 - val_soft_acc: 0.5490\n",
      "Epoch 189/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4668 - soft_acc: 0.6795 - val_loss: 1.3337 - val_soft_acc: 0.5539\n",
      "Epoch 190/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.3507 - soft_acc: 0.73 - 1s 77us/step - loss: 0.3537 - soft_acc: 0.7380 - val_loss: 1.3931 - val_soft_acc: 0.5410\n",
      "Epoch 191/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3762 - soft_acc: 0.7285 - val_loss: 1.5763 - val_soft_acc: 0.4977\n",
      "Epoch 192/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.3520 - soft_acc: 0.7347 - val_loss: 1.2861 - val_soft_acc: 0.5736\n",
      "Epoch 193/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.3774 - soft_acc: 0.7348 - val_loss: 1.5683 - val_soft_acc: 0.5000\n",
      "Epoch 194/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.6693 - soft_acc: 0.6112 - val_loss: 1.5160 - val_soft_acc: 0.5234\n",
      "Epoch 195/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.6038 - soft_acc: 0.63 - 1s 77us/step - loss: 0.6041 - soft_acc: 0.6309 - val_loss: 1.6623 - val_soft_acc: 0.5086\n",
      "Epoch 196/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4996 - soft_acc: 0.6627 - val_loss: 1.5549 - val_soft_acc: 0.5328\n",
      "Epoch 197/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.3733 - soft_acc: 0.7201 - val_loss: 1.4000 - val_soft_acc: 0.5514\n",
      "Epoch 198/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3716 - soft_acc: 0.7343 - val_loss: 1.3436 - val_soft_acc: 0.5590\n",
      "Epoch 199/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4577 - soft_acc: 0.6966 - val_loss: 1.3882 - val_soft_acc: 0.5557\n",
      "Epoch 200/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3901 - soft_acc: 0.7211 - val_loss: 1.4812 - val_soft_acc: 0.5434\n",
      "Epoch 201/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3616 - soft_acc: 0.7343 - val_loss: 1.3227 - val_soft_acc: 0.5730\n",
      "Epoch 202/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.7018 - soft_acc: 0.6241 - val_loss: 1.5547 - val_soft_acc: 0.5213\n",
      "Epoch 203/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.7049 - soft_acc: 0.6061 - val_loss: 1.5138 - val_soft_acc: 0.5283\n",
      "Epoch 204/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3727 - soft_acc: 0.7222 - val_loss: 1.3756 - val_soft_acc: 0.5590\n",
      "Epoch 205/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.3168 - soft_acc: 0.7580 - val_loss: 1.3042 - val_soft_acc: 0.5916\n",
      "Epoch 206/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3216 - soft_acc: 0.7671 - val_loss: 1.4581 - val_soft_acc: 0.5520\n",
      "Epoch 207/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3398 - soft_acc: 0.7468 - val_loss: 1.4327 - val_soft_acc: 0.5699\n",
      "Epoch 208/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4102 - soft_acc: 0.7165 - val_loss: 1.3687 - val_soft_acc: 0.5582\n",
      "Epoch 209/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.3302 - soft_acc: 0.74 - 1s 77us/step - loss: 0.3300 - soft_acc: 0.7489 - val_loss: 1.3194 - val_soft_acc: 0.5801\n",
      "Epoch 210/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3803 - soft_acc: 0.7273 - val_loss: 1.4419 - val_soft_acc: 0.5541\n",
      "Epoch 211/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.4998 - soft_acc: 0.6791 - val_loss: 1.7147 - val_soft_acc: 0.4742\n",
      "Epoch 212/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.5420 - soft_acc: 0.6588 - val_loss: 1.3845 - val_soft_acc: 0.5520\n",
      "Epoch 213/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3419 - soft_acc: 0.7324 - val_loss: 1.4748 - val_soft_acc: 0.5574\n",
      "Epoch 214/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4352 - soft_acc: 0.7048 - val_loss: 1.3997 - val_soft_acc: 0.5857\n",
      "Epoch 215/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3642 - soft_acc: 0.7321 - val_loss: 1.4367 - val_soft_acc: 0.5844\n",
      "Epoch 216/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2860 - soft_acc: 0.7749 - val_loss: 1.4040 - val_soft_acc: 0.5682\n",
      "Epoch 217/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2781 - soft_acc: 0.7760 - val_loss: 1.2973 - val_soft_acc: 0.5971\n",
      "Epoch 218/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.2907 - soft_acc: 0.7868 - val_loss: 1.4424 - val_soft_acc: 0.5625\n",
      "Epoch 219/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3285 - soft_acc: 0.7633 - val_loss: 1.5711 - val_soft_acc: 0.5244\n",
      "Epoch 220/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.6757 - soft_acc: 0.6233 - val_loss: 1.5076 - val_soft_acc: 0.5344\n",
      "Epoch 221/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3648 - soft_acc: 0.7212 - val_loss: 1.4045 - val_soft_acc: 0.5799\n",
      "Epoch 222/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3712 - soft_acc: 0.7196 - val_loss: 1.3573 - val_soft_acc: 0.5939\n",
      "Epoch 223/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2159 - soft_acc: 0.8098 - val_loss: 1.3220 - val_soft_acc: 0.5996\n",
      "Epoch 224/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4565 - soft_acc: 0.7068 - val_loss: 1.5601 - val_soft_acc: 0.5367\n",
      "Epoch 225/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.4245 - soft_acc: 0.7071 - val_loss: 1.6128 - val_soft_acc: 0.5299\n",
      "Epoch 226/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.3240 - soft_acc: 0.7503 - val_loss: 1.3184 - val_soft_acc: 0.5850\n",
      "Epoch 227/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.2522 - soft_acc: 0.7987 - val_loss: 1.4407 - val_soft_acc: 0.5879\n",
      "Epoch 228/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5052 - soft_acc: 0.6832 - val_loss: 1.6002 - val_soft_acc: 0.5225\n",
      "Epoch 229/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3497 - soft_acc: 0.7340 - val_loss: 1.3720 - val_soft_acc: 0.5766\n",
      "Epoch 230/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2539 - soft_acc: 0.7907 - val_loss: 1.4245 - val_soft_acc: 0.5758\n",
      "Epoch 231/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.4093 - soft_acc: 0.7308 - val_loss: 1.3532 - val_soft_acc: 0.5852\n",
      "Epoch 232/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.2749 - soft_acc: 0.7827 - val_loss: 1.3291 - val_soft_acc: 0.5959\n",
      "Epoch 233/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2416 - soft_acc: 0.8014 - val_loss: 1.2990 - val_soft_acc: 0.6107\n",
      "Epoch 234/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2544 - soft_acc: 0.7992 - val_loss: 1.5266 - val_soft_acc: 0.5613\n",
      "Epoch 235/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3146 - soft_acc: 0.7630 - val_loss: 1.5365 - val_soft_acc: 0.5551\n",
      "Epoch 236/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3108 - soft_acc: 0.7630 - val_loss: 1.3364 - val_soft_acc: 0.5875\n",
      "Epoch 237/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3486 - soft_acc: 0.7427 - val_loss: 1.6301 - val_soft_acc: 0.5215\n",
      "Epoch 238/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4378 - soft_acc: 0.7075 - val_loss: 1.4093 - val_soft_acc: 0.5893\n",
      "Epoch 239/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.3726 - soft_acc: 0.7311 - val_loss: 1.3502 - val_soft_acc: 0.5883\n",
      "Epoch 240/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.2211 - soft_acc: 0.8061 - val_loss: 1.4126 - val_soft_acc: 0.6018\n",
      "Epoch 241/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2191 - soft_acc: 0.8156 - val_loss: 1.3375 - val_soft_acc: 0.5920\n",
      "Epoch 242/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3601 - soft_acc: 0.7505 - val_loss: 1.6025 - val_soft_acc: 0.5543\n",
      "Epoch 00242: early stopping\n",
      ">0.554\n",
      "Train on 14640 samples, validate on 2440 samples\n",
      "Epoch 1/5000\n",
      "14640/14640 [==============================] - 2s 117us/step - loss: 5.4219 - soft_acc: 0.1486 - val_loss: 4.5107 - val_soft_acc: 0.1449\n",
      "Epoch 2/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 4.2608 - soft_acc: 0.1592 - val_loss: 4.0893 - val_soft_acc: 0.1441\n",
      "Epoch 3/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 4.1501 - soft_acc: 0.1622 - val_loss: 4.2288 - val_soft_acc: 0.1764\n",
      "Epoch 4/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 4.0572 - soft_acc: 0.1654 - val_loss: 3.9917 - val_soft_acc: 0.1775\n",
      "Epoch 5/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 4.0683 - soft_acc: 0.1698 - val_loss: 3.8104 - val_soft_acc: 0.1742\n",
      "Epoch 6/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.9470 - soft_acc: 0.1706 - val_loss: 3.7411 - val_soft_acc: 0.1764\n",
      "Epoch 7/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.8594 - soft_acc: 0.1732 - val_loss: 3.7612 - val_soft_acc: 0.1818\n",
      "Epoch 8/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.8427 - soft_acc: 0.1756 - val_loss: 3.7228 - val_soft_acc: 0.1883\n",
      "Epoch 9/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.8112 - soft_acc: 0.1774 - val_loss: 3.8739 - val_soft_acc: 0.1658\n",
      "Epoch 10/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 3.8064 - soft_acc: 0.1777 - val_loss: 3.6544 - val_soft_acc: 0.1754\n",
      "Epoch 11/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 3.7594 - soft_acc: 0.1791 - val_loss: 3.6868 - val_soft_acc: 0.1738\n",
      "Epoch 12/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.7288 - soft_acc: 0.1798 - val_loss: 3.6001 - val_soft_acc: 0.1932\n",
      "Epoch 13/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 3.6985 - soft_acc: 0.1830 - val_loss: 3.7124 - val_soft_acc: 0.1875\n",
      "Epoch 14/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 3.6781 - soft_acc: 0.1862 ETA - 1s 77us/step - loss: 3.6829 - soft_acc: 0.1859 - val_loss: 3.6935 - val_soft_acc: 0.1721\n",
      "Epoch 15/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.6738 - soft_acc: 0.1790 - val_loss: 3.6322 - val_soft_acc: 0.1971\n",
      "Epoch 16/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.6664 - soft_acc: 0.1855 - val_loss: 3.5529 - val_soft_acc: 0.1838\n",
      "Epoch 17/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.6622 - soft_acc: 0.1853 - val_loss: 3.5279 - val_soft_acc: 0.1967\n",
      "Epoch 18/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.6180 - soft_acc: 0.1921 - val_loss: 3.4823 - val_soft_acc: 0.1945\n",
      "Epoch 19/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.5758 - soft_acc: 0.1872 - val_loss: 3.4474 - val_soft_acc: 0.1982\n",
      "Epoch 20/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.6019 - soft_acc: 0.1856 - val_loss: 3.4853 - val_soft_acc: 0.1855\n",
      "Epoch 21/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.5245 - soft_acc: 0.1931 - val_loss: 3.4223 - val_soft_acc: 0.1988\n",
      "Epoch 22/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.5210 - soft_acc: 0.1919 - val_loss: 3.4498 - val_soft_acc: 0.1809\n",
      "Epoch 23/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.5038 - soft_acc: 0.1936 - val_loss: 3.4046 - val_soft_acc: 0.1883\n",
      "Epoch 24/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.4704 - soft_acc: 0.1929 - val_loss: 3.4109 - val_soft_acc: 0.1959\n",
      "Epoch 25/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 3.4804 - soft_acc: 0.1954 - val_loss: 3.3453 - val_soft_acc: 0.1986\n",
      "Epoch 26/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.4354 - soft_acc: 0.1986 - val_loss: 3.4216 - val_soft_acc: 0.1963\n",
      "Epoch 27/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.4045 - soft_acc: 0.1972 - val_loss: 3.3519 - val_soft_acc: 0.2076\n",
      "Epoch 28/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 3.4698 - soft_acc: 0.1988 - val_loss: 3.4907 - val_soft_acc: 0.1885\n",
      "Epoch 29/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.3865 - soft_acc: 0.2024 - val_loss: 3.3242 - val_soft_acc: 0.1857\n",
      "Epoch 30/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.3640 - soft_acc: 0.2032 - val_loss: 3.3017 - val_soft_acc: 0.2139\n",
      "Epoch 31/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.3495 - soft_acc: 0.2063 - val_loss: 3.2746 - val_soft_acc: 0.2102\n",
      "Epoch 32/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.3256 - soft_acc: 0.2068 - val_loss: 3.3366 - val_soft_acc: 0.2221\n",
      "Epoch 33/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.3088 - soft_acc: 0.2074 - val_loss: 3.1970 - val_soft_acc: 0.2119\n",
      "Epoch 34/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.2792 - soft_acc: 0.2080 - val_loss: 3.2916 - val_soft_acc: 0.2143\n",
      "Epoch 35/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.2733 - soft_acc: 0.2126 - val_loss: 3.2210 - val_soft_acc: 0.2082\n",
      "Epoch 36/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.2558 - soft_acc: 0.2150 - val_loss: 3.2629 - val_soft_acc: 0.2174\n",
      "Epoch 37/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.2516 - soft_acc: 0.2103 - val_loss: 3.1946 - val_soft_acc: 0.2117\n",
      "Epoch 38/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.2313 - soft_acc: 0.2168 - val_loss: 3.1159 - val_soft_acc: 0.2248\n",
      "Epoch 39/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 3.2179 - soft_acc: 0.2138 - val_loss: 3.5339 - val_soft_acc: 0.1936\n",
      "Epoch 40/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 3.1981 - soft_acc: 0.21 - 1s 77us/step - loss: 3.2010 - soft_acc: 0.2150 - val_loss: 3.1360 - val_soft_acc: 0.2037\n",
      "Epoch 41/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.1442 - soft_acc: 0.2199 - val_loss: 3.0680 - val_soft_acc: 0.2264\n",
      "Epoch 42/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.1250 - soft_acc: 0.2224 - val_loss: 3.0984 - val_soft_acc: 0.2318\n",
      "Epoch 43/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.1391 - soft_acc: 0.2207 - val_loss: 3.1104 - val_soft_acc: 0.2205\n",
      "Epoch 44/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.0938 - soft_acc: 0.2251 - val_loss: 3.0519 - val_soft_acc: 0.2309\n",
      "Epoch 45/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.0653 - soft_acc: 0.2250 - val_loss: 3.0413 - val_soft_acc: 0.2285\n",
      "Epoch 46/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.0803 - soft_acc: 0.2263 - val_loss: 3.0798 - val_soft_acc: 0.2322\n",
      "Epoch 47/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.0240 - soft_acc: 0.2309 - val_loss: 3.0115 - val_soft_acc: 0.2307\n",
      "Epoch 48/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 3.0175 - soft_acc: 0.2304 - val_loss: 3.0299 - val_soft_acc: 0.2232\n",
      "Epoch 49/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 2.9929 - soft_acc: 0.2327 - val_loss: 2.9305 - val_soft_acc: 0.2451\n",
      "Epoch 50/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.9699 - soft_acc: 0.2376 - val_loss: 2.9220 - val_soft_acc: 0.2324\n",
      "Epoch 51/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.9639 - soft_acc: 0.2357 - val_loss: 2.9797 - val_soft_acc: 0.2338\n",
      "Epoch 52/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 2.9366 - soft_acc: 0.2367 - val_loss: 2.9278 - val_soft_acc: 0.2412\n",
      "Epoch 53/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 2.9041 - soft_acc: 0.2408 - val_loss: 2.9181 - val_soft_acc: 0.2363\n",
      "Epoch 54/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.8775 - soft_acc: 0.2404 - val_loss: 2.8669 - val_soft_acc: 0.2480\n",
      "Epoch 55/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 2.8756 - soft_acc: 0.2458 - val_loss: 2.9891 - val_soft_acc: 0.2336\n",
      "Epoch 56/5000\n",
      "14640/14640 [==============================] - 1s 80us/step - loss: 2.8407 - soft_acc: 0.2498 - val_loss: 2.7985 - val_soft_acc: 0.2387\n",
      "Epoch 57/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.8013 - soft_acc: 0.2523 - val_loss: 2.8710 - val_soft_acc: 0.2510\n",
      "Epoch 58/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.7988 - soft_acc: 0.2487 - val_loss: 2.8737 - val_soft_acc: 0.2529\n",
      "Epoch 59/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.7641 - soft_acc: 0.2555 - val_loss: 2.8201 - val_soft_acc: 0.2523\n",
      "Epoch 60/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.7740 - soft_acc: 0.2506 - val_loss: 2.8157 - val_soft_acc: 0.2352\n",
      "Epoch 61/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.7427 - soft_acc: 0.2579 - val_loss: 2.8677 - val_soft_acc: 0.2605\n",
      "Epoch 62/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 2.7022 - soft_acc: 0.26 - 1s 77us/step - loss: 2.7060 - soft_acc: 0.2596 - val_loss: 2.9069 - val_soft_acc: 0.2586\n",
      "Epoch 63/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.6631 - soft_acc: 0.2649 - val_loss: 2.7755 - val_soft_acc: 0.2648\n",
      "Epoch 64/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.6455 - soft_acc: 0.2610 - val_loss: 2.7535 - val_soft_acc: 0.2621\n",
      "Epoch 65/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.6290 - soft_acc: 0.2659 - val_loss: 2.6947 - val_soft_acc: 0.2762\n",
      "Epoch 66/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.5899 - soft_acc: 0.2699 - val_loss: 2.8206 - val_soft_acc: 0.2469\n",
      "Epoch 67/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 2.5700 - soft_acc: 0.2709 - val_loss: 2.6389 - val_soft_acc: 0.2795\n",
      "Epoch 68/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.4969 - soft_acc: 0.2800 - val_loss: 2.6325 - val_soft_acc: 0.2898\n",
      "Epoch 69/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.4912 - soft_acc: 0.2818 - val_loss: 2.6462 - val_soft_acc: 0.2695\n",
      "Epoch 70/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.4989 - soft_acc: 0.2820 - val_loss: 2.6502 - val_soft_acc: 0.2752\n",
      "Epoch 71/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.4940 - soft_acc: 0.2812 - val_loss: 2.6361 - val_soft_acc: 0.2590\n",
      "Epoch 72/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.4479 - soft_acc: 0.2827 - val_loss: 2.5308 - val_soft_acc: 0.2918\n",
      "Epoch 73/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.3971 - soft_acc: 0.2936 - val_loss: 2.5798 - val_soft_acc: 0.2875\n",
      "Epoch 74/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.3655 - soft_acc: 0.2931 - val_loss: 2.5362 - val_soft_acc: 0.2963\n",
      "Epoch 75/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.3383 - soft_acc: 0.2958 - val_loss: 2.4624 - val_soft_acc: 0.2787\n",
      "Epoch 76/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.2953 - soft_acc: 0.2998 - val_loss: 2.4608 - val_soft_acc: 0.2791\n",
      "Epoch 77/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.2581 - soft_acc: 0.3088 - val_loss: 2.4422 - val_soft_acc: 0.3139\n",
      "Epoch 78/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.2490 - soft_acc: 0.3054 - val_loss: 2.3851 - val_soft_acc: 0.3117\n",
      "Epoch 79/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 2.2011 - soft_acc: 0.3092 - val_loss: 2.4549 - val_soft_acc: 0.3061\n",
      "Epoch 80/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.1600 - soft_acc: 0.3179 - val_loss: 2.4217 - val_soft_acc: 0.2959\n",
      "Epoch 81/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 2.1925 - soft_acc: 0.3143 - val_loss: 2.3664 - val_soft_acc: 0.3094\n",
      "Epoch 82/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 2.1171 - soft_acc: 0.3242 - val_loss: 2.4371 - val_soft_acc: 0.2957\n",
      "Epoch 83/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.0891 - soft_acc: 0.3268 - val_loss: 2.3339 - val_soft_acc: 0.3031\n",
      "Epoch 84/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 2.0673 - soft_acc: 0.32 - 1s 77us/step - loss: 2.0795 - soft_acc: 0.3250 - val_loss: 2.2649 - val_soft_acc: 0.3115\n",
      "Epoch 85/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.0140 - soft_acc: 0.3338 - val_loss: 2.3125 - val_soft_acc: 0.3020\n",
      "Epoch 86/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 2.0094 - soft_acc: 0.3305 - val_loss: 2.5176 - val_soft_acc: 0.3256\n",
      "Epoch 87/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.9481 - soft_acc: 0.3428 - val_loss: 2.3173 - val_soft_acc: 0.3213\n",
      "Epoch 88/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.9286 - soft_acc: 0.3433 - val_loss: 2.2961 - val_soft_acc: 0.3266\n",
      "Epoch 89/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 1.8807 - soft_acc: 0.35 - 1s 77us/step - loss: 1.8872 - soft_acc: 0.3505 - val_loss: 2.2485 - val_soft_acc: 0.3299\n",
      "Epoch 90/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.8958 - soft_acc: 0.3477 - val_loss: 2.2344 - val_soft_acc: 0.3281\n",
      "Epoch 91/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.8163 - soft_acc: 0.3596 - val_loss: 2.2343 - val_soft_acc: 0.3256\n",
      "Epoch 92/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.8178 - soft_acc: 0.3589 - val_loss: 2.2091 - val_soft_acc: 0.3387\n",
      "Epoch 93/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.7762 - soft_acc: 0.3697 - val_loss: 2.1915 - val_soft_acc: 0.3426\n",
      "Epoch 94/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 1.7778 - soft_acc: 0.3661 - val_loss: 2.1060 - val_soft_acc: 0.3496\n",
      "Epoch 95/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 1.7303 - soft_acc: 0.3775 - val_loss: 2.1132 - val_soft_acc: 0.3531\n",
      "Epoch 96/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 1.7047 - soft_acc: 0.3741 - val_loss: 2.0983 - val_soft_acc: 0.3635\n",
      "Epoch 97/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.7054 - soft_acc: 0.3775 - val_loss: 2.0260 - val_soft_acc: 0.3498\n",
      "Epoch 98/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.6345 - soft_acc: 0.3867 - val_loss: 1.9919 - val_soft_acc: 0.3566\n",
      "Epoch 99/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 1.6028 - soft_acc: 0.39 - 1s 77us/step - loss: 1.6018 - soft_acc: 0.3905 - val_loss: 2.0693 - val_soft_acc: 0.3480\n",
      "Epoch 100/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.5754 - soft_acc: 0.4012 - val_loss: 2.0364 - val_soft_acc: 0.3531\n",
      "Epoch 101/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.5130 - soft_acc: 0.4056 - val_loss: 2.1449 - val_soft_acc: 0.3533\n",
      "Epoch 102/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.5014 - soft_acc: 0.4062 - val_loss: 2.0799 - val_soft_acc: 0.3449\n",
      "Epoch 103/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.6185 - soft_acc: 0.3888 - val_loss: 2.1291 - val_soft_acc: 0.3648\n",
      "Epoch 104/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.4803 - soft_acc: 0.4074 - val_loss: 1.9780 - val_soft_acc: 0.3836\n",
      "Epoch 105/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.4286 - soft_acc: 0.4242 - val_loss: 2.0803 - val_soft_acc: 0.3699\n",
      "Epoch 106/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.4093 - soft_acc: 0.4192 - val_loss: 1.9065 - val_soft_acc: 0.3775\n",
      "Epoch 107/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.3485 - soft_acc: 0.4346 - val_loss: 2.0023 - val_soft_acc: 0.3846\n",
      "Epoch 108/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.3977 - soft_acc: 0.4226 - val_loss: 2.1520 - val_soft_acc: 0.3566\n",
      "Epoch 109/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 1.3983 - soft_acc: 0.4290 - val_loss: 1.9369 - val_soft_acc: 0.3846\n",
      "Epoch 110/5000\n",
      "14640/14640 [==============================] - 1s 81us/step - loss: 1.3735 - soft_acc: 0.4326 - val_loss: 1.8637 - val_soft_acc: 0.3990\n",
      "Epoch 111/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.2742 - soft_acc: 0.4455 - val_loss: 1.9468 - val_soft_acc: 0.3848\n",
      "Epoch 112/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.4074 - soft_acc: 0.4231 - val_loss: 1.9746 - val_soft_acc: 0.3826\n",
      "Epoch 113/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.3498 - soft_acc: 0.4386 - val_loss: 1.9535 - val_soft_acc: 0.3811\n",
      "Epoch 114/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.2144 - soft_acc: 0.4600 - val_loss: 1.8554 - val_soft_acc: 0.4080\n",
      "Epoch 115/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.1731 - soft_acc: 0.4716 - val_loss: 1.8808 - val_soft_acc: 0.3945\n",
      "Epoch 116/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.1817 - soft_acc: 0.4694 - val_loss: 1.9374 - val_soft_acc: 0.4008\n",
      "Epoch 117/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.1524 - soft_acc: 0.4712 - val_loss: 1.8690 - val_soft_acc: 0.4141\n",
      "Epoch 118/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.2023 - soft_acc: 0.4678 - val_loss: 2.0462 - val_soft_acc: 0.4010\n",
      "Epoch 119/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.1863 - soft_acc: 0.4650 - val_loss: 1.8674 - val_soft_acc: 0.4059\n",
      "Epoch 120/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.0779 - soft_acc: 0.4864 - val_loss: 1.7279 - val_soft_acc: 0.4268\n",
      "Epoch 121/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 1.0449 - soft_acc: 0.49 - 1s 77us/step - loss: 1.0496 - soft_acc: 0.4915 - val_loss: 1.8234 - val_soft_acc: 0.4301\n",
      "Epoch 122/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.0624 - soft_acc: 0.4918 - val_loss: 1.9292 - val_soft_acc: 0.4078\n",
      "Epoch 123/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.0791 - soft_acc: 0.4923 - val_loss: 1.7848 - val_soft_acc: 0.4213\n",
      "Epoch 124/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 1.0521 - soft_acc: 0.5013 - val_loss: 1.8233 - val_soft_acc: 0.4148\n",
      "Epoch 125/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.0043 - soft_acc: 0.5066 - val_loss: 1.8240 - val_soft_acc: 0.4266\n",
      "Epoch 126/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.0034 - soft_acc: 0.5076 - val_loss: 1.8334 - val_soft_acc: 0.4139\n",
      "Epoch 127/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.9757 - soft_acc: 0.5129 - val_loss: 1.7251 - val_soft_acc: 0.4281\n",
      "Epoch 128/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.0608 - soft_acc: 0.4919 - val_loss: 1.8890 - val_soft_acc: 0.4018\n",
      "Epoch 129/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 1.0099 - soft_acc: 0.5018 - val_loss: 1.7413 - val_soft_acc: 0.4436\n",
      "Epoch 130/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.9325 - soft_acc: 0.5257 - val_loss: 1.6979 - val_soft_acc: 0.4430\n",
      "Epoch 131/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.9302 - soft_acc: 0.5305 - val_loss: 1.8237 - val_soft_acc: 0.4402\n",
      "Epoch 132/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.9439 - soft_acc: 0.5266 - val_loss: 1.6943 - val_soft_acc: 0.4436\n",
      "Epoch 133/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.8911 - soft_acc: 0.5369 - val_loss: 1.6481 - val_soft_acc: 0.4420\n",
      "Epoch 134/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8774 - soft_acc: 0.5422 - val_loss: 1.5784 - val_soft_acc: 0.4670\n",
      "Epoch 135/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.8125 - soft_acc: 0.5600 - val_loss: 1.6700 - val_soft_acc: 0.4410\n",
      "Epoch 136/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.8405 - soft_acc: 0.5566 - val_loss: 1.6998 - val_soft_acc: 0.4357\n",
      "Epoch 137/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.0604 - soft_acc: 0.5015 - val_loss: 1.8420 - val_soft_acc: 0.4180\n",
      "Epoch 138/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.9614 - soft_acc: 0.5162 - val_loss: 1.6939 - val_soft_acc: 0.4498\n",
      "Epoch 139/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.7897 - soft_acc: 0.5628 - val_loss: 1.6990 - val_soft_acc: 0.4645\n",
      "Epoch 140/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.9615 - soft_acc: 0.5256 - val_loss: 1.7224 - val_soft_acc: 0.4383\n",
      "Epoch 141/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.8517 - soft_acc: 0.54 - 1s 77us/step - loss: 0.8571 - soft_acc: 0.5416 - val_loss: 1.7136 - val_soft_acc: 0.4371\n",
      "Epoch 142/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.7256 - soft_acc: 0.5845 - val_loss: 1.5655 - val_soft_acc: 0.4818\n",
      "Epoch 143/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.7647 - soft_acc: 0.5751 - val_loss: 1.6154 - val_soft_acc: 0.4803\n",
      "Epoch 144/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.7738 - soft_acc: 0.5811 - val_loss: 1.6433 - val_soft_acc: 0.4631\n",
      "Epoch 145/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.7271 - soft_acc: 0.5847 - val_loss: 1.5089 - val_soft_acc: 0.4867\n",
      "Epoch 146/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.7508 - soft_acc: 0.5809 - val_loss: 1.8122 - val_soft_acc: 0.4369\n",
      "Epoch 147/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.7959 - soft_acc: 0.5734 - val_loss: 1.6886 - val_soft_acc: 0.4629\n",
      "Epoch 148/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.7336 - soft_acc: 0.5792 - val_loss: 1.7787 - val_soft_acc: 0.4639\n",
      "Epoch 149/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.7294 - soft_acc: 0.5913 - val_loss: 1.6574 - val_soft_acc: 0.4791\n",
      "Epoch 150/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.6674 - soft_acc: 0.6035 - val_loss: 1.5559 - val_soft_acc: 0.5000\n",
      "Epoch 151/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.7787 - soft_acc: 0.5737 - val_loss: 1.7177 - val_soft_acc: 0.4658\n",
      "Epoch 152/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.7236 - soft_acc: 0.5940 - val_loss: 1.7771 - val_soft_acc: 0.4553\n",
      "Epoch 153/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.8049 - soft_acc: 0.5677 - val_loss: 1.6857 - val_soft_acc: 0.4578\n",
      "Epoch 154/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.7433 - soft_acc: 0.5789 - val_loss: 1.6454 - val_soft_acc: 0.4758\n",
      "Epoch 155/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.6623 - soft_acc: 0.6082 - val_loss: 1.6036 - val_soft_acc: 0.4975\n",
      "Epoch 156/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5750 - soft_acc: 0.6397 - val_loss: 1.5646 - val_soft_acc: 0.5082\n",
      "Epoch 157/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5785 - soft_acc: 0.6477 - val_loss: 1.5177 - val_soft_acc: 0.4973\n",
      "Epoch 158/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.6384 - soft_acc: 0.6209 - val_loss: 1.5810 - val_soft_acc: 0.4801\n",
      "Epoch 159/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.6063 - soft_acc: 0.6336 - val_loss: 1.6873 - val_soft_acc: 0.4959\n",
      "Epoch 160/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.6973 - soft_acc: 0.6014 - val_loss: 1.6037 - val_soft_acc: 0.4881\n",
      "Epoch 161/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.6876 - soft_acc: 0.6016 - val_loss: 1.5925 - val_soft_acc: 0.4852\n",
      "Epoch 162/5000\n",
      "14640/14640 [==============================] - 1s 80us/step - loss: 0.6206 - soft_acc: 0.6276 - val_loss: 1.5875 - val_soft_acc: 0.4914\n",
      "Epoch 163/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5724 - soft_acc: 0.6417 - val_loss: 1.5903 - val_soft_acc: 0.4930\n",
      "Epoch 164/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5433 - soft_acc: 0.6539 - val_loss: 1.5087 - val_soft_acc: 0.5232\n",
      "Epoch 165/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5132 - soft_acc: 0.6570 - val_loss: 1.5083 - val_soft_acc: 0.5186\n",
      "Epoch 166/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.4782 - soft_acc: 0.6738 - val_loss: 1.4903 - val_soft_acc: 0.5299\n",
      "Epoch 167/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.4906 - soft_acc: 0.6742 - val_loss: 1.5731 - val_soft_acc: 0.5170\n",
      "Epoch 168/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.5197 - soft_acc: 0.6680 - val_loss: 1.5791 - val_soft_acc: 0.4953\n",
      "Epoch 169/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.7984 - soft_acc: 0.5746 - val_loss: 1.7408 - val_soft_acc: 0.4500\n",
      "Epoch 170/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.7218 - soft_acc: 0.5905 - val_loss: 1.6660 - val_soft_acc: 0.5004\n",
      "Epoch 171/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.9117 - soft_acc: 0.5452 - val_loss: 1.6083 - val_soft_acc: 0.4914\n",
      "Epoch 172/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5126 - soft_acc: 0.6563 - val_loss: 1.4198 - val_soft_acc: 0.5350\n",
      "Epoch 173/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4108 - soft_acc: 0.7070 - val_loss: 1.4372 - val_soft_acc: 0.5365\n",
      "Epoch 174/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4007 - soft_acc: 0.7168 - val_loss: 1.3454 - val_soft_acc: 0.5664\n",
      "Epoch 175/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4099 - soft_acc: 0.7126 - val_loss: 1.3935 - val_soft_acc: 0.5496\n",
      "Epoch 176/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4014 - soft_acc: 0.7222 - val_loss: 1.4208 - val_soft_acc: 0.5424\n",
      "Epoch 177/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5486 - soft_acc: 0.6709 - val_loss: 1.6790 - val_soft_acc: 0.4758\n",
      "Epoch 178/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5848 - soft_acc: 0.6357 - val_loss: 1.5415 - val_soft_acc: 0.5084\n",
      "Epoch 179/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5520 - soft_acc: 0.6465 - val_loss: 1.4187 - val_soft_acc: 0.5385\n",
      "Epoch 180/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.4361 - soft_acc: 0.6950 - val_loss: 1.4692 - val_soft_acc: 0.5350\n",
      "Epoch 181/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.4542 - soft_acc: 0.6876 - val_loss: 1.5529 - val_soft_acc: 0.5232\n",
      "Epoch 182/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5560 - soft_acc: 0.6534 - val_loss: 1.5832 - val_soft_acc: 0.5072\n",
      "Epoch 183/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.5212 - soft_acc: 0.65 - 1s 77us/step - loss: 0.5325 - soft_acc: 0.6566 - val_loss: 1.6211 - val_soft_acc: 0.4982\n",
      "Epoch 184/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.6454 - soft_acc: 0.61 - 1s 77us/step - loss: 0.6476 - soft_acc: 0.6153 - val_loss: 1.8473 - val_soft_acc: 0.4859\n",
      "Epoch 185/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5256 - soft_acc: 0.6584 - val_loss: 1.3389 - val_soft_acc: 0.5512\n",
      "Epoch 186/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4221 - soft_acc: 0.7088 - val_loss: 1.4990 - val_soft_acc: 0.5387\n",
      "Epoch 187/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4283 - soft_acc: 0.7051 - val_loss: 1.4734 - val_soft_acc: 0.5342\n",
      "Epoch 188/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4171 - soft_acc: 0.7164 - val_loss: 1.5798 - val_soft_acc: 0.5189\n",
      "Epoch 189/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5637 - soft_acc: 0.6482 - val_loss: 1.4552 - val_soft_acc: 0.5416\n",
      "Epoch 190/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3462 - soft_acc: 0.7351 - val_loss: 1.4549 - val_soft_acc: 0.5480\n",
      "Epoch 191/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.3458 - soft_acc: 0.7490 - val_loss: 1.3438 - val_soft_acc: 0.5541\n",
      "Epoch 192/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4246 - soft_acc: 0.7127 - val_loss: 1.5050 - val_soft_acc: 0.5195\n",
      "Epoch 193/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3834 - soft_acc: 0.7230 - val_loss: 1.3638 - val_soft_acc: 0.5668\n",
      "Epoch 194/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4041 - soft_acc: 0.7209 - val_loss: 1.5203 - val_soft_acc: 0.5332\n",
      "Epoch 195/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.5783 - soft_acc: 0.6410 - val_loss: 1.7828 - val_soft_acc: 0.4723\n",
      "Epoch 196/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5348 - soft_acc: 0.6516 - val_loss: 1.4595 - val_soft_acc: 0.5439\n",
      "Epoch 197/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4944 - soft_acc: 0.6704 - val_loss: 1.3374 - val_soft_acc: 0.5631\n",
      "Epoch 198/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3017 - soft_acc: 0.7581 - val_loss: 1.4158 - val_soft_acc: 0.5750\n",
      "Epoch 199/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3303 - soft_acc: 0.7497 - val_loss: 1.2681 - val_soft_acc: 0.5883\n",
      "Epoch 200/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.2479 - soft_acc: 0.7987 - val_loss: 1.4081 - val_soft_acc: 0.5686\n",
      "Epoch 201/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4576 - soft_acc: 0.7018 - val_loss: 1.5801 - val_soft_acc: 0.5137\n",
      "Epoch 202/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.6159 - soft_acc: 0.6329 - val_loss: 1.5624 - val_soft_acc: 0.5090\n",
      "Epoch 203/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3968 - soft_acc: 0.7049 - val_loss: 1.4832 - val_soft_acc: 0.5594\n",
      "Epoch 204/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4384 - soft_acc: 0.7150 - val_loss: 1.5136 - val_soft_acc: 0.5266\n",
      "Epoch 205/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3802 - soft_acc: 0.7325 - val_loss: 1.8136 - val_soft_acc: 0.4654\n",
      "Epoch 206/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.6003 - soft_acc: 0.6392 - val_loss: 1.4937 - val_soft_acc: 0.5367\n",
      "Epoch 207/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3358 - soft_acc: 0.7428 - val_loss: 1.4199 - val_soft_acc: 0.5682\n",
      "Epoch 208/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3253 - soft_acc: 0.7647 - val_loss: 1.3497 - val_soft_acc: 0.5801\n",
      "Epoch 209/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.3755 - soft_acc: 0.7329 - val_loss: 1.5436 - val_soft_acc: 0.5393\n",
      "Epoch 210/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.3959 - soft_acc: 0.72 - 1s 77us/step - loss: 0.3924 - soft_acc: 0.7225 - val_loss: 1.4248 - val_soft_acc: 0.5652\n",
      "Epoch 211/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3227 - soft_acc: 0.7541 - val_loss: 1.3716 - val_soft_acc: 0.5588\n",
      "Epoch 212/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3285 - soft_acc: 0.7655 - val_loss: 1.7482 - val_soft_acc: 0.4912\n",
      "Epoch 213/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5645 - soft_acc: 0.6522 - val_loss: 1.3979 - val_soft_acc: 0.5523\n",
      "Epoch 214/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3223 - soft_acc: 0.7455 - val_loss: 1.4046 - val_soft_acc: 0.5564\n",
      "Epoch 215/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.3725 - soft_acc: 0.7286 - val_loss: 1.4660 - val_soft_acc: 0.5488\n",
      "Epoch 216/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.2917 - soft_acc: 0.7707 - val_loss: 1.4114 - val_soft_acc: 0.5736\n",
      "Epoch 217/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2966 - soft_acc: 0.7718 - val_loss: 1.3225 - val_soft_acc: 0.5949\n",
      "Epoch 218/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3894 - soft_acc: 0.7339 - val_loss: 1.6048 - val_soft_acc: 0.5180\n",
      "Epoch 219/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4299 - soft_acc: 0.7027 - val_loss: 1.3794 - val_soft_acc: 0.5531\n",
      "Epoch 220/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.3424 - soft_acc: 0.7326 - val_loss: 1.3020 - val_soft_acc: 0.5814\n",
      "Epoch 221/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2348 - soft_acc: 0.7971 - val_loss: 1.4418 - val_soft_acc: 0.5656\n",
      "Epoch 222/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3237 - soft_acc: 0.7544 - val_loss: 1.3708 - val_soft_acc: 0.5969\n",
      "Epoch 223/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.2624 - soft_acc: 0.7916 - val_loss: 1.4510 - val_soft_acc: 0.5838\n",
      "Epoch 224/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3133 - soft_acc: 0.7670 - val_loss: 1.4901 - val_soft_acc: 0.5549\n",
      "Epoch 225/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4432 - soft_acc: 0.6966 - val_loss: 1.5051 - val_soft_acc: 0.5410\n",
      "Epoch 226/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4358 - soft_acc: 0.6959 - val_loss: 1.7909 - val_soft_acc: 0.4758\n",
      "Epoch 227/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4873 - soft_acc: 0.6827 - val_loss: 1.6191 - val_soft_acc: 0.5090\n",
      "Epoch 228/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4115 - soft_acc: 0.7119 - val_loss: 1.4964 - val_soft_acc: 0.5490\n",
      "Epoch 229/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3281 - soft_acc: 0.7412 - val_loss: 1.3118 - val_soft_acc: 0.6010\n",
      "Epoch 230/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2883 - soft_acc: 0.7753 - val_loss: 1.3367 - val_soft_acc: 0.6072\n",
      "Epoch 231/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1809 - soft_acc: 0.8301 - val_loss: 1.1961 - val_soft_acc: 0.6371\n",
      "Epoch 232/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1784 - soft_acc: 0.8460 - val_loss: 1.1794 - val_soft_acc: 0.6398\n",
      "Epoch 233/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.1972 - soft_acc: 0.8391 ETA: 1s - loss: 0.1114 - soft_acc: 0.87 - ETA - 1s 77us/step - loss: 0.1986 - soft_acc: 0.8378 - val_loss: 1.3330 - val_soft_acc: 0.5926\n",
      "Epoch 234/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3620 - soft_acc: 0.7318 - val_loss: 1.3812 - val_soft_acc: 0.5805\n",
      "Epoch 235/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4046 - soft_acc: 0.7176 - val_loss: 1.3947 - val_soft_acc: 0.5738\n",
      "Epoch 236/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2455 - soft_acc: 0.7884 - val_loss: 1.2267 - val_soft_acc: 0.5924\n",
      "Epoch 237/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.3453 - soft_acc: 0.7537 - val_loss: 1.5181 - val_soft_acc: 0.5512\n",
      "Epoch 238/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4762 - soft_acc: 0.6991 - val_loss: 1.4254 - val_soft_acc: 0.5652\n",
      "Epoch 239/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3041 - soft_acc: 0.7615 - val_loss: 1.4946 - val_soft_acc: 0.5643\n",
      "Epoch 240/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3994 - soft_acc: 0.7178 - val_loss: 1.4070 - val_soft_acc: 0.5592\n",
      "Epoch 241/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4117 - soft_acc: 0.7115 - val_loss: 1.4088 - val_soft_acc: 0.5715\n",
      "Epoch 242/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2203 - soft_acc: 0.8063 - val_loss: 1.2407 - val_soft_acc: 0.6141\n",
      "Epoch 243/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1653 - soft_acc: 0.8523 - val_loss: 1.1392 - val_soft_acc: 0.6469\n",
      "Epoch 244/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1657 - soft_acc: 0.8546 - val_loss: 1.2781 - val_soft_acc: 0.6113\n",
      "Epoch 245/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1748 - soft_acc: 0.8502 - val_loss: 1.2032 - val_soft_acc: 0.6336\n",
      "Epoch 246/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1284 - soft_acc: 0.8783 - val_loss: 1.2181 - val_soft_acc: 0.6342\n",
      "Epoch 247/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1639 - soft_acc: 0.8626 - val_loss: 1.2394 - val_soft_acc: 0.6244\n",
      "Epoch 248/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1943 - soft_acc: 0.8398 - val_loss: 1.2229 - val_soft_acc: 0.6217\n",
      "Epoch 249/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3035 - soft_acc: 0.7652 - val_loss: 1.4607 - val_soft_acc: 0.5697\n",
      "Epoch 250/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.7511 - soft_acc: 0.60 - 1s 77us/step - loss: 0.7453 - soft_acc: 0.6029 - val_loss: 1.5773 - val_soft_acc: 0.5195\n",
      "Epoch 251/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.4750 - soft_acc: 0.6786 - val_loss: 1.4527 - val_soft_acc: 0.5506\n",
      "Epoch 252/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.3453 - soft_acc: 0.7372 - val_loss: 1.3919 - val_soft_acc: 0.5805\n",
      "Epoch 253/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1978 - soft_acc: 0.8152 - val_loss: 1.1700 - val_soft_acc: 0.6400\n",
      "Epoch 254/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.1343 - soft_acc: 0.86 - 1s 77us/step - loss: 0.1333 - soft_acc: 0.8662 - val_loss: 1.1932 - val_soft_acc: 0.6420\n",
      "Epoch 255/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1334 - soft_acc: 0.8769 - val_loss: 1.2202 - val_soft_acc: 0.6350\n",
      "Epoch 256/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1789 - soft_acc: 0.8568 - val_loss: 1.3100 - val_soft_acc: 0.5990\n",
      "Epoch 257/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1982 - soft_acc: 0.8381 - val_loss: 1.2348 - val_soft_acc: 0.6205\n",
      "Epoch 258/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1649 - soft_acc: 0.8539 - val_loss: 1.2821 - val_soft_acc: 0.6049\n",
      "Epoch 259/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2200 - soft_acc: 0.8248 - val_loss: 1.4617 - val_soft_acc: 0.5643\n",
      "Epoch 260/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.7089 - soft_acc: 0.6204 - val_loss: 1.6556 - val_soft_acc: 0.5025\n",
      "Epoch 261/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3776 - soft_acc: 0.7108 - val_loss: 1.3025 - val_soft_acc: 0.5877\n",
      "Epoch 262/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1650 - soft_acc: 0.8291 - val_loss: 1.2054 - val_soft_acc: 0.6305\n",
      "Epoch 263/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1049 - soft_acc: 0.8828 - val_loss: 1.1799 - val_soft_acc: 0.6471\n",
      "Epoch 264/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0996 - soft_acc: 0.8997 - val_loss: 1.1555 - val_soft_acc: 0.6535\n",
      "Epoch 265/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.1070 - soft_acc: 0.9006 - val_loss: 1.2728 - val_soft_acc: 0.6100\n",
      "Epoch 266/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.3062 - soft_acc: 0.7791 - val_loss: 1.4701 - val_soft_acc: 0.5607\n",
      "Epoch 267/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.1990 - soft_acc: 0.5043 - val_loss: 1.6005 - val_soft_acc: 0.5117\n",
      "Epoch 268/5000\n",
      "14640/14640 [==============================] - 1s 80us/step - loss: 0.3634 - soft_acc: 0.7181 - val_loss: 1.2447 - val_soft_acc: 0.5914\n",
      "Epoch 269/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1906 - soft_acc: 0.8202 - val_loss: 1.2103 - val_soft_acc: 0.6133\n",
      "Epoch 270/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1160 - soft_acc: 0.8724 - val_loss: 1.1623 - val_soft_acc: 0.6418\n",
      "Epoch 271/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1633 - soft_acc: 0.8631 - val_loss: 1.3253 - val_soft_acc: 0.6131\n",
      "Epoch 272/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2156 - soft_acc: 0.8189 - val_loss: 1.2498 - val_soft_acc: 0.6158\n",
      "Epoch 273/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1792 - soft_acc: 0.8399 - val_loss: 1.1958 - val_soft_acc: 0.6328\n",
      "Epoch 274/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1072 - soft_acc: 0.8932 - val_loss: 1.1586 - val_soft_acc: 0.6449\n",
      "Epoch 275/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1772 - soft_acc: 0.8481 - val_loss: 1.2132 - val_soft_acc: 0.6346\n",
      "Epoch 276/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1607 - soft_acc: 0.8634 - val_loss: 1.2281 - val_soft_acc: 0.6326\n",
      "Epoch 277/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1662 - soft_acc: 0.8545 - val_loss: 1.1924 - val_soft_acc: 0.6240\n",
      "Epoch 278/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2008 - soft_acc: 0.8236 - val_loss: 1.4633 - val_soft_acc: 0.5475\n",
      "Epoch 279/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.6891 - soft_acc: 0.6230 - val_loss: 1.6832 - val_soft_acc: 0.4895\n",
      "Epoch 280/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.7013 - soft_acc: 0.6091 - val_loss: 1.6233 - val_soft_acc: 0.5139\n",
      "Epoch 281/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3259 - soft_acc: 0.7373 - val_loss: 1.2412 - val_soft_acc: 0.6125\n",
      "Epoch 282/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1441 - soft_acc: 0.8474 - val_loss: 1.3314 - val_soft_acc: 0.6182\n",
      "Epoch 283/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1108 - soft_acc: 0.8869 - val_loss: 1.1302 - val_soft_acc: 0.6641\n",
      "Epoch 284/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1025 - soft_acc: 0.9000 - val_loss: 1.1151 - val_soft_acc: 0.6715\n",
      "Epoch 285/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1217 - soft_acc: 0.8937 - val_loss: 1.1908 - val_soft_acc: 0.6305\n",
      "Epoch 286/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1291 - soft_acc: 0.8758 - val_loss: 1.1384 - val_soft_acc: 0.6584\n",
      "Epoch 287/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0694 - soft_acc: 0.9179 - val_loss: 1.1411 - val_soft_acc: 0.6584\n",
      "Epoch 288/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1052 - soft_acc: 0.8980 - val_loss: 1.2317 - val_soft_acc: 0.6334\n",
      "Epoch 289/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.1386 - soft_acc: 0.8630 - val_loss: 1.1971 - val_soft_acc: 0.6406\n",
      "Epoch 290/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2327 - soft_acc: 0.8169 - val_loss: 1.4480 - val_soft_acc: 0.5824\n",
      "Epoch 291/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.4610 - soft_acc: 0.6960 - val_loss: 1.8655 - val_soft_acc: 0.4824\n",
      "Epoch 292/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.5045 - soft_acc: 0.67 - 1s 76us/step - loss: 0.4993 - soft_acc: 0.6775 - val_loss: 1.4628 - val_soft_acc: 0.5529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 293/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2834 - soft_acc: 0.7626 - val_loss: 1.3321 - val_soft_acc: 0.5955\n",
      "Epoch 294/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.1857 - soft_acc: 0.8316 - val_loss: 1.1751 - val_soft_acc: 0.6453\n",
      "Epoch 295/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0760 - soft_acc: 0.9051 - val_loss: 1.0853 - val_soft_acc: 0.6691\n",
      "Epoch 296/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0615 - soft_acc: 0.9281 - val_loss: 1.1511 - val_soft_acc: 0.6648\n",
      "Epoch 297/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1667 - soft_acc: 0.8699 - val_loss: 1.2260 - val_soft_acc: 0.6232\n",
      "Epoch 298/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1217 - soft_acc: 0.8779 - val_loss: 1.1994 - val_soft_acc: 0.6533\n",
      "Epoch 299/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2084 - soft_acc: 0.8174 - val_loss: 1.2513 - val_soft_acc: 0.6287\n",
      "Epoch 300/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1742 - soft_acc: 0.8408 - val_loss: 1.2556 - val_soft_acc: 0.6234\n",
      "Epoch 301/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2479 - soft_acc: 0.8029 - val_loss: 1.5724 - val_soft_acc: 0.5451\n",
      "Epoch 302/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5587 - soft_acc: 0.6727 - val_loss: 1.7013 - val_soft_acc: 0.5197\n",
      "Epoch 303/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3381 - soft_acc: 0.7328 - val_loss: 1.3592 - val_soft_acc: 0.5801\n",
      "Epoch 304/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1678 - soft_acc: 0.8398 - val_loss: 1.2130 - val_soft_acc: 0.6365\n",
      "Epoch 305/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0782 - soft_acc: 0.8998 - val_loss: 1.1097 - val_soft_acc: 0.6674\n",
      "Epoch 306/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.0674 - soft_acc: 0.9264 - val_loss: 1.2875 - val_soft_acc: 0.6305\n",
      "Epoch 307/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2101 - soft_acc: 0.8370 - val_loss: 1.3295 - val_soft_acc: 0.6244\n",
      "Epoch 308/5000\n",
      "14640/14640 [==============================] - 1s 81us/step - loss: 0.1564 - soft_acc: 0.8513 - val_loss: 1.1867 - val_soft_acc: 0.6436\n",
      "Epoch 309/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2217 - soft_acc: 0.8091 - val_loss: 1.3457 - val_soft_acc: 0.5951\n",
      "Epoch 310/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1344 - soft_acc: 0.8578 - val_loss: 1.1396 - val_soft_acc: 0.6598\n",
      "Epoch 311/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0748 - soft_acc: 0.9077 - val_loss: 1.1456 - val_soft_acc: 0.6654\n",
      "Epoch 312/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0768 - soft_acc: 0.9172 - val_loss: 1.1316 - val_soft_acc: 0.6701\n",
      "Epoch 313/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1352 - soft_acc: 0.8816 - val_loss: 1.1703 - val_soft_acc: 0.6482\n",
      "Epoch 314/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0904 - soft_acc: 0.9013 - val_loss: 1.1794 - val_soft_acc: 0.6404\n",
      "Epoch 315/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.1586 - soft_acc: 0.8579 - val_loss: 1.3283 - val_soft_acc: 0.5998\n",
      "Epoch 316/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4721 - soft_acc: 0.7242 - val_loss: 1.8137 - val_soft_acc: 0.4865\n",
      "Epoch 317/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.6006 - soft_acc: 0.6339 - val_loss: 1.5940 - val_soft_acc: 0.5455\n",
      "Epoch 318/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2762 - soft_acc: 0.7661 - val_loss: 1.2618 - val_soft_acc: 0.6219\n",
      "Epoch 319/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1928 - soft_acc: 0.8196 - val_loss: 1.2027 - val_soft_acc: 0.6447\n",
      "Epoch 320/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0833 - soft_acc: 0.9033 - val_loss: 1.0908 - val_soft_acc: 0.6725\n",
      "Epoch 321/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.0426 - soft_acc: 0.9391 - val_loss: 1.0792 - val_soft_acc: 0.6873\n",
      "Epoch 322/5000\n",
      "14640/14640 [==============================] - 1s 81us/step - loss: 0.0503 - soft_acc: 0.9435 - val_loss: 1.1379 - val_soft_acc: 0.6742\n",
      "Epoch 323/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0670 - soft_acc: 0.9326 - val_loss: 1.1530 - val_soft_acc: 0.6686\n",
      "Epoch 324/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0728 - soft_acc: 0.9199 - val_loss: 1.2264 - val_soft_acc: 0.6576\n",
      "Epoch 325/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1433 - soft_acc: 0.8620 - val_loss: 1.2289 - val_soft_acc: 0.6359\n",
      "Epoch 326/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1535 - soft_acc: 0.8567 - val_loss: 1.2763 - val_soft_acc: 0.6279\n",
      "Epoch 327/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3010 - soft_acc: 0.7853 - val_loss: 1.4954 - val_soft_acc: 0.5580\n",
      "Epoch 328/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.5550 - soft_acc: 0.6616 - val_loss: 1.5108 - val_soft_acc: 0.5551\n",
      "Epoch 329/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2326 - soft_acc: 0.7872 - val_loss: 1.2343 - val_soft_acc: 0.6262\n",
      "Epoch 330/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2380 - soft_acc: 0.8009 - val_loss: 1.2264 - val_soft_acc: 0.6246\n",
      "Epoch 331/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0794 - soft_acc: 0.8980 - val_loss: 1.0702 - val_soft_acc: 0.6734\n",
      "Epoch 332/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0635 - soft_acc: 0.9222 - val_loss: 1.0610 - val_soft_acc: 0.6873\n",
      "Epoch 333/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0391 - soft_acc: 0.9452 - val_loss: 1.1093 - val_soft_acc: 0.6840\n",
      "Epoch 334/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0340 - soft_acc: 0.9514 - val_loss: 1.0690 - val_soft_acc: 0.6957\n",
      "Epoch 335/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0694 - soft_acc: 0.9278 - val_loss: 1.1320 - val_soft_acc: 0.6676\n",
      "Epoch 336/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.0850 - soft_acc: 0.9109 - val_loss: 1.1415 - val_soft_acc: 0.6740\n",
      "Epoch 337/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1290 - soft_acc: 0.8891 - val_loss: 1.2892 - val_soft_acc: 0.6266\n",
      "Epoch 338/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1957 - soft_acc: 0.8404 - val_loss: 1.4876 - val_soft_acc: 0.5545\n",
      "Epoch 339/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5088 - soft_acc: 0.6907 - val_loss: 1.9437 - val_soft_acc: 0.4758\n",
      "Epoch 340/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.4658 - soft_acc: 0.69 - 1s 76us/step - loss: 0.4561 - soft_acc: 0.6963 - val_loss: 1.3225 - val_soft_acc: 0.5975\n",
      "Epoch 341/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1473 - soft_acc: 0.8390 - val_loss: 1.1615 - val_soft_acc: 0.6424\n",
      "Epoch 342/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0725 - soft_acc: 0.9032 - val_loss: 1.0950 - val_soft_acc: 0.6861\n",
      "Epoch 343/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0391 - soft_acc: 0.9390 - val_loss: 1.0681 - val_soft_acc: 0.6887\n",
      "Epoch 344/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0423 - soft_acc: 0.9450 - val_loss: 1.0580 - val_soft_acc: 0.6973\n",
      "Epoch 345/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0249 - soft_acc: 0.9565 - val_loss: 1.0575 - val_soft_acc: 0.6975\n",
      "Epoch 346/5000\n",
      "14640/14640 [==============================] - 1s 76us/step - loss: 0.0286 - soft_acc: 0.9552 - val_loss: 1.0453 - val_soft_acc: 0.7059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 347/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0570 - soft_acc: 0.9408 - val_loss: 1.2074 - val_soft_acc: 0.6490\n",
      "Epoch 348/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1243 - soft_acc: 0.8913 - val_loss: 1.3293 - val_soft_acc: 0.6203\n",
      "Epoch 349/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4932 - soft_acc: 0.7174 - val_loss: 2.0509 - val_soft_acc: 0.4631\n",
      "Epoch 350/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.8531 - soft_acc: 0.5938 - val_loss: 1.4850 - val_soft_acc: 0.5436\n",
      "Epoch 351/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.2282 - soft_acc: 0.7857 - val_loss: 1.2417 - val_soft_acc: 0.6252\n",
      "Epoch 352/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0864 - soft_acc: 0.8860 - val_loss: 1.1198 - val_soft_acc: 0.6697\n",
      "Epoch 353/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0426 - soft_acc: 0.9315 - val_loss: 1.1129 - val_soft_acc: 0.6900\n",
      "Epoch 354/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0388 - soft_acc: 0.9441 - val_loss: 1.0953 - val_soft_acc: 0.6889\n",
      "Epoch 355/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0582 - soft_acc: 0.9332 - val_loss: 1.1447 - val_soft_acc: 0.6750\n",
      "Epoch 356/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0403 - soft_acc: 0.9435 - val_loss: 1.0805 - val_soft_acc: 0.6879\n",
      "Epoch 357/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2066 - soft_acc: 0.8338 - val_loss: 1.3015 - val_soft_acc: 0.6193\n",
      "Epoch 358/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2398 - soft_acc: 0.7932 - val_loss: 1.2937 - val_soft_acc: 0.6211\n",
      "Epoch 359/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3099 - soft_acc: 0.7721 - val_loss: 1.4305 - val_soft_acc: 0.5881\n",
      "Epoch 360/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2338 - soft_acc: 0.8040 - val_loss: 1.3790 - val_soft_acc: 0.6100\n",
      "Epoch 361/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1690 - soft_acc: 0.8391 - val_loss: 1.1994 - val_soft_acc: 0.6545\n",
      "Epoch 362/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1119 - soft_acc: 0.8864 - val_loss: 1.1925 - val_soft_acc: 0.6689\n",
      "Epoch 363/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1003 - soft_acc: 0.9038 - val_loss: 1.1326 - val_soft_acc: 0.6738\n",
      "Epoch 364/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.0480 - soft_acc: 0.9373 - val_loss: 1.0735 - val_soft_acc: 0.6951\n",
      "Epoch 365/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.0283 - soft_acc: 0.9545 - val_loss: 1.0509 - val_soft_acc: 0.7076\n",
      "Epoch 366/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0144 - soft_acc: 0.9671 - val_loss: 1.0569 - val_soft_acc: 0.7047\n",
      "Epoch 367/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0098 - soft_acc: 0.9749 - val_loss: 1.0378 - val_soft_acc: 0.7109\n",
      "Epoch 368/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0071 - soft_acc: 0.9776 - val_loss: 1.0411 - val_soft_acc: 0.7080\n",
      "Epoch 369/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0446 - soft_acc: 0.9543 - val_loss: 1.1963 - val_soft_acc: 0.6586\n",
      "Epoch 370/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.6203 - soft_acc: 0.6917 - val_loss: 2.0254 - val_soft_acc: 0.4539\n",
      "Epoch 371/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.6796 - soft_acc: 0.6163 - val_loss: 1.4343 - val_soft_acc: 0.5641\n",
      "Epoch 372/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.2555 - soft_acc: 0.7779 - val_loss: 1.1927 - val_soft_acc: 0.6305\n",
      "Epoch 373/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0909 - soft_acc: 0.8804 - val_loss: 1.1052 - val_soft_acc: 0.6648\n",
      "Epoch 374/5000\n",
      "14640/14640 [==============================] - 1s 80us/step - loss: 0.0424 - soft_acc: 0.9332 - val_loss: 1.0460 - val_soft_acc: 0.7000\n",
      "Epoch 375/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.0509 - soft_acc: 0.9458 - val_loss: 1.1542 - val_soft_acc: 0.6844\n",
      "Epoch 376/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0774 - soft_acc: 0.9258 - val_loss: 1.0866 - val_soft_acc: 0.6855\n",
      "Epoch 377/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0523 - soft_acc: 0.9399 - val_loss: 1.0702 - val_soft_acc: 0.6945\n",
      "Epoch 378/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.0224 - soft_acc: 0.9612 - val_loss: 1.0346 - val_soft_acc: 0.7043\n",
      "Epoch 379/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.0123 - soft_acc: 0.9722 - val_loss: 1.0301 - val_soft_acc: 0.7055\n",
      "Epoch 380/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0341 - soft_acc: 0.9595 - val_loss: 1.1532 - val_soft_acc: 0.6791\n",
      "Epoch 381/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1424 - soft_acc: 0.8896 - val_loss: 1.6327 - val_soft_acc: 0.5453\n",
      "Epoch 382/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.8970 - soft_acc: 0.5712 - val_loss: 1.8412 - val_soft_acc: 0.4898\n",
      "Epoch 383/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.3918 - soft_acc: 0.7174 - val_loss: 1.5971 - val_soft_acc: 0.5506\n",
      "Epoch 384/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4701 - soft_acc: 0.7157 - val_loss: 1.2882 - val_soft_acc: 0.6084\n",
      "Epoch 385/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1774 - soft_acc: 0.8304 - val_loss: 1.1891 - val_soft_acc: 0.6568\n",
      "Epoch 386/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0689 - soft_acc: 0.9095 - val_loss: 1.2303 - val_soft_acc: 0.6453\n",
      "Epoch 387/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.0380 - soft_acc: 0.93 - 1s 77us/step - loss: 0.0375 - soft_acc: 0.9385 - val_loss: 1.0717 - val_soft_acc: 0.7002\n",
      "Epoch 388/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0168 - soft_acc: 0.9630 - val_loss: 1.0622 - val_soft_acc: 0.7033\n",
      "Epoch 389/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0119 - soft_acc: 0.9709 - val_loss: 1.0625 - val_soft_acc: 0.7076\n",
      "Epoch 390/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0087 - soft_acc: 0.9761 - val_loss: 1.0480 - val_soft_acc: 0.7072\n",
      "Epoch 391/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0097 - soft_acc: 0.9770 - val_loss: 1.0491 - val_soft_acc: 0.7100\n",
      "Epoch 392/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0063 - soft_acc: 0.9805 - val_loss: 1.0553 - val_soft_acc: 0.7113\n",
      "Epoch 393/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.0051 - soft_acc: 0.9827 - val_loss: 1.0492 - val_soft_acc: 0.7100\n",
      "Epoch 394/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0040 - soft_acc: 0.9836 - val_loss: 1.0421 - val_soft_acc: 0.7109\n",
      "Epoch 395/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0040 - soft_acc: 0.9850 - val_loss: 1.0442 - val_soft_acc: 0.7084\n",
      "Epoch 396/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0100 - soft_acc: 0.9766 - val_loss: 1.0522 - val_soft_acc: 0.7023\n",
      "Epoch 397/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.5021 - soft_acc: 0.7691 - val_loss: 2.5116 - val_soft_acc: 0.3982\n",
      "Epoch 398/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 1.0956 - soft_acc: 0.5242 - val_loss: 1.4569 - val_soft_acc: 0.5334\n",
      "Epoch 399/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.3242 - soft_acc: 0.7456 - val_loss: 1.2451 - val_soft_acc: 0.6119\n",
      "Epoch 400/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1120 - soft_acc: 0.8620 - val_loss: 1.1563 - val_soft_acc: 0.6670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 401/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0436 - soft_acc: 0.9283 - val_loss: 1.1066 - val_soft_acc: 0.6830\n",
      "Epoch 402/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0272 - soft_acc: 0.9517 - val_loss: 1.1016 - val_soft_acc: 0.6900\n",
      "Epoch 403/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0155 - soft_acc: 0.9645 - val_loss: 1.0953 - val_soft_acc: 0.6957\n",
      "Epoch 404/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0100 - soft_acc: 0.9739 - val_loss: 1.0729 - val_soft_acc: 0.7033\n",
      "Epoch 405/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0065 - soft_acc: 0.9792 - val_loss: 1.0766 - val_soft_acc: 0.7045\n",
      "Epoch 406/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0049 - soft_acc: 0.9816 - val_loss: 1.0742 - val_soft_acc: 0.7010\n",
      "Epoch 407/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.0038 - soft_acc: 0.9848 - val_loss: 1.0779 - val_soft_acc: 0.7035\n",
      "Epoch 408/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0034 - soft_acc: 0.9854 - val_loss: 1.0691 - val_soft_acc: 0.7037\n",
      "Epoch 409/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.0036 - soft_acc: 0.98 - 1s 77us/step - loss: 0.0036 - soft_acc: 0.9856 - val_loss: 1.0670 - val_soft_acc: 0.7041\n",
      "Epoch 410/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0064 - soft_acc: 0.9814 - val_loss: 1.0641 - val_soft_acc: 0.7012\n",
      "Epoch 411/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0398 - soft_acc: 0.9564 - val_loss: 1.4469 - val_soft_acc: 0.5797\n",
      "Epoch 412/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 1.0988 - soft_acc: 0.5248 - val_loss: 1.7459 - val_soft_acc: 0.4781\n",
      "Epoch 413/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.5465 - soft_acc: 0.65 - 1s 77us/step - loss: 0.5437 - soft_acc: 0.6527 - val_loss: 1.4974 - val_soft_acc: 0.5752\n",
      "Epoch 414/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.2430 - soft_acc: 0.7875 - val_loss: 1.2552 - val_soft_acc: 0.6348\n",
      "Epoch 415/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.1077 - soft_acc: 0.8731 - val_loss: 1.1825 - val_soft_acc: 0.6658\n",
      "Epoch 416/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0605 - soft_acc: 0.9198 - val_loss: 1.1664 - val_soft_acc: 0.6664\n",
      "Epoch 417/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0357 - soft_acc: 0.9459 - val_loss: 1.1164 - val_soft_acc: 0.6947\n",
      "Epoch 418/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0149 - soft_acc: 0.9668 - val_loss: 1.0854 - val_soft_acc: 0.7010\n",
      "Epoch 419/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.0091 - soft_acc: 0.9741 - val_loss: 1.0826 - val_soft_acc: 0.6996\n",
      "Epoch 420/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0061 - soft_acc: 0.9798 - val_loss: 1.0836 - val_soft_acc: 0.7020\n",
      "Epoch 421/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.0044 - soft_acc: 0.9843 - val_loss: 1.0793 - val_soft_acc: 0.7000\n",
      "Epoch 422/5000\n",
      "14640/14640 [==============================] - 1s 78us/step - loss: 0.0033 - soft_acc: 0.9851 - val_loss: 1.0787 - val_soft_acc: 0.7010\n",
      "Epoch 423/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0029 - soft_acc: 0.9865 - val_loss: 1.0776 - val_soft_acc: 0.7014\n",
      "Epoch 424/5000\n",
      "14640/14640 [==============================] - ETA: 0s - loss: 0.0028 - soft_acc: 0.98 - 1s 77us/step - loss: 0.0028 - soft_acc: 0.9859 - val_loss: 1.0739 - val_soft_acc: 0.7035\n",
      "Epoch 425/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0034 - soft_acc: 0.9850 - val_loss: 1.0700 - val_soft_acc: 0.7027\n",
      "Epoch 426/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.0046 - soft_acc: 0.9819 - val_loss: 1.0729 - val_soft_acc: 0.7049\n",
      "Epoch 427/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 0.1725 - soft_acc: 0.8965 - val_loss: 2.4833 - val_soft_acc: 0.3906\n",
      "Epoch 428/5000\n",
      "14640/14640 [==============================] - 1s 79us/step - loss: 1.4790 - soft_acc: 0.4497 - val_loss: 1.6093 - val_soft_acc: 0.4980\n",
      "Epoch 429/5000\n",
      "14640/14640 [==============================] - 1s 77us/step - loss: 0.4080 - soft_acc: 0.6903 - val_loss: 1.2426 - val_soft_acc: 0.6201\n",
      "Epoch 00429: early stopping\n",
      ">0.620\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# multiple train-test splits\n",
    "n_splits = 10\n",
    "scores, members = list(), list()\n",
    "for _ in range(n_splits):\n",
    "    # split data\n",
    "    # evaluate model\n",
    "    model, test_acc = evaluate_model(X_scaled_training, Y_scaled_training, X_scaled_testing, Y_scaled_testing)\n",
    "    print('>%.3f' % test_acc)\n",
    "    scores.append(test_acc)\n",
    "    members.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6529508196721312"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.6 , 7.71],\n",
       "       [7.6 , 7.71],\n",
       "       [7.31, 8.1 ],\n",
       "       ...,\n",
       "       [7.17, 3.99],\n",
       "       [7.17, 3.99],\n",
       "       [4.03, 7.15]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_scaled_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "ynew = ann.predict(X_scaled_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.29418  , 7.7717905],\n",
       "       [7.5049553, 7.6978884],\n",
       "       [8.305904 , 8.124946 ],\n",
       "       ...,\n",
       "       [3.8355913, 6.850955 ],\n",
       "       [3.9891179, 6.9222474],\n",
       "       [4.130633 , 6.7201843]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ynew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\anaconda\\envs\\mahim\\lib\\site-packages\\dask\\dataframe\\utils.py:13: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Epoch: 0 - Training Cost: 30.309858322143555  Testing Cost: 30.241701126098633\n",
      "Epoch: 1 - Training Cost: 29.853925704956055  Testing Cost: 29.78636932373047\n",
      "Epoch: 2 - Training Cost: 29.410423278808594  Testing Cost: 29.343551635742188\n",
      "Epoch: 3 - Training Cost: 28.9670467376709  Testing Cost: 28.900634765625\n",
      "Epoch: 4 - Training Cost: 28.513917922973633  Testing Cost: 28.447925567626953\n",
      "Epoch: 5 - Training Cost: 28.04292106628418  Testing Cost: 27.977638244628906\n",
      "Epoch: 6 - Training Cost: 27.546302795410156  Testing Cost: 27.481800079345703\n",
      "Epoch: 7 - Training Cost: 27.015918731689453  Testing Cost: 26.952272415161133\n",
      "Epoch: 8 - Training Cost: 26.444438934326172  Testing Cost: 26.381746292114258\n",
      "Epoch: 9 - Training Cost: 25.825841903686523  Testing Cost: 25.764341354370117\n",
      "Epoch: 10 - Training Cost: 25.154584884643555  Testing Cost: 25.094112396240234\n",
      "Epoch: 11 - Training Cost: 24.425487518310547  Testing Cost: 24.365890502929688\n",
      "Epoch: 12 - Training Cost: 23.633153915405273  Testing Cost: 23.574504852294922\n",
      "Epoch: 13 - Training Cost: 22.772268295288086  Testing Cost: 22.714595794677734\n",
      "Epoch: 14 - Training Cost: 21.83816146850586  Testing Cost: 21.781646728515625\n",
      "Epoch: 15 - Training Cost: 20.827198028564453  Testing Cost: 20.77204132080078\n",
      "Epoch: 16 - Training Cost: 19.73706817626953  Testing Cost: 19.683345794677734\n",
      "Epoch: 17 - Training Cost: 18.566633224487305  Testing Cost: 18.514554977416992\n",
      "Epoch: 18 - Training Cost: 17.31669807434082  Testing Cost: 17.266433715820312\n",
      "Epoch: 19 - Training Cost: 15.990690231323242  Testing Cost: 15.942387580871582\n",
      "Epoch: 20 - Training Cost: 14.59609603881836  Testing Cost: 14.549851417541504\n",
      "Epoch: 21 - Training Cost: 13.146470069885254  Testing Cost: 13.102463722229004\n",
      "Epoch: 22 - Training Cost: 11.663419723510742  Testing Cost: 11.621700286865234\n",
      "Epoch: 23 - Training Cost: 10.176705360412598  Testing Cost: 10.137185096740723\n",
      "Epoch: 24 - Training Cost: 8.725473403930664  Testing Cost: 8.688188552856445\n",
      "Epoch: 25 - Training Cost: 7.360080242156982  Testing Cost: 7.325128078460693\n",
      "Epoch: 26 - Training Cost: 6.143146991729736  Testing Cost: 6.110601902008057\n",
      "Epoch: 27 - Training Cost: 5.149313926696777  Testing Cost: 5.119087219238281\n",
      "Epoch: 28 - Training Cost: 4.459779262542725  Testing Cost: 4.431701183319092\n",
      "Epoch: 29 - Training Cost: 4.147907733917236  Testing Cost: 4.121622562408447\n",
      "Epoch: 30 - Training Cost: 4.248000144958496  Testing Cost: 4.223156929016113\n",
      "Epoch: 31 - Training Cost: 4.702936172485352  Testing Cost: 4.679111957550049\n",
      "Epoch: 32 - Training Cost: 5.327807426452637  Testing Cost: 5.30468225479126\n",
      "Epoch: 33 - Training Cost: 5.874893665313721  Testing Cost: 5.852203845977783\n",
      "Epoch: 34 - Training Cost: 6.164112567901611  Testing Cost: 6.141741752624512\n",
      "Epoch: 35 - Training Cost: 6.147762775421143  Testing Cost: 6.125609397888184\n",
      "Epoch: 36 - Training Cost: 5.884410381317139  Testing Cost: 5.8623576164245605\n",
      "Epoch: 37 - Training Cost: 5.480947017669678  Testing Cost: 5.458883762359619\n",
      "Epoch: 38 - Training Cost: 5.045056343078613  Testing Cost: 5.022807598114014\n",
      "Epoch: 39 - Training Cost: 4.658679008483887  Testing Cost: 4.6360673904418945\n",
      "Epoch: 40 - Training Cost: 4.369065284729004  Testing Cost: 4.345961093902588\n",
      "Epoch: 41 - Training Cost: 4.19120979309082  Testing Cost: 4.16748571395874\n",
      "Epoch: 42 - Training Cost: 4.1164093017578125  Testing Cost: 4.091994762420654\n",
      "Epoch: 43 - Training Cost: 4.122140884399414  Testing Cost: 4.097047805786133\n",
      "Epoch: 44 - Training Cost: 4.180649280548096  Testing Cost: 4.1549272537231445\n",
      "Epoch: 45 - Training Cost: 4.264981746673584  Testing Cost: 4.238735675811768\n",
      "Epoch: 46 - Training Cost: 4.352570056915283  Testing Cost: 4.325916290283203\n",
      "Epoch: 47 - Training Cost: 4.426794528961182  Testing Cost: 4.399868011474609\n",
      "Epoch: 48 - Training Cost: 4.477163314819336  Testing Cost: 4.450072288513184\n",
      "Epoch: 49 - Training Cost: 4.498693466186523  Testing Cost: 4.471569061279297\n",
      "Epoch: 50 - Training Cost: 4.490949630737305  Testing Cost: 4.463898181915283\n",
      "Epoch: 51 - Training Cost: 4.45711612701416  Testing Cost: 4.430265426635742\n",
      "Epoch: 52 - Training Cost: 4.403204917907715  Testing Cost: 4.376675128936768\n",
      "Epoch: 53 - Training Cost: 4.336851596832275  Testing Cost: 4.31072998046875\n",
      "Epoch: 54 - Training Cost: 4.266255855560303  Testing Cost: 4.240621089935303\n",
      "Epoch: 55 - Training Cost: 4.19936990737915  Testing Cost: 4.1742963790893555\n",
      "Epoch: 56 - Training Cost: 4.14295768737793  Testing Cost: 4.118492126464844\n",
      "Epoch: 57 - Training Cost: 4.101843357086182  Testing Cost: 4.078011512756348\n",
      "Epoch: 58 - Training Cost: 4.078329563140869  Testing Cost: 4.05515193939209\n",
      "Epoch: 59 - Training Cost: 4.071935176849365  Testing Cost: 4.049405097961426\n",
      "Epoch: 60 - Training Cost: 4.079586505889893  Testing Cost: 4.057663440704346\n",
      "Epoch: 61 - Training Cost: 4.096213340759277  Testing Cost: 4.074847221374512\n",
      "Epoch: 62 - Training Cost: 4.115781784057617  Testing Cost: 4.094906806945801\n",
      "Epoch: 63 - Training Cost: 4.132519245147705  Testing Cost: 4.112061500549316\n",
      "Epoch: 64 - Training Cost: 4.142086982727051  Testing Cost: 4.121959686279297\n",
      "Epoch: 65 - Training Cost: 4.142338752746582  Testing Cost: 4.1224493980407715\n",
      "Epoch: 66 - Training Cost: 4.13344669342041  Testing Cost: 4.113708019256592\n",
      "Epoch: 67 - Training Cost: 4.117493629455566  Testing Cost: 4.097834587097168\n",
      "Epoch: 68 - Training Cost: 4.097671031951904  Testing Cost: 4.0780029296875\n",
      "Epoch: 69 - Training Cost: 4.0773749351501465  Testing Cost: 4.05766487121582\n",
      "Epoch: 70 - Training Cost: 4.059475421905518  Testing Cost: 4.039679527282715\n",
      "Epoch: 71 - Training Cost: 4.045843601226807  Testing Cost: 4.0259504318237305\n",
      "Epoch: 72 - Training Cost: 4.037182331085205  Testing Cost: 4.017196178436279\n",
      "Epoch: 73 - Training Cost: 4.033130168914795  Testing Cost: 4.013050079345703\n",
      "Epoch: 74 - Training Cost: 4.032544136047363  Testing Cost: 4.012395858764648\n",
      "Epoch: 75 - Training Cost: 4.033884048461914  Testing Cost: 4.013698101043701\n",
      "Epoch: 76 - Training Cost: 4.0355610847473145  Testing Cost: 4.015401840209961\n",
      "Epoch: 77 - Training Cost: 4.036261558532715  Testing Cost: 4.016225337982178\n",
      "Epoch: 78 - Training Cost: 4.0351104736328125  Testing Cost: 4.015268802642822\n",
      "Epoch: 79 - Training Cost: 4.031766414642334  Testing Cost: 4.012165546417236\n",
      "Epoch: 80 - Training Cost: 4.026369571685791  Testing Cost: 4.007083892822266\n",
      "Epoch: 81 - Training Cost: 4.0194830894470215  Testing Cost: 4.000563621520996\n",
      "Epoch: 82 - Training Cost: 4.011871337890625  Testing Cost: 3.9933385848999023\n",
      "Epoch: 83 - Training Cost: 4.004443168640137  Testing Cost: 3.98626446723938\n",
      "Epoch: 84 - Training Cost: 3.997995138168335  Testing Cost: 3.9801266193389893\n",
      "Epoch: 85 - Training Cost: 3.9929981231689453  Testing Cost: 3.975417375564575\n",
      "Epoch: 86 - Training Cost: 3.9895334243774414  Testing Cost: 3.9722564220428467\n",
      "Epoch: 87 - Training Cost: 3.987335205078125  Testing Cost: 3.9704060554504395\n",
      "Epoch: 88 - Training Cost: 3.985870361328125  Testing Cost: 3.969301462173462\n",
      "Epoch: 89 - Training Cost: 3.9844706058502197  Testing Cost: 3.9681975841522217\n",
      "Epoch: 90 - Training Cost: 3.9825241565704346  Testing Cost: 3.966480255126953\n",
      "Epoch: 91 - Training Cost: 3.9796862602233887  Testing Cost: 3.963775873184204\n",
      "Epoch: 92 - Training Cost: 3.9759232997894287  Testing Cost: 3.9600865840911865\n",
      "Epoch: 93 - Training Cost: 3.9714841842651367  Testing Cost: 3.955657958984375\n",
      "Epoch: 94 - Training Cost: 3.9667253494262695  Testing Cost: 3.950868844985962\n",
      "Epoch: 95 - Training Cost: 3.9620819091796875  Testing Cost: 3.9461617469787598\n",
      "Epoch: 96 - Training Cost: 3.9578444957733154  Testing Cost: 3.941859722137451\n",
      "Epoch: 97 - Training Cost: 3.954157590866089  Testing Cost: 3.938133955001831\n",
      "Epoch: 98 - Training Cost: 3.950965404510498  Testing Cost: 3.934927225112915\n",
      "Epoch: 99 - Training Cost: 3.9480674266815186  Testing Cost: 3.932061195373535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 - Training Cost: 3.9452521800994873  Testing Cost: 3.9292945861816406\n",
      "Epoch: 101 - Training Cost: 3.942317008972168  Testing Cost: 3.9264471530914307\n",
      "Epoch: 102 - Training Cost: 3.939133644104004  Testing Cost: 3.923391819000244\n",
      "Epoch: 103 - Training Cost: 3.935695171356201  Testing Cost: 3.920125961303711\n",
      "Epoch: 104 - Training Cost: 3.932080030441284  Testing Cost: 3.9167191982269287\n",
      "Epoch: 105 - Training Cost: 3.9284234046936035  Testing Cost: 3.9133358001708984\n",
      "Epoch: 106 - Training Cost: 3.924884796142578  Testing Cost: 3.910135507583618\n",
      "Epoch: 107 - Training Cost: 3.9215681552886963  Testing Cost: 3.9071736335754395\n",
      "Epoch: 108 - Training Cost: 3.9184811115264893  Testing Cost: 3.9044713973999023\n",
      "Epoch: 109 - Training Cost: 3.915571689605713  Testing Cost: 3.901921033859253\n",
      "Epoch: 110 - Training Cost: 3.9127094745635986  Testing Cost: 3.8993585109710693\n",
      "Epoch: 111 - Training Cost: 3.9097588062286377  Testing Cost: 3.8966588973999023\n",
      "Epoch: 112 - Training Cost: 3.906644821166992  Testing Cost: 3.8937675952911377\n",
      "Epoch: 113 - Training Cost: 3.9033620357513428  Testing Cost: 3.8906517028808594\n",
      "Epoch: 114 - Training Cost: 3.8999972343444824  Testing Cost: 3.887394428253174\n",
      "Epoch: 115 - Training Cost: 3.896655559539795  Testing Cost: 3.8841400146484375\n",
      "Epoch: 116 - Training Cost: 3.893429756164551  Testing Cost: 3.8809938430786133\n",
      "Epoch: 117 - Training Cost: 3.8903303146362305  Testing Cost: 3.8779940605163574\n",
      "Epoch: 118 - Training Cost: 3.8873281478881836  Testing Cost: 3.875103235244751\n",
      "Epoch: 119 - Training Cost: 3.8843696117401123  Testing Cost: 3.872276782989502\n",
      "Epoch: 120 - Training Cost: 3.8813791275024414  Testing Cost: 3.869478464126587\n",
      "Epoch: 121 - Training Cost: 3.878333806991577  Testing Cost: 3.866638422012329\n",
      "Epoch: 122 - Training Cost: 3.875236749649048  Testing Cost: 3.8637731075286865\n",
      "Epoch: 123 - Training Cost: 3.872114658355713  Testing Cost: 3.8609213829040527\n",
      "Epoch: 124 - Training Cost: 3.8690226078033447  Testing Cost: 3.8581106662750244\n",
      "Epoch: 125 - Training Cost: 3.8659777641296387  Testing Cost: 3.855355978012085\n",
      "Epoch: 126 - Training Cost: 3.8629722595214844  Testing Cost: 3.852647542953491\n",
      "Epoch: 127 - Training Cost: 3.85996413230896  Testing Cost: 3.8499279022216797\n",
      "Epoch: 128 - Training Cost: 3.856919288635254  Testing Cost: 3.8471295833587646\n",
      "Epoch: 129 - Training Cost: 3.853820323944092  Testing Cost: 3.844271183013916\n",
      "Epoch: 130 - Training Cost: 3.8506898880004883  Testing Cost: 3.841362714767456\n",
      "Epoch: 131 - Training Cost: 3.847546339035034  Testing Cost: 3.838438034057617\n",
      "Epoch: 132 - Training Cost: 3.8444323539733887  Testing Cost: 3.8355517387390137\n",
      "Epoch: 133 - Training Cost: 3.8413493633270264  Testing Cost: 3.8327057361602783\n",
      "Epoch: 134 - Training Cost: 3.8382718563079834  Testing Cost: 3.8298823833465576\n",
      "Epoch: 135 - Training Cost: 3.8351852893829346  Testing Cost: 3.827078104019165\n",
      "Epoch: 136 - Training Cost: 3.8320958614349365  Testing Cost: 3.8243019580841064\n",
      "Epoch: 137 - Training Cost: 3.829017162322998  Testing Cost: 3.8215668201446533\n",
      "Epoch: 138 - Training Cost: 3.825958251953125  Testing Cost: 3.818861246109009\n",
      "Epoch: 139 - Training Cost: 3.8229196071624756  Testing Cost: 3.8161439895629883\n",
      "Epoch: 140 - Training Cost: 3.8198845386505127  Testing Cost: 3.8134164810180664\n",
      "Epoch: 141 - Training Cost: 3.8168506622314453  Testing Cost: 3.8106849193573\n",
      "Epoch: 142 - Training Cost: 3.8138208389282227  Testing Cost: 3.80794620513916\n",
      "Epoch: 143 - Training Cost: 3.810791492462158  Testing Cost: 3.8051910400390625\n",
      "Epoch: 144 - Training Cost: 3.807769298553467  Testing Cost: 3.8024446964263916\n",
      "Epoch: 145 - Training Cost: 3.804765462875366  Testing Cost: 3.799700975418091\n",
      "Epoch: 146 - Training Cost: 3.8017752170562744  Testing Cost: 3.796968698501587\n",
      "Epoch: 147 - Training Cost: 3.7987892627716064  Testing Cost: 3.794264793395996\n",
      "Epoch: 148 - Training Cost: 3.795807123184204  Testing Cost: 3.7915830612182617\n",
      "Epoch: 149 - Training Cost: 3.7928178310394287  Testing Cost: 3.788895606994629\n",
      "Epoch: 150 - Training Cost: 3.7898378372192383  Testing Cost: 3.786237955093384\n",
      "Epoch: 151 - Training Cost: 3.7868804931640625  Testing Cost: 3.783597469329834\n",
      "Epoch: 152 - Training Cost: 3.7839481830596924  Testing Cost: 3.780959367752075\n",
      "Epoch: 153 - Training Cost: 3.7810306549072266  Testing Cost: 3.7783098220825195\n",
      "Epoch: 154 - Training Cost: 3.7781217098236084  Testing Cost: 3.7756571769714355\n",
      "Epoch: 155 - Training Cost: 3.7752132415771484  Testing Cost: 3.773000478744507\n",
      "Epoch: 156 - Training Cost: 3.7723169326782227  Testing Cost: 3.7703261375427246\n",
      "Epoch: 157 - Training Cost: 3.7694239616394043  Testing Cost: 3.7676405906677246\n",
      "Epoch: 158 - Training Cost: 3.7665328979492188  Testing Cost: 3.7649729251861572\n",
      "Epoch: 159 - Training Cost: 3.7636499404907227  Testing Cost: 3.7623214721679688\n",
      "Epoch: 160 - Training Cost: 3.760775566101074  Testing Cost: 3.7596664428710938\n",
      "Epoch: 161 - Training Cost: 3.757913827896118  Testing Cost: 3.7570104598999023\n",
      "Epoch: 162 - Training Cost: 3.7550673484802246  Testing Cost: 3.7543416023254395\n",
      "Epoch: 163 - Training Cost: 3.752228021621704  Testing Cost: 3.7516610622406006\n",
      "Epoch: 164 - Training Cost: 3.7494020462036133  Testing Cost: 3.748985767364502\n",
      "Epoch: 165 - Training Cost: 3.7465906143188477  Testing Cost: 3.746342658996582\n",
      "Epoch: 166 - Training Cost: 3.743788957595825  Testing Cost: 3.7437212467193604\n",
      "Epoch: 167 - Training Cost: 3.741002321243286  Testing Cost: 3.7411177158355713\n",
      "Epoch: 168 - Training Cost: 3.7382261753082275  Testing Cost: 3.7385284900665283\n",
      "Epoch: 169 - Training Cost: 3.7354695796966553  Testing Cost: 3.7359530925750732\n",
      "Epoch: 170 - Training Cost: 3.73272967338562  Testing Cost: 3.7333908081054688\n",
      "Epoch: 171 - Training Cost: 3.7300076484680176  Testing Cost: 3.730853796005249\n",
      "Epoch: 172 - Training Cost: 3.7272980213165283  Testing Cost: 3.7283270359039307\n",
      "Epoch: 173 - Training Cost: 3.7245991230010986  Testing Cost: 3.725800037384033\n",
      "Epoch: 174 - Training Cost: 3.7219111919403076  Testing Cost: 3.7232861518859863\n",
      "Epoch: 175 - Training Cost: 3.719224452972412  Testing Cost: 3.7207939624786377\n",
      "Epoch: 176 - Training Cost: 3.716543197631836  Testing Cost: 3.718327283859253\n",
      "Epoch: 177 - Training Cost: 3.7138710021972656  Testing Cost: 3.715879440307617\n",
      "Epoch: 178 - Training Cost: 3.7112066745758057  Testing Cost: 3.713426113128662\n",
      "Epoch: 179 - Training Cost: 3.7085719108581543  Testing Cost: 3.711015462875366\n",
      "Epoch: 180 - Training Cost: 3.7059803009033203  Testing Cost: 3.7086329460144043\n",
      "Epoch: 181 - Training Cost: 3.7034313678741455  Testing Cost: 3.706291913986206\n",
      "Epoch: 182 - Training Cost: 3.700914144515991  Testing Cost: 3.7039871215820312\n",
      "Epoch: 183 - Training Cost: 3.698408842086792  Testing Cost: 3.7016654014587402\n",
      "Epoch: 184 - Training Cost: 3.695911169052124  Testing Cost: 3.699337959289551\n",
      "Epoch: 185 - Training Cost: 3.6934256553649902  Testing Cost: 3.6970055103302\n",
      "Epoch: 186 - Training Cost: 3.6909589767456055  Testing Cost: 3.694690465927124\n",
      "Epoch: 187 - Training Cost: 3.6885054111480713  Testing Cost: 3.692380905151367\n",
      "Epoch: 188 - Training Cost: 3.6860623359680176  Testing Cost: 3.6900711059570312\n",
      "Epoch: 189 - Training Cost: 3.6836342811584473  Testing Cost: 3.6877760887145996\n",
      "Epoch: 190 - Training Cost: 3.6812260150909424  Testing Cost: 3.6854851245880127\n",
      "Epoch: 191 - Training Cost: 3.678819179534912  Testing Cost: 3.6831822395324707\n",
      "Epoch: 192 - Training Cost: 3.6764328479766846  Testing Cost: 3.6808876991271973\n",
      "Epoch: 193 - Training Cost: 3.674071788787842  Testing Cost: 3.6785964965820312\n",
      "Epoch: 194 - Training Cost: 3.6717422008514404  Testing Cost: 3.6763486862182617\n",
      "Epoch: 195 - Training Cost: 3.6694374084472656  Testing Cost: 3.6741347312927246\n",
      "Epoch: 196 - Training Cost: 3.667146921157837  Testing Cost: 3.6719281673431396\n",
      "Epoch: 197 - Training Cost: 3.6648671627044678  Testing Cost: 3.669728994369507\n",
      "Epoch: 198 - Training Cost: 3.662592887878418  Testing Cost: 3.667534828186035\n",
      "Epoch: 199 - Training Cost: 3.660334587097168  Testing Cost: 3.6653473377227783\n",
      "Epoch: 200 - Training Cost: 3.6581039428710938  Testing Cost: 3.6631715297698975\n",
      "Epoch: 201 - Training Cost: 3.655900716781616  Testing Cost: 3.6610496044158936\n",
      "Epoch: 202 - Training Cost: 3.6537094116210938  Testing Cost: 3.6589419841766357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 203 - Training Cost: 3.651546001434326  Testing Cost: 3.6568591594696045\n",
      "Epoch: 204 - Training Cost: 3.6494104862213135  Testing Cost: 3.6547844409942627\n",
      "Epoch: 205 - Training Cost: 3.6472911834716797  Testing Cost: 3.652735948562622\n",
      "Epoch: 206 - Training Cost: 3.6451871395111084  Testing Cost: 3.6507108211517334\n",
      "Epoch: 207 - Training Cost: 3.643109083175659  Testing Cost: 3.6486923694610596\n",
      "Epoch: 208 - Training Cost: 3.6410632133483887  Testing Cost: 3.6466896533966064\n",
      "Epoch: 209 - Training Cost: 3.639039993286133  Testing Cost: 3.6447393894195557\n",
      "Epoch: 210 - Training Cost: 3.6370351314544678  Testing Cost: 3.642815113067627\n",
      "Epoch: 211 - Training Cost: 3.6350455284118652  Testing Cost: 3.6408891677856445\n",
      "Epoch: 212 - Training Cost: 3.6330740451812744  Testing Cost: 3.638962745666504\n",
      "Epoch: 213 - Training Cost: 3.6311163902282715  Testing Cost: 3.637035846710205\n",
      "Epoch: 214 - Training Cost: 3.629157066345215  Testing Cost: 3.635093927383423\n",
      "Epoch: 215 - Training Cost: 3.6272082328796387  Testing Cost: 3.633148670196533\n",
      "Epoch: 216 - Training Cost: 3.6252706050872803  Testing Cost: 3.6312003135681152\n",
      "Epoch: 217 - Training Cost: 3.623335838317871  Testing Cost: 3.629246473312378\n",
      "Epoch: 218 - Training Cost: 3.6214044094085693  Testing Cost: 3.627295732498169\n",
      "Epoch: 219 - Training Cost: 3.619476318359375  Testing Cost: 3.6253321170806885\n",
      "Epoch: 220 - Training Cost: 3.617558002471924  Testing Cost: 3.623382568359375\n",
      "Epoch: 221 - Training Cost: 3.615647077560425  Testing Cost: 3.6214122772216797\n",
      "Epoch: 222 - Training Cost: 3.6137375831604004  Testing Cost: 3.619410276412964\n",
      "Epoch: 223 - Training Cost: 3.611844539642334  Testing Cost: 3.6174068450927734\n",
      "Epoch: 224 - Training Cost: 3.609959840774536  Testing Cost: 3.615384817123413\n",
      "Epoch: 225 - Training Cost: 3.6080777645111084  Testing Cost: 3.6133460998535156\n",
      "Epoch: 226 - Training Cost: 3.606189250946045  Testing Cost: 3.6113250255584717\n",
      "Epoch: 227 - Training Cost: 3.6043076515197754  Testing Cost: 3.609334945678711\n",
      "Epoch: 228 - Training Cost: 3.6024317741394043  Testing Cost: 3.607356309890747\n",
      "Epoch: 229 - Training Cost: 3.6005501747131348  Testing Cost: 3.6053872108459473\n",
      "Epoch: 230 - Training Cost: 3.5986685752868652  Testing Cost: 3.6034135818481445\n",
      "Epoch: 231 - Training Cost: 3.5968050956726074  Testing Cost: 3.601447343826294\n",
      "Epoch: 232 - Training Cost: 3.594958782196045  Testing Cost: 3.599526882171631\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "#model parameters\n",
    "t = time.time()\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 5000\n",
    "display_step = 1\n",
    "\n",
    "#  inputs and outputs in our neural network\n",
    "number_of_inputs = 70\n",
    "number_of_outputs = 1\n",
    "\n",
    "# Define how many neurons we want in each layer of our neural network\n",
    "layer_1_nodes = 512\n",
    "layer_2_nodes = 1024\n",
    "layer_3_nodes = 1024\n",
    "layer_4_nodes = 512\n",
    "\n",
    "RUN_NAME = str(int(round(t * 1000))) + '_' + str(layer_1_nodes) + '_' + str(layer_2_nodes) + '_' + str(layer_3_nodes) + '_' + str(layer_4_nodes) + '_' + str(learning_rate) + '_' + str(training_epochs) + '_' + 'Val'\n",
    "\n",
    "\n",
    "# Input Layer\n",
    "with tf.variable_scope('input'):\n",
    "    X = tf.placeholder(tf.float32, shape=(None, number_of_inputs))\n",
    "\n",
    "# Layer 1\n",
    "with tf.variable_scope('layer_1'):\n",
    "    weights = tf.get_variable(\"weights1\", shape=[number_of_inputs, layer_1_nodes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases1\", shape=[layer_1_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_1_output = tf.nn.relu(tf.matmul(X, weights) + biases)\n",
    "\n",
    "# Layer 2\n",
    "with tf.variable_scope('layer_2'):\n",
    "    weights = tf.get_variable(\"weights2\", shape=[layer_1_nodes, layer_2_nodes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases2\", shape=[layer_2_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_2_output = tf.nn.relu(tf.matmul(layer_1_output, weights) + biases)\n",
    "\n",
    "# Layer 3\n",
    "with tf.variable_scope('layer_3'):\n",
    "    weights = tf.get_variable(\"weights3\", shape=[layer_2_nodes, layer_3_nodes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases3\", shape=[layer_3_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_3_output = tf.nn.relu(tf.matmul(layer_2_output, weights) + biases)\n",
    "\n",
    "# Layer 4\n",
    "with tf.variable_scope('layer_4'):\n",
    "    weights = tf.get_variable(\"weights4\", shape=[layer_3_nodes, layer_4_nodes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases4\", shape=[layer_4_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_4_output = tf.nn.relu(tf.matmul(layer_3_output, weights) + biases)\n",
    "\n",
    "# Output Layer\n",
    "with tf.variable_scope('output'):\n",
    "    weights = tf.get_variable(\"weights5\", shape=[layer_4_nodes, number_of_outputs], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases5\", shape=[number_of_outputs], initializer=tf.zeros_initializer())\n",
    "    prediction = tf.matmul(layer_4_output, weights) + biases\n",
    "\n",
    "# Cost function\n",
    "\n",
    "with tf.variable_scope('cost'):\n",
    "    Y = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "    cost = tf.reduce_mean(tf.squared_difference(prediction, Y))\n",
    "\n",
    "#optimizer function\n",
    "\n",
    "with tf.variable_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# log summary of training\n",
    "with tf.variable_scope('logging'):\n",
    "    tf.summary.scalar('current_cost', cost)\n",
    "    summary = tf.summary.merge_all()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "with tf.Session() as session:\n",
    "\n",
    " \n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    # create log files\n",
    "    training_writer = tf.summary.FileWriter(\"./{}/logs/training\".format(RUN_NAME), session.graph)\n",
    "    testing_writer = tf.summary.FileWriter(\"./{}/logs/testing\".format(RUN_NAME), session.graph)\n",
    "\n",
    "    # Run the optimizer over and over to train the network.\n",
    "    for epoch in range(training_epochs):\n",
    "\n",
    "        # Feed in the training data and do one step of neural network training\n",
    "        session.run(optimizer, feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n",
    "\n",
    "        # Every few training steps, log our progress\n",
    "        if epoch % display_step == 0:\n",
    "            # Get the current accuracy scores by running the \"cost\" operation on the training and test data sets\n",
    "            training_cost, training_summary = session.run([cost, summary], feed_dict={X: X_scaled_training, Y:Y_scaled_training})\n",
    "            testing_cost, testing_summary = session.run([cost, summary], feed_dict={X: X_scaled_testing, Y:Y_scaled_testing})\n",
    "\n",
    "            # Write the current training status to the log files \n",
    "            training_writer.add_summary(training_summary, epoch)\n",
    "            testing_writer.add_summary(testing_summary, epoch)\n",
    "\n",
    "            # Print the current training status to the screen\n",
    "            print(\"Epoch: {} - Training Cost: {}  Testing Cost: {}\".format(epoch, training_cost, testing_cost))\n",
    "\n",
    "    # Training is now complete!\n",
    "\n",
    "    # Get the final accuracy scores by running the \"cost\" operation on the training and test data sets\n",
    "    final_training_cost = session.run(cost, feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n",
    "    final_testing_cost = session.run(cost, feed_dict={X: X_scaled_testing, Y: Y_scaled_testing})\n",
    "\n",
    "    print(\"Final Training cost: {}\".format(final_training_cost))\n",
    "    print(\"Final Testing cost: {}\".format(final_testing_cost))\n",
    "\n",
    "    save_path = saver.save(session, \"./{}/logs/trained_model.ckpt\".format(RUN_NAME))\n",
    "    print(\"Model saved: {}\".format(save_path))\n",
    "\n",
    "\n",
    "    model_builder = tf.saved_model.builder.SavedModelBuilder(\"./{}/exported_model\".format(RUN_NAME))\n",
    "\n",
    "    inputs = {\n",
    "        'input': tf.saved_model.utils.build_tensor_info(X)\n",
    "        }\n",
    "    outputs = {\n",
    "        'earnings': tf.saved_model.utils.build_tensor_info(prediction)\n",
    "        }\n",
    "\n",
    "    signature_def = tf.saved_model.signature_def_utils.build_signature_def(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME\n",
    "    )\n",
    "\n",
    "    model_builder.add_meta_graph_and_variables(\n",
    "        session,\n",
    "        tags=[tf.saved_model.tag_constants.SERVING],\n",
    "        signature_def_map={\n",
    "            tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature_def\n",
    "        }\n",
    "    )\n",
    "\n",
    "    model_builder.save()\n",
    "    print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model_path = \"./1587454858083_512_1024_1024_512_0.0001_5000_Val/logs/trained_model.ckpt\"\n",
    "detection_graph = tf.Graph()\n",
    "with tf.Session(graph=detection_graph) as sess:\n",
    "    # Load the graph with the trained states\n",
    "    loader = tf.train.import_meta_graph(model_path+'.meta')\n",
    "    loader.restore(sess, model_path)\n",
    "    \n",
    "    Y_predicted_scaled = sess.run(prediction, feed_dict={X: X_scaled_testing})\n",
    "    # Unscale the data back to it's original units (dollars)\n",
    "    Y_predicted = Y_scaler.inverse_transform(Y_predicted_scaled)\n",
    "    real_earnings = test_data_df['total_earnings'].values[0]\n",
    "    predicted_earnings = Y_predicted[0][0]\n",
    "    print(\"The actual earnings of Game #1 were ${}\".format(real_earnings))\n",
    "    print(\"Our neural network predicted earnings of ${}\".format(predicted_earnings))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from b'./1587454858083_512_1024_1024_512_0.0001_5000_Val/exported_model/variables\\\\variables'\n",
      "[<tf.Operation 'save_1/RestoreV2_31/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_31/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_30/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_30/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_29/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_29/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_28/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_28/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_27/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_27/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_26/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_26/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_25/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_25/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_24/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_24/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_23/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_23/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_22/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_22/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_21/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_21/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_20/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_20/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_19/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_19/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_18/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_18/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_17/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_17/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_16/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_16/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_15/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_15/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_14/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_14/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_13/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_13/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_12/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_12/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_11/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_11/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_10/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_10/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_9/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_9/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_8/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_8/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_7/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_7/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_6/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_6/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_5/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_5/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_4/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_4/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_3/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_3/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_2/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_2/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2_1/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2_1/tensor_names' type=Const>, <tf.Operation 'save_1/RestoreV2/shape_and_slices' type=Const>, <tf.Operation 'save_1/RestoreV2/tensor_names' type=Const>, <tf.Operation 'save_1/SaveV2/shape_and_slices' type=Const>, <tf.Operation 'save_1/SaveV2/tensor_names' type=Const>, <tf.Operation 'save_1/ShardedFilename/shard' type=Const>, <tf.Operation 'save_1/num_shards' type=Const>, <tf.Operation 'save_1/StringJoin/inputs_1' type=Const>, <tf.Operation 'save_1/Const' type=Const>, <tf.Operation 'save_1/RestoreV2_31' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_30' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_29' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_28' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_27' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_26' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_25' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_24' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_23' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_22' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_21' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_20' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_19' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_18' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_17' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_16' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_15' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_14' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_13' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_12' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_11' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_10' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_9' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_8' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_7' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_6' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_5' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_4' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_3' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_2' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2_1' type=RestoreV2>, <tf.Operation 'save_1/RestoreV2' type=RestoreV2>, <tf.Operation 'save_1/StringJoin' type=StringJoin>, <tf.Operation 'save_1/ShardedFilename' type=ShardedFilename>, <tf.Operation 'save/RestoreV2_31/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_31/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_30/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_30/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_29/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_29/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_28/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_28/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_27/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_27/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_26/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_26/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_25/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_25/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_24/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_24/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_23/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_23/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_22/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_22/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_21/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_21/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_20/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_20/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_19/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_19/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_18/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_18/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_17/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_17/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_16/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_16/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_15/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_15/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_14/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_14/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_13/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_13/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_12/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_12/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_11/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_11/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_10/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_10/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_9/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_9/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_8/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_8/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_7/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_7/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_6/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_6/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_5/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_5/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_4/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_4/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_3/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_3/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_2/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_2/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2_1/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2_1/tensor_names' type=Const>, <tf.Operation 'save/RestoreV2/shape_and_slices' type=Const>, <tf.Operation 'save/RestoreV2/tensor_names' type=Const>, <tf.Operation 'save/SaveV2/shape_and_slices' type=Const>, <tf.Operation 'save/SaveV2/tensor_names' type=Const>, <tf.Operation 'save/Const' type=Const>, <tf.Operation 'save/RestoreV2_31' type=RestoreV2>, <tf.Operation 'save/RestoreV2_30' type=RestoreV2>, <tf.Operation 'save/RestoreV2_29' type=RestoreV2>, <tf.Operation 'save/RestoreV2_28' type=RestoreV2>, <tf.Operation 'save/RestoreV2_27' type=RestoreV2>, <tf.Operation 'save/RestoreV2_26' type=RestoreV2>, <tf.Operation 'save/RestoreV2_25' type=RestoreV2>, <tf.Operation 'save/RestoreV2_24' type=RestoreV2>, <tf.Operation 'save/RestoreV2_23' type=RestoreV2>, <tf.Operation 'save/RestoreV2_22' type=RestoreV2>, <tf.Operation 'save/RestoreV2_21' type=RestoreV2>, <tf.Operation 'save/RestoreV2_20' type=RestoreV2>, <tf.Operation 'save/RestoreV2_19' type=RestoreV2>, <tf.Operation 'save/RestoreV2_18' type=RestoreV2>, <tf.Operation 'save/RestoreV2_17' type=RestoreV2>, <tf.Operation 'save/RestoreV2_16' type=RestoreV2>, <tf.Operation 'save/RestoreV2_15' type=RestoreV2>, <tf.Operation 'save/RestoreV2_14' type=RestoreV2>, <tf.Operation 'save/RestoreV2_13' type=RestoreV2>, <tf.Operation 'save/RestoreV2_12' type=RestoreV2>, <tf.Operation 'save/RestoreV2_11' type=RestoreV2>, <tf.Operation 'save/RestoreV2_10' type=RestoreV2>, <tf.Operation 'save/RestoreV2_9' type=RestoreV2>, <tf.Operation 'save/RestoreV2_8' type=RestoreV2>, <tf.Operation 'save/RestoreV2_7' type=RestoreV2>, <tf.Operation 'save/RestoreV2_6' type=RestoreV2>, <tf.Operation 'save/RestoreV2_5' type=RestoreV2>, <tf.Operation 'save/RestoreV2_4' type=RestoreV2>, <tf.Operation 'save/RestoreV2_3' type=RestoreV2>, <tf.Operation 'save/RestoreV2_2' type=RestoreV2>, <tf.Operation 'save/RestoreV2_1' type=RestoreV2>, <tf.Operation 'save/RestoreV2' type=RestoreV2>, <tf.Operation 'logging/current_cost/tags' type=Const>, <tf.Operation 'train/Adam/epsilon' type=Const>, <tf.Operation 'train/Adam/beta2' type=Const>, <tf.Operation 'train/Adam/beta1' type=Const>, <tf.Operation 'train/Adam/learning_rate' type=Const>, <tf.Operation 'train/output/biases5/Adam_1' type=VariableV2>, <tf.Operation 'save_1/Assign_29' type=Assign>, <tf.Operation 'save/Assign_29' type=Assign>, <tf.Operation 'train/output/biases5/Adam_1/read' type=Identity>, <tf.Operation 'train/output/biases5/Adam_1/Initializer/zeros' type=Const>, <tf.Operation 'train/output/biases5/Adam_1/Assign' type=Assign>, <tf.Operation 'train/output/biases5/Adam' type=VariableV2>, <tf.Operation 'save_1/Assign_28' type=Assign>, <tf.Operation 'save/Assign_28' type=Assign>, <tf.Operation 'train/output/biases5/Adam/read' type=Identity>, <tf.Operation 'train/output/biases5/Adam/Initializer/zeros' type=Const>, <tf.Operation 'train/output/biases5/Adam/Assign' type=Assign>, <tf.Operation 'train/output/weights5/Adam_1' type=VariableV2>, <tf.Operation 'save_1/Assign_31' type=Assign>, <tf.Operation 'save/Assign_31' type=Assign>, <tf.Operation 'train/output/weights5/Adam_1/read' type=Identity>, <tf.Operation 'train/output/weights5/Adam_1/Initializer/zeros' type=Const>, <tf.Operation 'train/output/weights5/Adam_1/Assign' type=Assign>, <tf.Operation 'train/output/weights5/Adam' type=VariableV2>, <tf.Operation 'save_1/Assign_30' type=Assign>, <tf.Operation 'save/Assign_30' type=Assign>, <tf.Operation 'train/output/weights5/Adam/read' type=Identity>, <tf.Operation 'train/output/weights5/Adam/Initializer/zeros' type=Const>, <tf.Operation 'train/output/weights5/Adam/Assign' type=Assign>, <tf.Operation 'train/layer_4/biases4/Adam_1' type=VariableV2>, <tf.Operation 'save_1/Assign_25' type=Assign>, <tf.Operation 'save/Assign_25' type=Assign>, <tf.Operation 'train/layer_4/biases4/Adam_1/read' type=Identity>, <tf.Operation 'train/layer_4/biases4/Adam_1/Initializer/zeros' type=Const>, <tf.Operation 'train/layer_4/biases4/Adam_1/Assign' type=Assign>, <tf.Operation 'train/layer_4/biases4/Adam' type=VariableV2>, <tf.Operation 'save_1/Assign_24' type=Assign>, <tf.Operation 'save/Assign_24' type=Assign>, <tf.Operation 'train/layer_4/biases4/Adam/read' type=Identity>, <tf.Operation 'train/layer_4/biases4/Adam/Initializer/zeros' type=Const>, <tf.Operation 'train/layer_4/biases4/Adam/Assign' type=Assign>, <tf.Operation 'train/layer_4/weights4/Adam_1' type=VariableV2>, <tf.Operation 'save_1/Assign_27' type=Assign>, <tf.Operation 'save/Assign_27' type=Assign>, <tf.Operation 'train/layer_4/weights4/Adam_1/read' type=Identity>, <tf.Operation 'train/layer_4/weights4/Adam_1/Initializer/zeros' type=Const>, <tf.Operation 'train/layer_4/weights4/Adam_1/Assign' type=Assign>, <tf.Operation 'train/layer_4/weights4/Adam' type=VariableV2>, <tf.Operation 'save_1/Assign_26' type=Assign>, <tf.Operation 'save/Assign_26' type=Assign>, <tf.Operation 'train/layer_4/weights4/Adam/read' type=Identity>, <tf.Operation 'train/layer_4/weights4/Adam/Initializer/zeros' type=Const>, <tf.Operation 'train/layer_4/weights4/Adam/Assign' type=Assign>, <tf.Operation 'train/layer_3/biases3/Adam_1' type=VariableV2>, <tf.Operation 'save_1/Assign_21' type=Assign>, <tf.Operation 'save/Assign_21' type=Assign>, <tf.Operation 'train/layer_3/biases3/Adam_1/read' type=Identity>, <tf.Operation 'train/layer_3/biases3/Adam_1/Initializer/zeros' type=Const>, <tf.Operation 'train/layer_3/biases3/Adam_1/Assign' type=Assign>, <tf.Operation 'train/layer_3/biases3/Adam' type=VariableV2>, <tf.Operation 'save_1/Assign_20' type=Assign>, <tf.Operation 'save/Assign_20' type=Assign>, <tf.Operation 'train/layer_3/biases3/Adam/read' type=Identity>, <tf.Operation 'train/layer_3/biases3/Adam/Initializer/zeros' type=Const>, <tf.Operation 'train/layer_3/biases3/Adam/Assign' type=Assign>, <tf.Operation 'train/layer_3/weights3/Adam_1' type=VariableV2>, <tf.Operation 'save_1/Assign_23' type=Assign>, <tf.Operation 'save/Assign_23' type=Assign>, <tf.Operation 'train/layer_3/weights3/Adam_1/read' type=Identity>, <tf.Operation 'train/layer_3/weights3/Adam_1/Initializer/zeros' type=Const>, <tf.Operation 'train/layer_3/weights3/Adam_1/Assign' type=Assign>, <tf.Operation 'train/layer_3/weights3/Adam' type=VariableV2>, <tf.Operation 'save_1/Assign_22' type=Assign>, <tf.Operation 'save/Assign_22' type=Assign>, <tf.Operation 'train/layer_3/weights3/Adam/read' type=Identity>, <tf.Operation 'train/layer_3/weights3/Adam/Initializer/zeros' type=Const>, <tf.Operation 'train/layer_3/weights3/Adam/Assign' type=Assign>, <tf.Operation 'train/layer_2/biases2/Adam_1' type=VariableV2>, <tf.Operation 'save_1/Assign_17' type=Assign>, <tf.Operation 'save/Assign_17' type=Assign>, <tf.Operation 'train/layer_2/biases2/Adam_1/read' type=Identity>, <tf.Operation 'train/layer_2/biases2/Adam_1/Initializer/zeros' type=Const>, <tf.Operation 'train/layer_2/biases2/Adam_1/Assign' type=Assign>, <tf.Operation 'train/layer_2/biases2/Adam' type=VariableV2>, <tf.Operation 'save_1/Assign_16' type=Assign>, <tf.Operation 'save/Assign_16' type=Assign>, <tf.Operation 'train/layer_2/biases2/Adam/read' type=Identity>, <tf.Operation 'train/layer_2/biases2/Adam/Initializer/zeros' type=Const>, <tf.Operation 'train/layer_2/biases2/Adam/Assign' type=Assign>, <tf.Operation 'train/layer_2/weights2/Adam_1' type=VariableV2>, <tf.Operation 'save_1/Assign_19' type=Assign>, <tf.Operation 'save/Assign_19' type=Assign>, <tf.Operation 'train/layer_2/weights2/Adam_1/read' type=Identity>, <tf.Operation 'train/layer_2/weights2/Adam_1/Initializer/zeros' type=Const>, <tf.Operation 'train/layer_2/weights2/Adam_1/Assign' type=Assign>, <tf.Operation 'train/layer_2/weights2/Adam' type=VariableV2>, <tf.Operation 'save_1/Assign_18' type=Assign>, <tf.Operation 'save/Assign_18' type=Assign>, <tf.Operation 'train/layer_2/weights2/Adam/read' type=Identity>, <tf.Operation 'train/layer_2/weights2/Adam/Initializer/zeros' type=Const>, <tf.Operation 'train/layer_2/weights2/Adam/Assign' type=Assign>, <tf.Operation 'train/layer_1/biases1/Adam_1' type=VariableV2>, <tf.Operation 'save_1/Assign_13' type=Assign>, <tf.Operation 'save/Assign_13' type=Assign>, <tf.Operation 'train/layer_1/biases1/Adam_1/read' type=Identity>, <tf.Operation 'train/layer_1/biases1/Adam_1/Initializer/zeros' type=Const>, <tf.Operation 'train/layer_1/biases1/Adam_1/Assign' type=Assign>, <tf.Operation 'train/layer_1/biases1/Adam' type=VariableV2>, <tf.Operation 'save_1/Assign_12' type=Assign>, <tf.Operation 'save/Assign_12' type=Assign>, <tf.Operation 'train/layer_1/biases1/Adam/read' type=Identity>, <tf.Operation 'train/layer_1/biases1/Adam/Initializer/zeros' type=Const>, <tf.Operation 'train/layer_1/biases1/Adam/Assign' type=Assign>, <tf.Operation 'train/layer_1/weights1/Adam_1' type=VariableV2>, <tf.Operation 'save_1/Assign_15' type=Assign>, <tf.Operation 'save/Assign_15' type=Assign>, <tf.Operation 'train/layer_1/weights1/Adam_1/read' type=Identity>, <tf.Operation 'train/layer_1/weights1/Adam_1/Initializer/zeros' type=Const>, <tf.Operation 'train/layer_1/weights1/Adam_1/Assign' type=Assign>, <tf.Operation 'train/layer_1/weights1/Adam' type=VariableV2>, <tf.Operation 'save_1/Assign_14' type=Assign>, <tf.Operation 'save/Assign_14' type=Assign>, <tf.Operation 'train/layer_1/weights1/Adam/read' type=Identity>, <tf.Operation 'train/layer_1/weights1/Adam/Initializer/zeros' type=Const>, <tf.Operation 'train/layer_1/weights1/Adam/Assign' type=Assign>, <tf.Operation 'train/beta2_power' type=VariableV2>, <tf.Operation 'save_1/Assign_11' type=Assign>, <tf.Operation 'save/Assign_11' type=Assign>, <tf.Operation 'train/beta2_power/read' type=Identity>, <tf.Operation 'train/beta2_power/initial_value' type=Const>, <tf.Operation 'train/beta2_power/Assign' type=Assign>, <tf.Operation 'train/beta1_power' type=VariableV2>, <tf.Operation 'save_1/Assign_10' type=Assign>, <tf.Operation 'save/Assign_10' type=Assign>, <tf.Operation 'train/beta1_power/read' type=Identity>, <tf.Operation 'train/beta1_power/initial_value' type=Const>, <tf.Operation 'train/beta1_power/Assign' type=Assign>, <tf.Operation 'train/gradients/layer_1/add_grad/Shape_1' type=Const>, <tf.Operation 'train/gradients/layer_2/add_grad/Shape_1' type=Const>, <tf.Operation 'train/gradients/layer_3/add_grad/Shape_1' type=Const>, <tf.Operation 'train/gradients/layer_4/add_grad/Shape_1' type=Const>, <tf.Operation 'train/gradients/output/add_grad/Shape_1' type=Const>, <tf.Operation 'train/gradients/cost/Mean_grad/Maximum/y' type=Const>, <tf.Operation 'train/gradients/cost/Mean_grad/Const_1' type=Const>, <tf.Operation 'train/gradients/cost/Mean_grad/Const' type=Const>, <tf.Operation 'train/gradients/cost/Mean_grad/Shape_2' type=Const>, <tf.Operation 'train/gradients/cost/Mean_grad/Prod_1' type=Prod>, <tf.Operation 'train/gradients/cost/Mean_grad/Maximum' type=Maximum>, <tf.Operation 'train/gradients/cost/Mean_grad/Reshape/shape' type=Const>, <tf.Operation 'train/gradients/Const' type=Const>, <tf.Operation 'train/gradients/Shape' type=Const>, <tf.Operation 'train/gradients/Fill' type=Fill>, <tf.Operation 'train/gradients/cost/Mean_grad/Reshape' type=Reshape>, <tf.Operation 'cost/Const' type=Const>, <tf.Operation 'cost/Placeholder' type=Placeholder>, <tf.Operation 'train/gradients/cost/SquaredDifference_grad/Shape_1' type=Shape>, <tf.Operation 'output/biases5' type=VariableV2>, <tf.Operation 'save_1/Assign_8' type=Assign>, <tf.Operation 'save/Assign_8' type=Assign>, <tf.Operation 'output/biases5/read' type=Identity>, <tf.Operation 'output/biases5/Initializer/zeros' type=Const>, <tf.Operation 'output/biases5/Assign' type=Assign>, <tf.Operation 'output/weights5' type=VariableV2>, <tf.Operation 'save_1/Assign_9' type=Assign>, <tf.Operation 'save/Assign_9' type=Assign>, <tf.Operation 'output/weights5/read' type=Identity>, <tf.Operation 'output/weights5/Initializer/random_uniform/max' type=Const>, <tf.Operation 'output/weights5/Initializer/random_uniform/min' type=Const>, <tf.Operation 'output/weights5/Initializer/random_uniform/sub' type=Sub>, <tf.Operation 'output/weights5/Initializer/random_uniform/shape' type=Const>, <tf.Operation 'output/weights5/Initializer/random_uniform/RandomUniform' type=RandomUniform>, <tf.Operation 'output/weights5/Initializer/random_uniform/mul' type=Mul>, <tf.Operation 'output/weights5/Initializer/random_uniform' type=Add>, <tf.Operation 'output/weights5/Assign' type=Assign>, <tf.Operation 'layer_4/biases4' type=VariableV2>, <tf.Operation 'save_1/Assign_6' type=Assign>, <tf.Operation 'save/Assign_6' type=Assign>, <tf.Operation 'layer_4/biases4/read' type=Identity>, <tf.Operation 'layer_4/biases4/Initializer/zeros' type=Const>, <tf.Operation 'layer_4/biases4/Assign' type=Assign>, <tf.Operation 'layer_4/weights4' type=VariableV2>, <tf.Operation 'save_1/Assign_7' type=Assign>, <tf.Operation 'save/Assign_7' type=Assign>, <tf.Operation 'layer_4/weights4/read' type=Identity>, <tf.Operation 'layer_4/weights4/Initializer/random_uniform/max' type=Const>, <tf.Operation 'layer_4/weights4/Initializer/random_uniform/min' type=Const>, <tf.Operation 'layer_4/weights4/Initializer/random_uniform/sub' type=Sub>, <tf.Operation 'layer_4/weights4/Initializer/random_uniform/shape' type=Const>, <tf.Operation 'layer_4/weights4/Initializer/random_uniform/RandomUniform' type=RandomUniform>, <tf.Operation 'layer_4/weights4/Initializer/random_uniform/mul' type=Mul>, <tf.Operation 'layer_4/weights4/Initializer/random_uniform' type=Add>, <tf.Operation 'layer_4/weights4/Assign' type=Assign>, <tf.Operation 'layer_3/biases3' type=VariableV2>, <tf.Operation 'save_1/Assign_4' type=Assign>, <tf.Operation 'save/Assign_4' type=Assign>, <tf.Operation 'layer_3/biases3/read' type=Identity>, <tf.Operation 'layer_3/biases3/Initializer/zeros' type=Const>, <tf.Operation 'layer_3/biases3/Assign' type=Assign>, <tf.Operation 'layer_3/weights3' type=VariableV2>, <tf.Operation 'save_1/Assign_5' type=Assign>, <tf.Operation 'save/Assign_5' type=Assign>, <tf.Operation 'layer_3/weights3/read' type=Identity>, <tf.Operation 'layer_3/weights3/Initializer/random_uniform/max' type=Const>, <tf.Operation 'layer_3/weights3/Initializer/random_uniform/min' type=Const>, <tf.Operation 'layer_3/weights3/Initializer/random_uniform/sub' type=Sub>, <tf.Operation 'layer_3/weights3/Initializer/random_uniform/shape' type=Const>, <tf.Operation 'layer_3/weights3/Initializer/random_uniform/RandomUniform' type=RandomUniform>, <tf.Operation 'layer_3/weights3/Initializer/random_uniform/mul' type=Mul>, <tf.Operation 'layer_3/weights3/Initializer/random_uniform' type=Add>, <tf.Operation 'layer_3/weights3/Assign' type=Assign>, <tf.Operation 'layer_2/biases2' type=VariableV2>, <tf.Operation 'save_1/Assign_2' type=Assign>, <tf.Operation 'save/Assign_2' type=Assign>, <tf.Operation 'layer_2/biases2/read' type=Identity>, <tf.Operation 'layer_2/biases2/Initializer/zeros' type=Const>, <tf.Operation 'layer_2/biases2/Assign' type=Assign>, <tf.Operation 'layer_2/weights2' type=VariableV2>, <tf.Operation 'save_1/Assign_3' type=Assign>, <tf.Operation 'save/Assign_3' type=Assign>, <tf.Operation 'layer_2/weights2/read' type=Identity>, <tf.Operation 'layer_2/weights2/Initializer/random_uniform/max' type=Const>, <tf.Operation 'layer_2/weights2/Initializer/random_uniform/min' type=Const>, <tf.Operation 'layer_2/weights2/Initializer/random_uniform/sub' type=Sub>, <tf.Operation 'layer_2/weights2/Initializer/random_uniform/shape' type=Const>, <tf.Operation 'layer_2/weights2/Initializer/random_uniform/RandomUniform' type=RandomUniform>, <tf.Operation 'layer_2/weights2/Initializer/random_uniform/mul' type=Mul>, <tf.Operation 'layer_2/weights2/Initializer/random_uniform' type=Add>, <tf.Operation 'layer_2/weights2/Assign' type=Assign>, <tf.Operation 'layer_1/biases1' type=VariableV2>, <tf.Operation 'save_1/Assign' type=Assign>, <tf.Operation 'save/Assign' type=Assign>, <tf.Operation 'layer_1/biases1/read' type=Identity>, <tf.Operation 'layer_1/biases1/Initializer/zeros' type=Const>, <tf.Operation 'layer_1/biases1/Assign' type=Assign>, <tf.Operation 'layer_1/weights1' type=VariableV2>, <tf.Operation 'save_1/Assign_1' type=Assign>, <tf.Operation 'save_1/restore_shard' type=NoOp>, <tf.Operation 'save_1/restore_all' type=NoOp>, <tf.Operation 'save_1/SaveV2' type=SaveV2>, <tf.Operation 'save_1/control_dependency' type=Identity>, <tf.Operation 'save_1/MergeV2Checkpoints/checkpoint_prefixes' type=Pack>, <tf.Operation 'save_1/MergeV2Checkpoints' type=MergeV2Checkpoints>, <tf.Operation 'save_1/Identity' type=Identity>, <tf.Operation 'save/Assign_1' type=Assign>, <tf.Operation 'save/restore_all' type=NoOp>, <tf.Operation 'save/SaveV2' type=SaveV2>, <tf.Operation 'save/control_dependency' type=Identity>, <tf.Operation 'layer_1/weights1/read' type=Identity>, <tf.Operation 'layer_1/weights1/Initializer/random_uniform/max' type=Const>, <tf.Operation 'layer_1/weights1/Initializer/random_uniform/min' type=Const>, <tf.Operation 'layer_1/weights1/Initializer/random_uniform/sub' type=Sub>, <tf.Operation 'layer_1/weights1/Initializer/random_uniform/shape' type=Const>, <tf.Operation 'layer_1/weights1/Initializer/random_uniform/RandomUniform' type=RandomUniform>, <tf.Operation 'layer_1/weights1/Initializer/random_uniform/mul' type=Mul>, <tf.Operation 'layer_1/weights1/Initializer/random_uniform' type=Add>, <tf.Operation 'layer_1/weights1/Assign' type=Assign>, <tf.Operation 'init' type=NoOp>, <tf.Operation 'input/Placeholder' type=Placeholder>, <tf.Operation 'layer_1/MatMul' type=MatMul>, <tf.Operation 'train/gradients/layer_1/add_grad/Shape' type=Shape>, <tf.Operation 'train/gradients/layer_1/add_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>, <tf.Operation 'layer_1/add' type=Add>, <tf.Operation 'layer_1/Relu' type=Relu>, <tf.Operation 'layer_2/MatMul' type=MatMul>, <tf.Operation 'train/gradients/layer_2/add_grad/Shape' type=Shape>, <tf.Operation 'train/gradients/layer_2/add_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>, <tf.Operation 'layer_2/add' type=Add>, <tf.Operation 'layer_2/Relu' type=Relu>, <tf.Operation 'layer_3/MatMul' type=MatMul>, <tf.Operation 'train/gradients/layer_3/add_grad/Shape' type=Shape>, <tf.Operation 'train/gradients/layer_3/add_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>, <tf.Operation 'layer_3/add' type=Add>, <tf.Operation 'layer_3/Relu' type=Relu>, <tf.Operation 'layer_4/MatMul' type=MatMul>, <tf.Operation 'train/gradients/layer_4/add_grad/Shape' type=Shape>, <tf.Operation 'train/gradients/layer_4/add_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>, <tf.Operation 'layer_4/add' type=Add>, <tf.Operation 'layer_4/Relu' type=Relu>, <tf.Operation 'output/MatMul' type=MatMul>, <tf.Operation 'train/gradients/output/add_grad/Shape' type=Shape>, <tf.Operation 'train/gradients/output/add_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>, <tf.Operation 'output/add' type=Add>, <tf.Operation 'train/gradients/cost/SquaredDifference_grad/Shape' type=Shape>, <tf.Operation 'train/gradients/cost/SquaredDifference_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>, <tf.Operation 'cost/SquaredDifference' type=SquaredDifference>, <tf.Operation 'train/gradients/cost/Mean_grad/Shape_1' type=Shape>, <tf.Operation 'train/gradients/cost/Mean_grad/Prod' type=Prod>, <tf.Operation 'train/gradients/cost/Mean_grad/floordiv' type=FloorDiv>, <tf.Operation 'train/gradients/cost/Mean_grad/Cast' type=Cast>, <tf.Operation 'train/gradients/cost/Mean_grad/Shape' type=Shape>, <tf.Operation 'train/gradients/cost/Mean_grad/Tile' type=Tile>, <tf.Operation 'train/gradients/cost/Mean_grad/truediv' type=RealDiv>, <tf.Operation 'train/gradients/cost/SquaredDifference_grad/sub' type=Sub>, <tf.Operation 'train/gradients/cost/SquaredDifference_grad/scalar' type=Const>, <tf.Operation 'train/gradients/cost/SquaredDifference_grad/mul' type=Mul>, <tf.Operation 'train/gradients/cost/SquaredDifference_grad/mul_1' type=Mul>, <tf.Operation 'train/gradients/cost/SquaredDifference_grad/Sum_1' type=Sum>, <tf.Operation 'train/gradients/cost/SquaredDifference_grad/Reshape_1' type=Reshape>, <tf.Operation 'train/gradients/cost/SquaredDifference_grad/Neg' type=Neg>, <tf.Operation 'train/gradients/cost/SquaredDifference_grad/Sum' type=Sum>, <tf.Operation 'train/gradients/cost/SquaredDifference_grad/Reshape' type=Reshape>, <tf.Operation 'train/gradients/cost/SquaredDifference_grad/tuple/group_deps' type=NoOp>, <tf.Operation 'train/gradients/cost/SquaredDifference_grad/tuple/control_dependency_1' type=Identity>, <tf.Operation 'train/gradients/cost/SquaredDifference_grad/tuple/control_dependency' type=Identity>, <tf.Operation 'train/gradients/output/add_grad/Sum_1' type=Sum>, <tf.Operation 'train/gradients/output/add_grad/Reshape_1' type=Reshape>, <tf.Operation 'train/gradients/output/add_grad/Sum' type=Sum>, <tf.Operation 'train/gradients/output/add_grad/Reshape' type=Reshape>, <tf.Operation 'train/gradients/output/add_grad/tuple/group_deps' type=NoOp>, <tf.Operation 'train/gradients/output/add_grad/tuple/control_dependency_1' type=Identity>, <tf.Operation 'train/Adam/update_output/biases5/ApplyAdam' type=ApplyAdam>, <tf.Operation 'train/gradients/output/add_grad/tuple/control_dependency' type=Identity>, <tf.Operation 'train/gradients/output/MatMul_grad/MatMul_1' type=MatMul>, <tf.Operation 'train/gradients/output/MatMul_grad/MatMul' type=MatMul>, <tf.Operation 'train/gradients/output/MatMul_grad/tuple/group_deps' type=NoOp>, <tf.Operation 'train/gradients/output/MatMul_grad/tuple/control_dependency_1' type=Identity>, <tf.Operation 'train/Adam/update_output/weights5/ApplyAdam' type=ApplyAdam>, <tf.Operation 'train/gradients/output/MatMul_grad/tuple/control_dependency' type=Identity>, <tf.Operation 'train/gradients/layer_4/Relu_grad/ReluGrad' type=ReluGrad>, <tf.Operation 'train/gradients/layer_4/add_grad/Sum_1' type=Sum>, <tf.Operation 'train/gradients/layer_4/add_grad/Reshape_1' type=Reshape>, <tf.Operation 'train/gradients/layer_4/add_grad/Sum' type=Sum>, <tf.Operation 'train/gradients/layer_4/add_grad/Reshape' type=Reshape>, <tf.Operation 'train/gradients/layer_4/add_grad/tuple/group_deps' type=NoOp>, <tf.Operation 'train/gradients/layer_4/add_grad/tuple/control_dependency_1' type=Identity>, <tf.Operation 'train/Adam/update_layer_4/biases4/ApplyAdam' type=ApplyAdam>, <tf.Operation 'train/gradients/layer_4/add_grad/tuple/control_dependency' type=Identity>, <tf.Operation 'train/gradients/layer_4/MatMul_grad/MatMul_1' type=MatMul>, <tf.Operation 'train/gradients/layer_4/MatMul_grad/MatMul' type=MatMul>, <tf.Operation 'train/gradients/layer_4/MatMul_grad/tuple/group_deps' type=NoOp>, <tf.Operation 'train/gradients/layer_4/MatMul_grad/tuple/control_dependency_1' type=Identity>, <tf.Operation 'train/Adam/update_layer_4/weights4/ApplyAdam' type=ApplyAdam>, <tf.Operation 'train/gradients/layer_4/MatMul_grad/tuple/control_dependency' type=Identity>, <tf.Operation 'train/gradients/layer_3/Relu_grad/ReluGrad' type=ReluGrad>, <tf.Operation 'train/gradients/layer_3/add_grad/Sum_1' type=Sum>, <tf.Operation 'train/gradients/layer_3/add_grad/Reshape_1' type=Reshape>, <tf.Operation 'train/gradients/layer_3/add_grad/Sum' type=Sum>, <tf.Operation 'train/gradients/layer_3/add_grad/Reshape' type=Reshape>, <tf.Operation 'train/gradients/layer_3/add_grad/tuple/group_deps' type=NoOp>, <tf.Operation 'train/gradients/layer_3/add_grad/tuple/control_dependency_1' type=Identity>, <tf.Operation 'train/Adam/update_layer_3/biases3/ApplyAdam' type=ApplyAdam>, <tf.Operation 'train/gradients/layer_3/add_grad/tuple/control_dependency' type=Identity>, <tf.Operation 'train/gradients/layer_3/MatMul_grad/MatMul_1' type=MatMul>, <tf.Operation 'train/gradients/layer_3/MatMul_grad/MatMul' type=MatMul>, <tf.Operation 'train/gradients/layer_3/MatMul_grad/tuple/group_deps' type=NoOp>, <tf.Operation 'train/gradients/layer_3/MatMul_grad/tuple/control_dependency_1' type=Identity>, <tf.Operation 'train/Adam/update_layer_3/weights3/ApplyAdam' type=ApplyAdam>, <tf.Operation 'train/gradients/layer_3/MatMul_grad/tuple/control_dependency' type=Identity>, <tf.Operation 'train/gradients/layer_2/Relu_grad/ReluGrad' type=ReluGrad>, <tf.Operation 'train/gradients/layer_2/add_grad/Sum_1' type=Sum>, <tf.Operation 'train/gradients/layer_2/add_grad/Reshape_1' type=Reshape>, <tf.Operation 'train/gradients/layer_2/add_grad/Sum' type=Sum>, <tf.Operation 'train/gradients/layer_2/add_grad/Reshape' type=Reshape>, <tf.Operation 'train/gradients/layer_2/add_grad/tuple/group_deps' type=NoOp>, <tf.Operation 'train/gradients/layer_2/add_grad/tuple/control_dependency_1' type=Identity>, <tf.Operation 'train/Adam/update_layer_2/biases2/ApplyAdam' type=ApplyAdam>, <tf.Operation 'train/gradients/layer_2/add_grad/tuple/control_dependency' type=Identity>, <tf.Operation 'train/gradients/layer_2/MatMul_grad/MatMul_1' type=MatMul>, <tf.Operation 'train/gradients/layer_2/MatMul_grad/MatMul' type=MatMul>, <tf.Operation 'train/gradients/layer_2/MatMul_grad/tuple/group_deps' type=NoOp>, <tf.Operation 'train/gradients/layer_2/MatMul_grad/tuple/control_dependency_1' type=Identity>, <tf.Operation 'train/Adam/update_layer_2/weights2/ApplyAdam' type=ApplyAdam>, <tf.Operation 'train/gradients/layer_2/MatMul_grad/tuple/control_dependency' type=Identity>, <tf.Operation 'train/gradients/layer_1/Relu_grad/ReluGrad' type=ReluGrad>, <tf.Operation 'train/gradients/layer_1/add_grad/Sum_1' type=Sum>, <tf.Operation 'train/gradients/layer_1/add_grad/Reshape_1' type=Reshape>, <tf.Operation 'train/gradients/layer_1/add_grad/Sum' type=Sum>, <tf.Operation 'train/gradients/layer_1/add_grad/Reshape' type=Reshape>, <tf.Operation 'train/gradients/layer_1/add_grad/tuple/group_deps' type=NoOp>, <tf.Operation 'train/gradients/layer_1/add_grad/tuple/control_dependency_1' type=Identity>, <tf.Operation 'train/Adam/update_layer_1/biases1/ApplyAdam' type=ApplyAdam>, <tf.Operation 'train/gradients/layer_1/add_grad/tuple/control_dependency' type=Identity>, <tf.Operation 'train/gradients/layer_1/MatMul_grad/MatMul_1' type=MatMul>, <tf.Operation 'train/gradients/layer_1/MatMul_grad/MatMul' type=MatMul>, <tf.Operation 'train/gradients/layer_1/MatMul_grad/tuple/group_deps' type=NoOp>, <tf.Operation 'train/gradients/layer_1/MatMul_grad/tuple/control_dependency_1' type=Identity>, <tf.Operation 'train/Adam/update_layer_1/weights1/ApplyAdam' type=ApplyAdam>, <tf.Operation 'train/Adam/mul_1' type=Mul>, <tf.Operation 'train/Adam/Assign_1' type=Assign>, <tf.Operation 'train/Adam/mul' type=Mul>, <tf.Operation 'train/Adam/Assign' type=Assign>, <tf.Operation 'train/Adam' type=NoOp>, <tf.Operation 'train/gradients/layer_1/MatMul_grad/tuple/control_dependency' type=Identity>, <tf.Operation 'cost/Mean' type=Mean>, <tf.Operation 'logging/current_cost' type=ScalarSummary>, <tf.Operation 'logging/Merge/MergeSummary' type=MergeSummary>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./1587454858083_512_1024_1024_512_0.0001_5000_Val/logs/trained_model.ckpt\n",
      "save_1/RestoreV2_31/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_31/tensor_names \n",
      "\n",
      "save_1/RestoreV2_30/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_30/tensor_names \n",
      "\n",
      "save_1/RestoreV2_29/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_29/tensor_names \n",
      "\n",
      "save_1/RestoreV2_28/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_28/tensor_names \n",
      "\n",
      "save_1/RestoreV2_27/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_27/tensor_names \n",
      "\n",
      "save_1/RestoreV2_26/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_26/tensor_names \n",
      "\n",
      "save_1/RestoreV2_25/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_25/tensor_names \n",
      "\n",
      "save_1/RestoreV2_24/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_24/tensor_names \n",
      "\n",
      "save_1/RestoreV2_23/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_23/tensor_names \n",
      "\n",
      "save_1/RestoreV2_22/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_22/tensor_names \n",
      "\n",
      "save_1/RestoreV2_21/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_21/tensor_names \n",
      "\n",
      "save_1/RestoreV2_20/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_20/tensor_names \n",
      "\n",
      "save_1/RestoreV2_19/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_19/tensor_names \n",
      "\n",
      "save_1/RestoreV2_18/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_18/tensor_names \n",
      "\n",
      "save_1/RestoreV2_17/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_17/tensor_names \n",
      "\n",
      "save_1/RestoreV2_16/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_16/tensor_names \n",
      "\n",
      "save_1/RestoreV2_15/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_15/tensor_names \n",
      "\n",
      "save_1/RestoreV2_14/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_14/tensor_names \n",
      "\n",
      "save_1/RestoreV2_13/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_13/tensor_names \n",
      "\n",
      "save_1/RestoreV2_12/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_12/tensor_names \n",
      "\n",
      "save_1/RestoreV2_11/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_11/tensor_names \n",
      "\n",
      "save_1/RestoreV2_10/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_10/tensor_names \n",
      "\n",
      "save_1/RestoreV2_9/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_9/tensor_names \n",
      "\n",
      "save_1/RestoreV2_8/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_8/tensor_names \n",
      "\n",
      "save_1/RestoreV2_7/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_7/tensor_names \n",
      "\n",
      "save_1/RestoreV2_6/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_6/tensor_names \n",
      "\n",
      "save_1/RestoreV2_5/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_5/tensor_names \n",
      "\n",
      "save_1/RestoreV2_4/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_4/tensor_names \n",
      "\n",
      "save_1/RestoreV2_3/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_3/tensor_names \n",
      "\n",
      "save_1/RestoreV2_2/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_2/tensor_names \n",
      "\n",
      "save_1/RestoreV2_1/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2_1/tensor_names \n",
      "\n",
      "save_1/RestoreV2/shape_and_slices \n",
      "\n",
      "save_1/RestoreV2/tensor_names \n",
      "\n",
      "save_1/SaveV2/shape_and_slices \n",
      "\n",
      "save_1/SaveV2/tensor_names \n",
      "\n",
      "save_1/ShardedFilename/shard \n",
      "\n",
      "save_1/num_shards \n",
      "\n",
      "save_1/StringJoin/inputs_1 \n",
      "\n",
      "save_1/Const \n",
      "\n",
      "save_1/RestoreV2_31 \n",
      "\n",
      "save_1/RestoreV2_30 \n",
      "\n",
      "save_1/RestoreV2_29 \n",
      "\n",
      "save_1/RestoreV2_28 \n",
      "\n",
      "save_1/RestoreV2_27 \n",
      "\n",
      "save_1/RestoreV2_26 \n",
      "\n",
      "save_1/RestoreV2_25 \n",
      "\n",
      "save_1/RestoreV2_24 \n",
      "\n",
      "save_1/RestoreV2_23 \n",
      "\n",
      "save_1/RestoreV2_22 \n",
      "\n",
      "save_1/RestoreV2_21 \n",
      "\n",
      "save_1/RestoreV2_20 \n",
      "\n",
      "save_1/RestoreV2_19 \n",
      "\n",
      "save_1/RestoreV2_18 \n",
      "\n",
      "save_1/RestoreV2_17 \n",
      "\n",
      "save_1/RestoreV2_16 \n",
      "\n",
      "save_1/RestoreV2_15 \n",
      "\n",
      "save_1/RestoreV2_14 \n",
      "\n",
      "save_1/RestoreV2_13 \n",
      "\n",
      "save_1/RestoreV2_12 \n",
      "\n",
      "save_1/RestoreV2_11 \n",
      "\n",
      "save_1/RestoreV2_10 \n",
      "\n",
      "save_1/RestoreV2_9 \n",
      "\n",
      "save_1/RestoreV2_8 \n",
      "\n",
      "save_1/RestoreV2_7 \n",
      "\n",
      "save_1/RestoreV2_6 \n",
      "\n",
      "save_1/RestoreV2_5 \n",
      "\n",
      "save_1/RestoreV2_4 \n",
      "\n",
      "save_1/RestoreV2_3 \n",
      "\n",
      "save_1/RestoreV2_2 \n",
      "\n",
      "save_1/RestoreV2_1 \n",
      "\n",
      "save_1/RestoreV2 \n",
      "\n",
      "save_1/StringJoin \n",
      "\n",
      "save_1/ShardedFilename \n",
      "\n",
      "save/RestoreV2_31/shape_and_slices \n",
      "\n",
      "save/RestoreV2_31/tensor_names \n",
      "\n",
      "save/RestoreV2_30/shape_and_slices \n",
      "\n",
      "save/RestoreV2_30/tensor_names \n",
      "\n",
      "save/RestoreV2_29/shape_and_slices \n",
      "\n",
      "save/RestoreV2_29/tensor_names \n",
      "\n",
      "save/RestoreV2_28/shape_and_slices \n",
      "\n",
      "save/RestoreV2_28/tensor_names \n",
      "\n",
      "save/RestoreV2_27/shape_and_slices \n",
      "\n",
      "save/RestoreV2_27/tensor_names \n",
      "\n",
      "save/RestoreV2_26/shape_and_slices \n",
      "\n",
      "save/RestoreV2_26/tensor_names \n",
      "\n",
      "save/RestoreV2_25/shape_and_slices \n",
      "\n",
      "save/RestoreV2_25/tensor_names \n",
      "\n",
      "save/RestoreV2_24/shape_and_slices \n",
      "\n",
      "save/RestoreV2_24/tensor_names \n",
      "\n",
      "save/RestoreV2_23/shape_and_slices \n",
      "\n",
      "save/RestoreV2_23/tensor_names \n",
      "\n",
      "save/RestoreV2_22/shape_and_slices \n",
      "\n",
      "save/RestoreV2_22/tensor_names \n",
      "\n",
      "save/RestoreV2_21/shape_and_slices \n",
      "\n",
      "save/RestoreV2_21/tensor_names \n",
      "\n",
      "save/RestoreV2_20/shape_and_slices \n",
      "\n",
      "save/RestoreV2_20/tensor_names \n",
      "\n",
      "save/RestoreV2_19/shape_and_slices \n",
      "\n",
      "save/RestoreV2_19/tensor_names \n",
      "\n",
      "save/RestoreV2_18/shape_and_slices \n",
      "\n",
      "save/RestoreV2_18/tensor_names \n",
      "\n",
      "save/RestoreV2_17/shape_and_slices \n",
      "\n",
      "save/RestoreV2_17/tensor_names \n",
      "\n",
      "save/RestoreV2_16/shape_and_slices \n",
      "\n",
      "save/RestoreV2_16/tensor_names \n",
      "\n",
      "save/RestoreV2_15/shape_and_slices \n",
      "\n",
      "save/RestoreV2_15/tensor_names \n",
      "\n",
      "save/RestoreV2_14/shape_and_slices \n",
      "\n",
      "save/RestoreV2_14/tensor_names \n",
      "\n",
      "save/RestoreV2_13/shape_and_slices \n",
      "\n",
      "save/RestoreV2_13/tensor_names \n",
      "\n",
      "save/RestoreV2_12/shape_and_slices \n",
      "\n",
      "save/RestoreV2_12/tensor_names \n",
      "\n",
      "save/RestoreV2_11/shape_and_slices \n",
      "\n",
      "save/RestoreV2_11/tensor_names \n",
      "\n",
      "save/RestoreV2_10/shape_and_slices \n",
      "\n",
      "save/RestoreV2_10/tensor_names \n",
      "\n",
      "save/RestoreV2_9/shape_and_slices \n",
      "\n",
      "save/RestoreV2_9/tensor_names \n",
      "\n",
      "save/RestoreV2_8/shape_and_slices \n",
      "\n",
      "save/RestoreV2_8/tensor_names \n",
      "\n",
      "save/RestoreV2_7/shape_and_slices \n",
      "\n",
      "save/RestoreV2_7/tensor_names \n",
      "\n",
      "save/RestoreV2_6/shape_and_slices \n",
      "\n",
      "save/RestoreV2_6/tensor_names \n",
      "\n",
      "save/RestoreV2_5/shape_and_slices \n",
      "\n",
      "save/RestoreV2_5/tensor_names \n",
      "\n",
      "save/RestoreV2_4/shape_and_slices \n",
      "\n",
      "save/RestoreV2_4/tensor_names \n",
      "\n",
      "save/RestoreV2_3/shape_and_slices \n",
      "\n",
      "save/RestoreV2_3/tensor_names \n",
      "\n",
      "save/RestoreV2_2/shape_and_slices \n",
      "\n",
      "save/RestoreV2_2/tensor_names \n",
      "\n",
      "save/RestoreV2_1/shape_and_slices \n",
      "\n",
      "save/RestoreV2_1/tensor_names \n",
      "\n",
      "save/RestoreV2/shape_and_slices \n",
      "\n",
      "save/RestoreV2/tensor_names \n",
      "\n",
      "save/SaveV2/shape_and_slices \n",
      "\n",
      "save/SaveV2/tensor_names \n",
      "\n",
      "save/Const \n",
      "\n",
      "save/RestoreV2_31 \n",
      "\n",
      "save/RestoreV2_30 \n",
      "\n",
      "save/RestoreV2_29 \n",
      "\n",
      "save/RestoreV2_28 \n",
      "\n",
      "save/RestoreV2_27 \n",
      "\n",
      "save/RestoreV2_26 \n",
      "\n",
      "save/RestoreV2_25 \n",
      "\n",
      "save/RestoreV2_24 \n",
      "\n",
      "save/RestoreV2_23 \n",
      "\n",
      "save/RestoreV2_22 \n",
      "\n",
      "save/RestoreV2_21 \n",
      "\n",
      "save/RestoreV2_20 \n",
      "\n",
      "save/RestoreV2_19 \n",
      "\n",
      "save/RestoreV2_18 \n",
      "\n",
      "save/RestoreV2_17 \n",
      "\n",
      "save/RestoreV2_16 \n",
      "\n",
      "save/RestoreV2_15 \n",
      "\n",
      "save/RestoreV2_14 \n",
      "\n",
      "save/RestoreV2_13 \n",
      "\n",
      "save/RestoreV2_12 \n",
      "\n",
      "save/RestoreV2_11 \n",
      "\n",
      "save/RestoreV2_10 \n",
      "\n",
      "save/RestoreV2_9 \n",
      "\n",
      "save/RestoreV2_8 \n",
      "\n",
      "save/RestoreV2_7 \n",
      "\n",
      "save/RestoreV2_6 \n",
      "\n",
      "save/RestoreV2_5 \n",
      "\n",
      "save/RestoreV2_4 \n",
      "\n",
      "save/RestoreV2_3 \n",
      "\n",
      "save/RestoreV2_2 \n",
      "\n",
      "save/RestoreV2_1 \n",
      "\n",
      "save/RestoreV2 \n",
      "\n",
      "logging/current_cost/tags \n",
      "\n",
      "train/Adam/epsilon \n",
      "\n",
      "train/Adam/beta2 \n",
      "\n",
      "train/Adam/beta1 \n",
      "\n",
      "train/Adam/learning_rate \n",
      "\n",
      "train/output/biases5/Adam_1 \n",
      "\n",
      "save_1/Assign_29 \n",
      "\n",
      "save/Assign_29 \n",
      "\n",
      "train/output/biases5/Adam_1/read \n",
      "\n",
      "train/output/biases5/Adam_1/Initializer/zeros \n",
      "\n",
      "train/output/biases5/Adam_1/Assign \n",
      "\n",
      "train/output/biases5/Adam \n",
      "\n",
      "save_1/Assign_28 \n",
      "\n",
      "save/Assign_28 \n",
      "\n",
      "train/output/biases5/Adam/read \n",
      "\n",
      "train/output/biases5/Adam/Initializer/zeros \n",
      "\n",
      "train/output/biases5/Adam/Assign \n",
      "\n",
      "train/output/weights5/Adam_1 \n",
      "\n",
      "save_1/Assign_31 \n",
      "\n",
      "save/Assign_31 \n",
      "\n",
      "train/output/weights5/Adam_1/read \n",
      "\n",
      "train/output/weights5/Adam_1/Initializer/zeros \n",
      "\n",
      "train/output/weights5/Adam_1/Assign \n",
      "\n",
      "train/output/weights5/Adam \n",
      "\n",
      "save_1/Assign_30 \n",
      "\n",
      "save/Assign_30 \n",
      "\n",
      "train/output/weights5/Adam/read \n",
      "\n",
      "train/output/weights5/Adam/Initializer/zeros \n",
      "\n",
      "train/output/weights5/Adam/Assign \n",
      "\n",
      "train/layer_4/biases4/Adam_1 \n",
      "\n",
      "save_1/Assign_25 \n",
      "\n",
      "save/Assign_25 \n",
      "\n",
      "train/layer_4/biases4/Adam_1/read \n",
      "\n",
      "train/layer_4/biases4/Adam_1/Initializer/zeros \n",
      "\n",
      "train/layer_4/biases4/Adam_1/Assign \n",
      "\n",
      "train/layer_4/biases4/Adam \n",
      "\n",
      "save_1/Assign_24 \n",
      "\n",
      "save/Assign_24 \n",
      "\n",
      "train/layer_4/biases4/Adam/read \n",
      "\n",
      "train/layer_4/biases4/Adam/Initializer/zeros \n",
      "\n",
      "train/layer_4/biases4/Adam/Assign \n",
      "\n",
      "train/layer_4/weights4/Adam_1 \n",
      "\n",
      "save_1/Assign_27 \n",
      "\n",
      "save/Assign_27 \n",
      "\n",
      "train/layer_4/weights4/Adam_1/read \n",
      "\n",
      "train/layer_4/weights4/Adam_1/Initializer/zeros \n",
      "\n",
      "train/layer_4/weights4/Adam_1/Assign \n",
      "\n",
      "train/layer_4/weights4/Adam \n",
      "\n",
      "save_1/Assign_26 \n",
      "\n",
      "save/Assign_26 \n",
      "\n",
      "train/layer_4/weights4/Adam/read \n",
      "\n",
      "train/layer_4/weights4/Adam/Initializer/zeros \n",
      "\n",
      "train/layer_4/weights4/Adam/Assign \n",
      "\n",
      "train/layer_3/biases3/Adam_1 \n",
      "\n",
      "save_1/Assign_21 \n",
      "\n",
      "save/Assign_21 \n",
      "\n",
      "train/layer_3/biases3/Adam_1/read \n",
      "\n",
      "train/layer_3/biases3/Adam_1/Initializer/zeros \n",
      "\n",
      "train/layer_3/biases3/Adam_1/Assign \n",
      "\n",
      "train/layer_3/biases3/Adam \n",
      "\n",
      "save_1/Assign_20 \n",
      "\n",
      "save/Assign_20 \n",
      "\n",
      "train/layer_3/biases3/Adam/read \n",
      "\n",
      "train/layer_3/biases3/Adam/Initializer/zeros \n",
      "\n",
      "train/layer_3/biases3/Adam/Assign \n",
      "\n",
      "train/layer_3/weights3/Adam_1 \n",
      "\n",
      "save_1/Assign_23 \n",
      "\n",
      "save/Assign_23 \n",
      "\n",
      "train/layer_3/weights3/Adam_1/read \n",
      "\n",
      "train/layer_3/weights3/Adam_1/Initializer/zeros \n",
      "\n",
      "train/layer_3/weights3/Adam_1/Assign \n",
      "\n",
      "train/layer_3/weights3/Adam \n",
      "\n",
      "save_1/Assign_22 \n",
      "\n",
      "save/Assign_22 \n",
      "\n",
      "train/layer_3/weights3/Adam/read \n",
      "\n",
      "train/layer_3/weights3/Adam/Initializer/zeros \n",
      "\n",
      "train/layer_3/weights3/Adam/Assign \n",
      "\n",
      "train/layer_2/biases2/Adam_1 \n",
      "\n",
      "save_1/Assign_17 \n",
      "\n",
      "save/Assign_17 \n",
      "\n",
      "train/layer_2/biases2/Adam_1/read \n",
      "\n",
      "train/layer_2/biases2/Adam_1/Initializer/zeros \n",
      "\n",
      "train/layer_2/biases2/Adam_1/Assign \n",
      "\n",
      "train/layer_2/biases2/Adam \n",
      "\n",
      "save_1/Assign_16 \n",
      "\n",
      "save/Assign_16 \n",
      "\n",
      "train/layer_2/biases2/Adam/read \n",
      "\n",
      "train/layer_2/biases2/Adam/Initializer/zeros \n",
      "\n",
      "train/layer_2/biases2/Adam/Assign \n",
      "\n",
      "train/layer_2/weights2/Adam_1 \n",
      "\n",
      "save_1/Assign_19 \n",
      "\n",
      "save/Assign_19 \n",
      "\n",
      "train/layer_2/weights2/Adam_1/read \n",
      "\n",
      "train/layer_2/weights2/Adam_1/Initializer/zeros \n",
      "\n",
      "train/layer_2/weights2/Adam_1/Assign \n",
      "\n",
      "train/layer_2/weights2/Adam \n",
      "\n",
      "save_1/Assign_18 \n",
      "\n",
      "save/Assign_18 \n",
      "\n",
      "train/layer_2/weights2/Adam/read \n",
      "\n",
      "train/layer_2/weights2/Adam/Initializer/zeros \n",
      "\n",
      "train/layer_2/weights2/Adam/Assign \n",
      "\n",
      "train/layer_1/biases1/Adam_1 \n",
      "\n",
      "save_1/Assign_13 \n",
      "\n",
      "save/Assign_13 \n",
      "\n",
      "train/layer_1/biases1/Adam_1/read \n",
      "\n",
      "train/layer_1/biases1/Adam_1/Initializer/zeros \n",
      "\n",
      "train/layer_1/biases1/Adam_1/Assign \n",
      "\n",
      "train/layer_1/biases1/Adam \n",
      "\n",
      "save_1/Assign_12 \n",
      "\n",
      "save/Assign_12 \n",
      "\n",
      "train/layer_1/biases1/Adam/read \n",
      "\n",
      "train/layer_1/biases1/Adam/Initializer/zeros \n",
      "\n",
      "train/layer_1/biases1/Adam/Assign \n",
      "\n",
      "train/layer_1/weights1/Adam_1 \n",
      "\n",
      "save_1/Assign_15 \n",
      "\n",
      "save/Assign_15 \n",
      "\n",
      "train/layer_1/weights1/Adam_1/read \n",
      "\n",
      "train/layer_1/weights1/Adam_1/Initializer/zeros \n",
      "\n",
      "train/layer_1/weights1/Adam_1/Assign \n",
      "\n",
      "train/layer_1/weights1/Adam \n",
      "\n",
      "save_1/Assign_14 \n",
      "\n",
      "save/Assign_14 \n",
      "\n",
      "train/layer_1/weights1/Adam/read \n",
      "\n",
      "train/layer_1/weights1/Adam/Initializer/zeros \n",
      "\n",
      "train/layer_1/weights1/Adam/Assign \n",
      "\n",
      "train/beta2_power \n",
      "\n",
      "save_1/Assign_11 \n",
      "\n",
      "save/Assign_11 \n",
      "\n",
      "train/beta2_power/read \n",
      "\n",
      "train/beta2_power/initial_value \n",
      "\n",
      "train/beta2_power/Assign \n",
      "\n",
      "train/beta1_power \n",
      "\n",
      "save_1/Assign_10 \n",
      "\n",
      "save/Assign_10 \n",
      "\n",
      "train/beta1_power/read \n",
      "\n",
      "train/beta1_power/initial_value \n",
      "\n",
      "train/beta1_power/Assign \n",
      "\n",
      "train/gradients/layer_1/add_grad/Shape_1 \n",
      "\n",
      "train/gradients/layer_2/add_grad/Shape_1 \n",
      "\n",
      "train/gradients/layer_3/add_grad/Shape_1 \n",
      "\n",
      "train/gradients/layer_4/add_grad/Shape_1 \n",
      "\n",
      "train/gradients/output/add_grad/Shape_1 \n",
      "\n",
      "train/gradients/cost/Mean_grad/Maximum/y \n",
      "\n",
      "train/gradients/cost/Mean_grad/Const_1 \n",
      "\n",
      "train/gradients/cost/Mean_grad/Const \n",
      "\n",
      "train/gradients/cost/Mean_grad/Shape_2 \n",
      "\n",
      "train/gradients/cost/Mean_grad/Prod_1 \n",
      "\n",
      "train/gradients/cost/Mean_grad/Maximum \n",
      "\n",
      "train/gradients/cost/Mean_grad/Reshape/shape \n",
      "\n",
      "train/gradients/Const \n",
      "\n",
      "train/gradients/Shape \n",
      "\n",
      "train/gradients/Fill \n",
      "\n",
      "train/gradients/cost/Mean_grad/Reshape \n",
      "\n",
      "cost/Const \n",
      "\n",
      "cost/Placeholder \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/Shape_1 \n",
      "\n",
      "output/biases5 \n",
      "\n",
      "save_1/Assign_8 \n",
      "\n",
      "save/Assign_8 \n",
      "\n",
      "output/biases5/read \n",
      "\n",
      "output/biases5/Initializer/zeros \n",
      "\n",
      "output/biases5/Assign \n",
      "\n",
      "output/weights5 \n",
      "\n",
      "save_1/Assign_9 \n",
      "\n",
      "save/Assign_9 \n",
      "\n",
      "output/weights5/read \n",
      "\n",
      "output/weights5/Initializer/random_uniform/max \n",
      "\n",
      "output/weights5/Initializer/random_uniform/min \n",
      "\n",
      "output/weights5/Initializer/random_uniform/sub \n",
      "\n",
      "output/weights5/Initializer/random_uniform/shape \n",
      "\n",
      "output/weights5/Initializer/random_uniform/RandomUniform \n",
      "\n",
      "output/weights5/Initializer/random_uniform/mul \n",
      "\n",
      "output/weights5/Initializer/random_uniform \n",
      "\n",
      "output/weights5/Assign \n",
      "\n",
      "layer_4/biases4 \n",
      "\n",
      "save_1/Assign_6 \n",
      "\n",
      "save/Assign_6 \n",
      "\n",
      "layer_4/biases4/read \n",
      "\n",
      "layer_4/biases4/Initializer/zeros \n",
      "\n",
      "layer_4/biases4/Assign \n",
      "\n",
      "layer_4/weights4 \n",
      "\n",
      "save_1/Assign_7 \n",
      "\n",
      "save/Assign_7 \n",
      "\n",
      "layer_4/weights4/read \n",
      "\n",
      "layer_4/weights4/Initializer/random_uniform/max \n",
      "\n",
      "layer_4/weights4/Initializer/random_uniform/min \n",
      "\n",
      "layer_4/weights4/Initializer/random_uniform/sub \n",
      "\n",
      "layer_4/weights4/Initializer/random_uniform/shape \n",
      "\n",
      "layer_4/weights4/Initializer/random_uniform/RandomUniform \n",
      "\n",
      "layer_4/weights4/Initializer/random_uniform/mul \n",
      "\n",
      "layer_4/weights4/Initializer/random_uniform \n",
      "\n",
      "layer_4/weights4/Assign \n",
      "\n",
      "layer_3/biases3 \n",
      "\n",
      "save_1/Assign_4 \n",
      "\n",
      "save/Assign_4 \n",
      "\n",
      "layer_3/biases3/read \n",
      "\n",
      "layer_3/biases3/Initializer/zeros \n",
      "\n",
      "layer_3/biases3/Assign \n",
      "\n",
      "layer_3/weights3 \n",
      "\n",
      "save_1/Assign_5 \n",
      "\n",
      "save/Assign_5 \n",
      "\n",
      "layer_3/weights3/read \n",
      "\n",
      "layer_3/weights3/Initializer/random_uniform/max \n",
      "\n",
      "layer_3/weights3/Initializer/random_uniform/min \n",
      "\n",
      "layer_3/weights3/Initializer/random_uniform/sub \n",
      "\n",
      "layer_3/weights3/Initializer/random_uniform/shape \n",
      "\n",
      "layer_3/weights3/Initializer/random_uniform/RandomUniform \n",
      "\n",
      "layer_3/weights3/Initializer/random_uniform/mul \n",
      "\n",
      "layer_3/weights3/Initializer/random_uniform \n",
      "\n",
      "layer_3/weights3/Assign \n",
      "\n",
      "layer_2/biases2 \n",
      "\n",
      "save_1/Assign_2 \n",
      "\n",
      "save/Assign_2 \n",
      "\n",
      "layer_2/biases2/read \n",
      "\n",
      "layer_2/biases2/Initializer/zeros \n",
      "\n",
      "layer_2/biases2/Assign \n",
      "\n",
      "layer_2/weights2 \n",
      "\n",
      "save_1/Assign_3 \n",
      "\n",
      "save/Assign_3 \n",
      "\n",
      "layer_2/weights2/read \n",
      "\n",
      "layer_2/weights2/Initializer/random_uniform/max \n",
      "\n",
      "layer_2/weights2/Initializer/random_uniform/min \n",
      "\n",
      "layer_2/weights2/Initializer/random_uniform/sub \n",
      "\n",
      "layer_2/weights2/Initializer/random_uniform/shape \n",
      "\n",
      "layer_2/weights2/Initializer/random_uniform/RandomUniform \n",
      "\n",
      "layer_2/weights2/Initializer/random_uniform/mul \n",
      "\n",
      "layer_2/weights2/Initializer/random_uniform \n",
      "\n",
      "layer_2/weights2/Assign \n",
      "\n",
      "layer_1/biases1 \n",
      "\n",
      "save_1/Assign \n",
      "\n",
      "save/Assign \n",
      "\n",
      "layer_1/biases1/read \n",
      "\n",
      "layer_1/biases1/Initializer/zeros \n",
      "\n",
      "layer_1/biases1/Assign \n",
      "\n",
      "layer_1/weights1 \n",
      "\n",
      "save_1/Assign_1 \n",
      "\n",
      "save_1/restore_shard \n",
      "\n",
      "save_1/restore_all \n",
      "\n",
      "save_1/SaveV2 \n",
      "\n",
      "save_1/control_dependency \n",
      "\n",
      "save_1/MergeV2Checkpoints/checkpoint_prefixes \n",
      "\n",
      "save_1/MergeV2Checkpoints \n",
      "\n",
      "save_1/Identity \n",
      "\n",
      "save/Assign_1 \n",
      "\n",
      "save/restore_all \n",
      "\n",
      "save/SaveV2 \n",
      "\n",
      "save/control_dependency \n",
      "\n",
      "layer_1/weights1/read \n",
      "\n",
      "layer_1/weights1/Initializer/random_uniform/max \n",
      "\n",
      "layer_1/weights1/Initializer/random_uniform/min \n",
      "\n",
      "layer_1/weights1/Initializer/random_uniform/sub \n",
      "\n",
      "layer_1/weights1/Initializer/random_uniform/shape \n",
      "\n",
      "layer_1/weights1/Initializer/random_uniform/RandomUniform \n",
      "\n",
      "layer_1/weights1/Initializer/random_uniform/mul \n",
      "\n",
      "layer_1/weights1/Initializer/random_uniform \n",
      "\n",
      "layer_1/weights1/Assign \n",
      "\n",
      "init \n",
      "\n",
      "input/Placeholder \n",
      "\n",
      "layer_1/MatMul \n",
      "\n",
      "train/gradients/layer_1/add_grad/Shape \n",
      "\n",
      "train/gradients/layer_1/add_grad/BroadcastGradientArgs \n",
      "\n",
      "layer_1/add \n",
      "\n",
      "layer_1/Relu \n",
      "\n",
      "layer_2/MatMul \n",
      "\n",
      "train/gradients/layer_2/add_grad/Shape \n",
      "\n",
      "train/gradients/layer_2/add_grad/BroadcastGradientArgs \n",
      "\n",
      "layer_2/add \n",
      "\n",
      "layer_2/Relu \n",
      "\n",
      "layer_3/MatMul \n",
      "\n",
      "train/gradients/layer_3/add_grad/Shape \n",
      "\n",
      "train/gradients/layer_3/add_grad/BroadcastGradientArgs \n",
      "\n",
      "layer_3/add \n",
      "\n",
      "layer_3/Relu \n",
      "\n",
      "layer_4/MatMul \n",
      "\n",
      "train/gradients/layer_4/add_grad/Shape \n",
      "\n",
      "train/gradients/layer_4/add_grad/BroadcastGradientArgs \n",
      "\n",
      "layer_4/add \n",
      "\n",
      "layer_4/Relu \n",
      "\n",
      "output/MatMul \n",
      "\n",
      "train/gradients/output/add_grad/Shape \n",
      "\n",
      "train/gradients/output/add_grad/BroadcastGradientArgs \n",
      "\n",
      "output/add \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/Shape \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/BroadcastGradientArgs \n",
      "\n",
      "cost/SquaredDifference \n",
      "\n",
      "train/gradients/cost/Mean_grad/Shape_1 \n",
      "\n",
      "train/gradients/cost/Mean_grad/Prod \n",
      "\n",
      "train/gradients/cost/Mean_grad/floordiv \n",
      "\n",
      "train/gradients/cost/Mean_grad/Cast \n",
      "\n",
      "train/gradients/cost/Mean_grad/Shape \n",
      "\n",
      "train/gradients/cost/Mean_grad/Tile \n",
      "\n",
      "train/gradients/cost/Mean_grad/truediv \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/sub \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/scalar \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/mul \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/mul_1 \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/Sum_1 \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/Reshape_1 \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/Neg \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/Sum \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/Reshape \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/tuple/group_deps \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/tuple/control_dependency_1 \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/tuple/control_dependency \n",
      "\n",
      "train/gradients/output/add_grad/Sum_1 \n",
      "\n",
      "train/gradients/output/add_grad/Reshape_1 \n",
      "\n",
      "train/gradients/output/add_grad/Sum \n",
      "\n",
      "train/gradients/output/add_grad/Reshape \n",
      "\n",
      "train/gradients/output/add_grad/tuple/group_deps \n",
      "\n",
      "train/gradients/output/add_grad/tuple/control_dependency_1 \n",
      "\n",
      "train/Adam/update_output/biases5/ApplyAdam \n",
      "\n",
      "train/gradients/output/add_grad/tuple/control_dependency \n",
      "\n",
      "train/gradients/output/MatMul_grad/MatMul_1 \n",
      "\n",
      "train/gradients/output/MatMul_grad/MatMul \n",
      "\n",
      "train/gradients/output/MatMul_grad/tuple/group_deps \n",
      "\n",
      "train/gradients/output/MatMul_grad/tuple/control_dependency_1 \n",
      "\n",
      "train/Adam/update_output/weights5/ApplyAdam \n",
      "\n",
      "train/gradients/output/MatMul_grad/tuple/control_dependency \n",
      "\n",
      "train/gradients/layer_4/Relu_grad/ReluGrad \n",
      "\n",
      "train/gradients/layer_4/add_grad/Sum_1 \n",
      "\n",
      "train/gradients/layer_4/add_grad/Reshape_1 \n",
      "\n",
      "train/gradients/layer_4/add_grad/Sum \n",
      "\n",
      "train/gradients/layer_4/add_grad/Reshape \n",
      "\n",
      "train/gradients/layer_4/add_grad/tuple/group_deps \n",
      "\n",
      "train/gradients/layer_4/add_grad/tuple/control_dependency_1 \n",
      "\n",
      "train/Adam/update_layer_4/biases4/ApplyAdam \n",
      "\n",
      "train/gradients/layer_4/add_grad/tuple/control_dependency \n",
      "\n",
      "train/gradients/layer_4/MatMul_grad/MatMul_1 \n",
      "\n",
      "train/gradients/layer_4/MatMul_grad/MatMul \n",
      "\n",
      "train/gradients/layer_4/MatMul_grad/tuple/group_deps \n",
      "\n",
      "train/gradients/layer_4/MatMul_grad/tuple/control_dependency_1 \n",
      "\n",
      "train/Adam/update_layer_4/weights4/ApplyAdam \n",
      "\n",
      "train/gradients/layer_4/MatMul_grad/tuple/control_dependency \n",
      "\n",
      "train/gradients/layer_3/Relu_grad/ReluGrad \n",
      "\n",
      "train/gradients/layer_3/add_grad/Sum_1 \n",
      "\n",
      "train/gradients/layer_3/add_grad/Reshape_1 \n",
      "\n",
      "train/gradients/layer_3/add_grad/Sum \n",
      "\n",
      "train/gradients/layer_3/add_grad/Reshape \n",
      "\n",
      "train/gradients/layer_3/add_grad/tuple/group_deps \n",
      "\n",
      "train/gradients/layer_3/add_grad/tuple/control_dependency_1 \n",
      "\n",
      "train/Adam/update_layer_3/biases3/ApplyAdam \n",
      "\n",
      "train/gradients/layer_3/add_grad/tuple/control_dependency \n",
      "\n",
      "train/gradients/layer_3/MatMul_grad/MatMul_1 \n",
      "\n",
      "train/gradients/layer_3/MatMul_grad/MatMul \n",
      "\n",
      "train/gradients/layer_3/MatMul_grad/tuple/group_deps \n",
      "\n",
      "train/gradients/layer_3/MatMul_grad/tuple/control_dependency_1 \n",
      "\n",
      "train/Adam/update_layer_3/weights3/ApplyAdam \n",
      "\n",
      "train/gradients/layer_3/MatMul_grad/tuple/control_dependency \n",
      "\n",
      "train/gradients/layer_2/Relu_grad/ReluGrad \n",
      "\n",
      "train/gradients/layer_2/add_grad/Sum_1 \n",
      "\n",
      "train/gradients/layer_2/add_grad/Reshape_1 \n",
      "\n",
      "train/gradients/layer_2/add_grad/Sum \n",
      "\n",
      "train/gradients/layer_2/add_grad/Reshape \n",
      "\n",
      "train/gradients/layer_2/add_grad/tuple/group_deps \n",
      "\n",
      "train/gradients/layer_2/add_grad/tuple/control_dependency_1 \n",
      "\n",
      "train/Adam/update_layer_2/biases2/ApplyAdam \n",
      "\n",
      "train/gradients/layer_2/add_grad/tuple/control_dependency \n",
      "\n",
      "train/gradients/layer_2/MatMul_grad/MatMul_1 \n",
      "\n",
      "train/gradients/layer_2/MatMul_grad/MatMul \n",
      "\n",
      "train/gradients/layer_2/MatMul_grad/tuple/group_deps \n",
      "\n",
      "train/gradients/layer_2/MatMul_grad/tuple/control_dependency_1 \n",
      "\n",
      "train/Adam/update_layer_2/weights2/ApplyAdam \n",
      "\n",
      "train/gradients/layer_2/MatMul_grad/tuple/control_dependency \n",
      "\n",
      "train/gradients/layer_1/Relu_grad/ReluGrad \n",
      "\n",
      "train/gradients/layer_1/add_grad/Sum_1 \n",
      "\n",
      "train/gradients/layer_1/add_grad/Reshape_1 \n",
      "\n",
      "train/gradients/layer_1/add_grad/Sum \n",
      "\n",
      "train/gradients/layer_1/add_grad/Reshape \n",
      "\n",
      "train/gradients/layer_1/add_grad/tuple/group_deps \n",
      "\n",
      "train/gradients/layer_1/add_grad/tuple/control_dependency_1 \n",
      "\n",
      "train/Adam/update_layer_1/biases1/ApplyAdam \n",
      "\n",
      "train/gradients/layer_1/add_grad/tuple/control_dependency \n",
      "\n",
      "train/gradients/layer_1/MatMul_grad/MatMul_1 \n",
      "\n",
      "train/gradients/layer_1/MatMul_grad/MatMul \n",
      "\n",
      "train/gradients/layer_1/MatMul_grad/tuple/group_deps \n",
      "\n",
      "train/gradients/layer_1/MatMul_grad/tuple/control_dependency_1 \n",
      "\n",
      "train/Adam/update_layer_1/weights1/ApplyAdam \n",
      "\n",
      "train/Adam/mul_1 \n",
      "\n",
      "train/Adam/Assign_1 \n",
      "\n",
      "train/Adam/mul \n",
      "\n",
      "train/Adam/Assign \n",
      "\n",
      "train/Adam \n",
      "\n",
      "train/gradients/layer_1/MatMul_grad/tuple/control_dependency \n",
      "\n",
      "cost/Mean \n",
      "\n",
      "logging/current_cost \n",
      "\n",
      "logging/Merge/MergeSummary \n",
      "\n",
      "save/RestoreV2_31/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_31/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_30/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_30/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_29/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_29/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_28/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_28/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_27/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_27/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_26/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_26/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_25/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_25/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_24/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_24/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_23/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_23/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_22/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_22/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_21/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_21/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_20/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_20/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_19/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_19/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_18/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_18/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_17/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_17/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_16/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_16/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_15/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_15/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_14/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_14/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_13/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_13/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_12/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_12/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_11/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_11/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_10/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_10/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_9/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_9/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_8/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_8/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_7/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_7/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_6/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_6/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_5/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_5/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_4/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_4/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_3/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_3/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_2/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_2/tensor_names_1 \n",
      "\n",
      "save/RestoreV2_1/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2_1/tensor_names_1 \n",
      "\n",
      "save/RestoreV2/shape_and_slices_1 \n",
      "\n",
      "save/RestoreV2/tensor_names_1 \n",
      "\n",
      "save/SaveV2/shape_and_slices_1 \n",
      "\n",
      "save/SaveV2/tensor_names_1 \n",
      "\n",
      "save/Const_1 \n",
      "\n",
      "save/RestoreV2_31_1 \n",
      "\n",
      "save/RestoreV2_30_1 \n",
      "\n",
      "save/RestoreV2_29_1 \n",
      "\n",
      "save/RestoreV2_28_1 \n",
      "\n",
      "save/RestoreV2_27_1 \n",
      "\n",
      "save/RestoreV2_26_1 \n",
      "\n",
      "save/RestoreV2_25_1 \n",
      "\n",
      "save/RestoreV2_24_1 \n",
      "\n",
      "save/RestoreV2_23_1 \n",
      "\n",
      "save/RestoreV2_22_1 \n",
      "\n",
      "save/RestoreV2_21_1 \n",
      "\n",
      "save/RestoreV2_20_1 \n",
      "\n",
      "save/RestoreV2_19_1 \n",
      "\n",
      "save/RestoreV2_18_1 \n",
      "\n",
      "save/RestoreV2_17_1 \n",
      "\n",
      "save/RestoreV2_16_1 \n",
      "\n",
      "save/RestoreV2_15_1 \n",
      "\n",
      "save/RestoreV2_14_1 \n",
      "\n",
      "save/RestoreV2_13_1 \n",
      "\n",
      "save/RestoreV2_12_1 \n",
      "\n",
      "save/RestoreV2_11_1 \n",
      "\n",
      "save/RestoreV2_10_1 \n",
      "\n",
      "save/RestoreV2_9_1 \n",
      "\n",
      "save/RestoreV2_8_1 \n",
      "\n",
      "save/RestoreV2_7_1 \n",
      "\n",
      "save/RestoreV2_6_1 \n",
      "\n",
      "save/RestoreV2_5_1 \n",
      "\n",
      "save/RestoreV2_4_1 \n",
      "\n",
      "save/RestoreV2_3_1 \n",
      "\n",
      "save/RestoreV2_2_1 \n",
      "\n",
      "save/RestoreV2_1_1 \n",
      "\n",
      "save/RestoreV2_32 \n",
      "\n",
      "logging/current_cost/tags_1 \n",
      "\n",
      "train/Adam/epsilon_1 \n",
      "\n",
      "train/Adam/beta2_1 \n",
      "\n",
      "train/Adam/beta1_1 \n",
      "\n",
      "train/Adam/learning_rate_1 \n",
      "\n",
      "train/output/biases5/Adam_1_1 \n",
      "\n",
      "save/Assign_29_1 \n",
      "\n",
      "train/output/biases5/Adam_1/read_1 \n",
      "\n",
      "train/output/biases5/Adam_1/Initializer/zeros_1 \n",
      "\n",
      "train/output/biases5/Adam_1/Assign_1 \n",
      "\n",
      "train/output/biases5/Adam_2 \n",
      "\n",
      "save/Assign_28_1 \n",
      "\n",
      "train/output/biases5/Adam/read_1 \n",
      "\n",
      "train/output/biases5/Adam/Initializer/zeros_1 \n",
      "\n",
      "train/output/biases5/Adam/Assign_1 \n",
      "\n",
      "train/output/weights5/Adam_1_1 \n",
      "\n",
      "save/Assign_31_1 \n",
      "\n",
      "train/output/weights5/Adam_1/read_1 \n",
      "\n",
      "train/output/weights5/Adam_1/Initializer/zeros_1 \n",
      "\n",
      "train/output/weights5/Adam_1/Assign_1 \n",
      "\n",
      "train/output/weights5/Adam_2 \n",
      "\n",
      "save/Assign_30_1 \n",
      "\n",
      "train/output/weights5/Adam/read_1 \n",
      "\n",
      "train/output/weights5/Adam/Initializer/zeros_1 \n",
      "\n",
      "train/output/weights5/Adam/Assign_1 \n",
      "\n",
      "train/layer_4/biases4/Adam_1_1 \n",
      "\n",
      "save/Assign_25_1 \n",
      "\n",
      "train/layer_4/biases4/Adam_1/read_1 \n",
      "\n",
      "train/layer_4/biases4/Adam_1/Initializer/zeros_1 \n",
      "\n",
      "train/layer_4/biases4/Adam_1/Assign_1 \n",
      "\n",
      "train/layer_4/biases4/Adam_2 \n",
      "\n",
      "save/Assign_24_1 \n",
      "\n",
      "train/layer_4/biases4/Adam/read_1 \n",
      "\n",
      "train/layer_4/biases4/Adam/Initializer/zeros_1 \n",
      "\n",
      "train/layer_4/biases4/Adam/Assign_1 \n",
      "\n",
      "train/layer_4/weights4/Adam_1_1 \n",
      "\n",
      "save/Assign_27_1 \n",
      "\n",
      "train/layer_4/weights4/Adam_1/read_1 \n",
      "\n",
      "train/layer_4/weights4/Adam_1/Initializer/zeros_1 \n",
      "\n",
      "train/layer_4/weights4/Adam_1/Assign_1 \n",
      "\n",
      "train/layer_4/weights4/Adam_2 \n",
      "\n",
      "save/Assign_26_1 \n",
      "\n",
      "train/layer_4/weights4/Adam/read_1 \n",
      "\n",
      "train/layer_4/weights4/Adam/Initializer/zeros_1 \n",
      "\n",
      "train/layer_4/weights4/Adam/Assign_1 \n",
      "\n",
      "train/layer_3/biases3/Adam_1_1 \n",
      "\n",
      "save/Assign_21_1 \n",
      "\n",
      "train/layer_3/biases3/Adam_1/read_1 \n",
      "\n",
      "train/layer_3/biases3/Adam_1/Initializer/zeros_1 \n",
      "\n",
      "train/layer_3/biases3/Adam_1/Assign_1 \n",
      "\n",
      "train/layer_3/biases3/Adam_2 \n",
      "\n",
      "save/Assign_20_1 \n",
      "\n",
      "train/layer_3/biases3/Adam/read_1 \n",
      "\n",
      "train/layer_3/biases3/Adam/Initializer/zeros_1 \n",
      "\n",
      "train/layer_3/biases3/Adam/Assign_1 \n",
      "\n",
      "train/layer_3/weights3/Adam_1_1 \n",
      "\n",
      "save/Assign_23_1 \n",
      "\n",
      "train/layer_3/weights3/Adam_1/read_1 \n",
      "\n",
      "train/layer_3/weights3/Adam_1/Initializer/zeros_1 \n",
      "\n",
      "train/layer_3/weights3/Adam_1/Assign_1 \n",
      "\n",
      "train/layer_3/weights3/Adam_2 \n",
      "\n",
      "save/Assign_22_1 \n",
      "\n",
      "train/layer_3/weights3/Adam/read_1 \n",
      "\n",
      "train/layer_3/weights3/Adam/Initializer/zeros_1 \n",
      "\n",
      "train/layer_3/weights3/Adam/Assign_1 \n",
      "\n",
      "train/layer_2/biases2/Adam_1_1 \n",
      "\n",
      "save/Assign_17_1 \n",
      "\n",
      "train/layer_2/biases2/Adam_1/read_1 \n",
      "\n",
      "train/layer_2/biases2/Adam_1/Initializer/zeros_1 \n",
      "\n",
      "train/layer_2/biases2/Adam_1/Assign_1 \n",
      "\n",
      "train/layer_2/biases2/Adam_2 \n",
      "\n",
      "save/Assign_16_1 \n",
      "\n",
      "train/layer_2/biases2/Adam/read_1 \n",
      "\n",
      "train/layer_2/biases2/Adam/Initializer/zeros_1 \n",
      "\n",
      "train/layer_2/biases2/Adam/Assign_1 \n",
      "\n",
      "train/layer_2/weights2/Adam_1_1 \n",
      "\n",
      "save/Assign_19_1 \n",
      "\n",
      "train/layer_2/weights2/Adam_1/read_1 \n",
      "\n",
      "train/layer_2/weights2/Adam_1/Initializer/zeros_1 \n",
      "\n",
      "train/layer_2/weights2/Adam_1/Assign_1 \n",
      "\n",
      "train/layer_2/weights2/Adam_2 \n",
      "\n",
      "save/Assign_18_1 \n",
      "\n",
      "train/layer_2/weights2/Adam/read_1 \n",
      "\n",
      "train/layer_2/weights2/Adam/Initializer/zeros_1 \n",
      "\n",
      "train/layer_2/weights2/Adam/Assign_1 \n",
      "\n",
      "train/layer_1/biases1/Adam_1_1 \n",
      "\n",
      "save/Assign_13_1 \n",
      "\n",
      "train/layer_1/biases1/Adam_1/read_1 \n",
      "\n",
      "train/layer_1/biases1/Adam_1/Initializer/zeros_1 \n",
      "\n",
      "train/layer_1/biases1/Adam_1/Assign_1 \n",
      "\n",
      "train/layer_1/biases1/Adam_2 \n",
      "\n",
      "save/Assign_12_1 \n",
      "\n",
      "train/layer_1/biases1/Adam/read_1 \n",
      "\n",
      "train/layer_1/biases1/Adam/Initializer/zeros_1 \n",
      "\n",
      "train/layer_1/biases1/Adam/Assign_1 \n",
      "\n",
      "train/layer_1/weights1/Adam_1_1 \n",
      "\n",
      "save/Assign_15_1 \n",
      "\n",
      "train/layer_1/weights1/Adam_1/read_1 \n",
      "\n",
      "train/layer_1/weights1/Adam_1/Initializer/zeros_1 \n",
      "\n",
      "train/layer_1/weights1/Adam_1/Assign_1 \n",
      "\n",
      "train/layer_1/weights1/Adam_2 \n",
      "\n",
      "save/Assign_14_1 \n",
      "\n",
      "train/layer_1/weights1/Adam/read_1 \n",
      "\n",
      "train/layer_1/weights1/Adam/Initializer/zeros_1 \n",
      "\n",
      "train/layer_1/weights1/Adam/Assign_1 \n",
      "\n",
      "train/beta2_power_1 \n",
      "\n",
      "save/Assign_11_1 \n",
      "\n",
      "train/beta2_power/read_1 \n",
      "\n",
      "train/beta2_power/initial_value_1 \n",
      "\n",
      "train/beta2_power/Assign_1 \n",
      "\n",
      "train/beta1_power_1 \n",
      "\n",
      "save/Assign_10_1 \n",
      "\n",
      "train/beta1_power/read_1 \n",
      "\n",
      "train/beta1_power/initial_value_1 \n",
      "\n",
      "train/beta1_power/Assign_1 \n",
      "\n",
      "train/gradients/layer_1/add_grad/Shape_1_1 \n",
      "\n",
      "train/gradients/layer_2/add_grad/Shape_1_1 \n",
      "\n",
      "train/gradients/layer_3/add_grad/Shape_1_1 \n",
      "\n",
      "train/gradients/layer_4/add_grad/Shape_1_1 \n",
      "\n",
      "train/gradients/output/add_grad/Shape_1_1 \n",
      "\n",
      "train/gradients/cost/Mean_grad/Maximum/y_1 \n",
      "\n",
      "train/gradients/cost/Mean_grad/Const_1_1 \n",
      "\n",
      "train/gradients/cost/Mean_grad/Const_2 \n",
      "\n",
      "train/gradients/cost/Mean_grad/Shape_2_1 \n",
      "\n",
      "train/gradients/cost/Mean_grad/Prod_1_1 \n",
      "\n",
      "train/gradients/cost/Mean_grad/Maximum_1 \n",
      "\n",
      "train/gradients/cost/Mean_grad/Reshape/shape_1 \n",
      "\n",
      "train/gradients/Const_1 \n",
      "\n",
      "train/gradients/Shape_1 \n",
      "\n",
      "train/gradients/Fill_1 \n",
      "\n",
      "train/gradients/cost/Mean_grad/Reshape_1 \n",
      "\n",
      "cost/Const_1 \n",
      "\n",
      "cost/Placeholder_1 \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/Shape_1_1 \n",
      "\n",
      "output/biases5_1 \n",
      "\n",
      "save/Assign_8_1 \n",
      "\n",
      "output/biases5/read_1 \n",
      "\n",
      "output/biases5/Initializer/zeros_1 \n",
      "\n",
      "output/biases5/Assign_1 \n",
      "\n",
      "output/weights5_1 \n",
      "\n",
      "save/Assign_9_1 \n",
      "\n",
      "output/weights5/read_1 \n",
      "\n",
      "output/weights5/Initializer/random_uniform/max_1 \n",
      "\n",
      "output/weights5/Initializer/random_uniform/min_1 \n",
      "\n",
      "output/weights5/Initializer/random_uniform/sub_1 \n",
      "\n",
      "output/weights5/Initializer/random_uniform/shape_1 \n",
      "\n",
      "output/weights5/Initializer/random_uniform/RandomUniform_1 \n",
      "\n",
      "output/weights5/Initializer/random_uniform/mul_1 \n",
      "\n",
      "output/weights5/Initializer/random_uniform_1 \n",
      "\n",
      "output/weights5/Assign_1 \n",
      "\n",
      "layer_4/biases4_1 \n",
      "\n",
      "save/Assign_6_1 \n",
      "\n",
      "layer_4/biases4/read_1 \n",
      "\n",
      "layer_4/biases4/Initializer/zeros_1 \n",
      "\n",
      "layer_4/biases4/Assign_1 \n",
      "\n",
      "layer_4/weights4_1 \n",
      "\n",
      "save/Assign_7_1 \n",
      "\n",
      "layer_4/weights4/read_1 \n",
      "\n",
      "layer_4/weights4/Initializer/random_uniform/max_1 \n",
      "\n",
      "layer_4/weights4/Initializer/random_uniform/min_1 \n",
      "\n",
      "layer_4/weights4/Initializer/random_uniform/sub_1 \n",
      "\n",
      "layer_4/weights4/Initializer/random_uniform/shape_1 \n",
      "\n",
      "layer_4/weights4/Initializer/random_uniform/RandomUniform_1 \n",
      "\n",
      "layer_4/weights4/Initializer/random_uniform/mul_1 \n",
      "\n",
      "layer_4/weights4/Initializer/random_uniform_1 \n",
      "\n",
      "layer_4/weights4/Assign_1 \n",
      "\n",
      "layer_3/biases3_1 \n",
      "\n",
      "save/Assign_4_1 \n",
      "\n",
      "layer_3/biases3/read_1 \n",
      "\n",
      "layer_3/biases3/Initializer/zeros_1 \n",
      "\n",
      "layer_3/biases3/Assign_1 \n",
      "\n",
      "layer_3/weights3_1 \n",
      "\n",
      "save/Assign_5_1 \n",
      "\n",
      "layer_3/weights3/read_1 \n",
      "\n",
      "layer_3/weights3/Initializer/random_uniform/max_1 \n",
      "\n",
      "layer_3/weights3/Initializer/random_uniform/min_1 \n",
      "\n",
      "layer_3/weights3/Initializer/random_uniform/sub_1 \n",
      "\n",
      "layer_3/weights3/Initializer/random_uniform/shape_1 \n",
      "\n",
      "layer_3/weights3/Initializer/random_uniform/RandomUniform_1 \n",
      "\n",
      "layer_3/weights3/Initializer/random_uniform/mul_1 \n",
      "\n",
      "layer_3/weights3/Initializer/random_uniform_1 \n",
      "\n",
      "layer_3/weights3/Assign_1 \n",
      "\n",
      "layer_2/biases2_1 \n",
      "\n",
      "save/Assign_2_1 \n",
      "\n",
      "layer_2/biases2/read_1 \n",
      "\n",
      "layer_2/biases2/Initializer/zeros_1 \n",
      "\n",
      "layer_2/biases2/Assign_1 \n",
      "\n",
      "layer_2/weights2_1 \n",
      "\n",
      "save/Assign_3_1 \n",
      "\n",
      "layer_2/weights2/read_1 \n",
      "\n",
      "layer_2/weights2/Initializer/random_uniform/max_1 \n",
      "\n",
      "layer_2/weights2/Initializer/random_uniform/min_1 \n",
      "\n",
      "layer_2/weights2/Initializer/random_uniform/sub_1 \n",
      "\n",
      "layer_2/weights2/Initializer/random_uniform/shape_1 \n",
      "\n",
      "layer_2/weights2/Initializer/random_uniform/RandomUniform_1 \n",
      "\n",
      "layer_2/weights2/Initializer/random_uniform/mul_1 \n",
      "\n",
      "layer_2/weights2/Initializer/random_uniform_1 \n",
      "\n",
      "layer_2/weights2/Assign_1 \n",
      "\n",
      "layer_1/biases1_1 \n",
      "\n",
      "save/Assign_32 \n",
      "\n",
      "layer_1/biases1/read_1 \n",
      "\n",
      "layer_1/biases1/Initializer/zeros_1 \n",
      "\n",
      "layer_1/biases1/Assign_1 \n",
      "\n",
      "layer_1/weights1_1 \n",
      "\n",
      "save/Assign_1_1 \n",
      "\n",
      "save/restore_all_1 \n",
      "\n",
      "save/SaveV2_1 \n",
      "\n",
      "save/control_dependency_1 \n",
      "\n",
      "layer_1/weights1/read_1 \n",
      "\n",
      "layer_1/weights1/Initializer/random_uniform/max_1 \n",
      "\n",
      "layer_1/weights1/Initializer/random_uniform/min_1 \n",
      "\n",
      "layer_1/weights1/Initializer/random_uniform/sub_1 \n",
      "\n",
      "layer_1/weights1/Initializer/random_uniform/shape_1 \n",
      "\n",
      "layer_1/weights1/Initializer/random_uniform/RandomUniform_1 \n",
      "\n",
      "layer_1/weights1/Initializer/random_uniform/mul_1 \n",
      "\n",
      "layer_1/weights1/Initializer/random_uniform_1 \n",
      "\n",
      "layer_1/weights1/Assign_1 \n",
      "\n",
      "init_1 \n",
      "\n",
      "input/Placeholder_1 \n",
      "\n",
      "layer_1/MatMul_1 \n",
      "\n",
      "train/gradients/layer_1/add_grad/Shape_2 \n",
      "\n",
      "train/gradients/layer_1/add_grad/BroadcastGradientArgs_1 \n",
      "\n",
      "layer_1/add_1 \n",
      "\n",
      "layer_1/Relu_1 \n",
      "\n",
      "layer_2/MatMul_1 \n",
      "\n",
      "train/gradients/layer_2/add_grad/Shape_2 \n",
      "\n",
      "train/gradients/layer_2/add_grad/BroadcastGradientArgs_1 \n",
      "\n",
      "layer_2/add_1 \n",
      "\n",
      "layer_2/Relu_1 \n",
      "\n",
      "layer_3/MatMul_1 \n",
      "\n",
      "train/gradients/layer_3/add_grad/Shape_2 \n",
      "\n",
      "train/gradients/layer_3/add_grad/BroadcastGradientArgs_1 \n",
      "\n",
      "layer_3/add_1 \n",
      "\n",
      "layer_3/Relu_1 \n",
      "\n",
      "layer_4/MatMul_1 \n",
      "\n",
      "train/gradients/layer_4/add_grad/Shape_2 \n",
      "\n",
      "train/gradients/layer_4/add_grad/BroadcastGradientArgs_1 \n",
      "\n",
      "layer_4/add_1 \n",
      "\n",
      "layer_4/Relu_1 \n",
      "\n",
      "output/MatMul_1 \n",
      "\n",
      "train/gradients/output/add_grad/Shape_2 \n",
      "\n",
      "train/gradients/output/add_grad/BroadcastGradientArgs_1 \n",
      "\n",
      "output/add_1 \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/Shape_2 \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/BroadcastGradientArgs_1 \n",
      "\n",
      "cost/SquaredDifference_1 \n",
      "\n",
      "train/gradients/cost/Mean_grad/Shape_1_1 \n",
      "\n",
      "train/gradients/cost/Mean_grad/Prod_2 \n",
      "\n",
      "train/gradients/cost/Mean_grad/floordiv_1 \n",
      "\n",
      "train/gradients/cost/Mean_grad/Cast_1 \n",
      "\n",
      "train/gradients/cost/Mean_grad/Shape_3 \n",
      "\n",
      "train/gradients/cost/Mean_grad/Tile_1 \n",
      "\n",
      "train/gradients/cost/Mean_grad/truediv_1 \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/sub_1 \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/scalar_1 \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/mul_2 \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/mul_1_1 \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/Sum_1_1 \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/Reshape_1_1 \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/Neg_1 \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/Sum_2 \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/Reshape_2 \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/tuple/group_deps_1 \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/tuple/control_dependency_1_1 \n",
      "\n",
      "train/gradients/cost/SquaredDifference_grad/tuple/control_dependency_2 \n",
      "\n",
      "train/gradients/output/add_grad/Sum_1_1 \n",
      "\n",
      "train/gradients/output/add_grad/Reshape_1_1 \n",
      "\n",
      "train/gradients/output/add_grad/Sum_2 \n",
      "\n",
      "train/gradients/output/add_grad/Reshape_2 \n",
      "\n",
      "train/gradients/output/add_grad/tuple/group_deps_1 \n",
      "\n",
      "train/gradients/output/add_grad/tuple/control_dependency_1_1 \n",
      "\n",
      "train/Adam/update_output/biases5/ApplyAdam_1 \n",
      "\n",
      "train/gradients/output/add_grad/tuple/control_dependency_2 \n",
      "\n",
      "train/gradients/output/MatMul_grad/MatMul_1_1 \n",
      "\n",
      "train/gradients/output/MatMul_grad/MatMul_2 \n",
      "\n",
      "train/gradients/output/MatMul_grad/tuple/group_deps_1 \n",
      "\n",
      "train/gradients/output/MatMul_grad/tuple/control_dependency_1_1 \n",
      "\n",
      "train/Adam/update_output/weights5/ApplyAdam_1 \n",
      "\n",
      "train/gradients/output/MatMul_grad/tuple/control_dependency_2 \n",
      "\n",
      "train/gradients/layer_4/Relu_grad/ReluGrad_1 \n",
      "\n",
      "train/gradients/layer_4/add_grad/Sum_1_1 \n",
      "\n",
      "train/gradients/layer_4/add_grad/Reshape_1_1 \n",
      "\n",
      "train/gradients/layer_4/add_grad/Sum_2 \n",
      "\n",
      "train/gradients/layer_4/add_grad/Reshape_2 \n",
      "\n",
      "train/gradients/layer_4/add_grad/tuple/group_deps_1 \n",
      "\n",
      "train/gradients/layer_4/add_grad/tuple/control_dependency_1_1 \n",
      "\n",
      "train/Adam/update_layer_4/biases4/ApplyAdam_1 \n",
      "\n",
      "train/gradients/layer_4/add_grad/tuple/control_dependency_2 \n",
      "\n",
      "train/gradients/layer_4/MatMul_grad/MatMul_1_1 \n",
      "\n",
      "train/gradients/layer_4/MatMul_grad/MatMul_2 \n",
      "\n",
      "train/gradients/layer_4/MatMul_grad/tuple/group_deps_1 \n",
      "\n",
      "train/gradients/layer_4/MatMul_grad/tuple/control_dependency_1_1 \n",
      "\n",
      "train/Adam/update_layer_4/weights4/ApplyAdam_1 \n",
      "\n",
      "train/gradients/layer_4/MatMul_grad/tuple/control_dependency_2 \n",
      "\n",
      "train/gradients/layer_3/Relu_grad/ReluGrad_1 \n",
      "\n",
      "train/gradients/layer_3/add_grad/Sum_1_1 \n",
      "\n",
      "train/gradients/layer_3/add_grad/Reshape_1_1 \n",
      "\n",
      "train/gradients/layer_3/add_grad/Sum_2 \n",
      "\n",
      "train/gradients/layer_3/add_grad/Reshape_2 \n",
      "\n",
      "train/gradients/layer_3/add_grad/tuple/group_deps_1 \n",
      "\n",
      "train/gradients/layer_3/add_grad/tuple/control_dependency_1_1 \n",
      "\n",
      "train/Adam/update_layer_3/biases3/ApplyAdam_1 \n",
      "\n",
      "train/gradients/layer_3/add_grad/tuple/control_dependency_2 \n",
      "\n",
      "train/gradients/layer_3/MatMul_grad/MatMul_1_1 \n",
      "\n",
      "train/gradients/layer_3/MatMul_grad/MatMul_2 \n",
      "\n",
      "train/gradients/layer_3/MatMul_grad/tuple/group_deps_1 \n",
      "\n",
      "train/gradients/layer_3/MatMul_grad/tuple/control_dependency_1_1 \n",
      "\n",
      "train/Adam/update_layer_3/weights3/ApplyAdam_1 \n",
      "\n",
      "train/gradients/layer_3/MatMul_grad/tuple/control_dependency_2 \n",
      "\n",
      "train/gradients/layer_2/Relu_grad/ReluGrad_1 \n",
      "\n",
      "train/gradients/layer_2/add_grad/Sum_1_1 \n",
      "\n",
      "train/gradients/layer_2/add_grad/Reshape_1_1 \n",
      "\n",
      "train/gradients/layer_2/add_grad/Sum_2 \n",
      "\n",
      "train/gradients/layer_2/add_grad/Reshape_2 \n",
      "\n",
      "train/gradients/layer_2/add_grad/tuple/group_deps_1 \n",
      "\n",
      "train/gradients/layer_2/add_grad/tuple/control_dependency_1_1 \n",
      "\n",
      "train/Adam/update_layer_2/biases2/ApplyAdam_1 \n",
      "\n",
      "train/gradients/layer_2/add_grad/tuple/control_dependency_2 \n",
      "\n",
      "train/gradients/layer_2/MatMul_grad/MatMul_1_1 \n",
      "\n",
      "train/gradients/layer_2/MatMul_grad/MatMul_2 \n",
      "\n",
      "train/gradients/layer_2/MatMul_grad/tuple/group_deps_1 \n",
      "\n",
      "train/gradients/layer_2/MatMul_grad/tuple/control_dependency_1_1 \n",
      "\n",
      "train/Adam/update_layer_2/weights2/ApplyAdam_1 \n",
      "\n",
      "train/gradients/layer_2/MatMul_grad/tuple/control_dependency_2 \n",
      "\n",
      "train/gradients/layer_1/Relu_grad/ReluGrad_1 \n",
      "\n",
      "train/gradients/layer_1/add_grad/Sum_1_1 \n",
      "\n",
      "train/gradients/layer_1/add_grad/Reshape_1_1 \n",
      "\n",
      "train/gradients/layer_1/add_grad/Sum_2 \n",
      "\n",
      "train/gradients/layer_1/add_grad/Reshape_2 \n",
      "\n",
      "train/gradients/layer_1/add_grad/tuple/group_deps_1 \n",
      "\n",
      "train/gradients/layer_1/add_grad/tuple/control_dependency_1_1 \n",
      "\n",
      "train/Adam/update_layer_1/biases1/ApplyAdam_1 \n",
      "\n",
      "train/gradients/layer_1/add_grad/tuple/control_dependency_2 \n",
      "\n",
      "train/gradients/layer_1/MatMul_grad/MatMul_1_1 \n",
      "\n",
      "train/gradients/layer_1/MatMul_grad/MatMul_2 \n",
      "\n",
      "train/gradients/layer_1/MatMul_grad/tuple/group_deps_1 \n",
      "\n",
      "train/gradients/layer_1/MatMul_grad/tuple/control_dependency_1_1 \n",
      "\n",
      "train/Adam/update_layer_1/weights1/ApplyAdam_1 \n",
      "\n",
      "train/Adam/mul_1_1 \n",
      "\n",
      "train/Adam/Assign_1_1 \n",
      "\n",
      "train/Adam/mul_2 \n",
      "\n",
      "train/Adam/Assign_2 \n",
      "\n",
      "train/Adam_1 \n",
      "\n",
      "train/gradients/layer_1/MatMul_grad/tuple/control_dependency_2 \n",
      "\n",
      "cost/Mean_1 \n",
      "\n",
      "logging/current_cost_1 \n",
      "\n",
      "logging/Merge/MergeSummary_1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "export_path = './1587454858083_512_1024_1024_512_0.0001_5000_Val/exported_model/'\n",
    "model_path = \"./1587454858083_512_1024_1024_512_0.0001_5000_Val/logs/trained_model.ckpt\"\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "\n",
    "    tf.saved_model.loader.load(sess, ['serve'], export_path)\n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "    list_of_tuples = [op.values() for op in graph.get_operations()]\n",
    "\n",
    "\n",
    "    # print your graph's ops, if needed\n",
    "    print(graph.get_operations())\n",
    "    \n",
    "    loader = tf.train.import_meta_graph(model_path+'.meta')\n",
    "    loader.restore(sess, model_path)\n",
    "\n",
    "    tensor_name_list = [tensor.name for tensor in graph.as_graph_def().node]\n",
    "    for tensor_name in tensor_name_list:\n",
    "        print(tensor_name, '\\n')\n",
    "            \n",
    "    Y_predicted_scaled = sess.run('output/MatMul:0', feed_dict={'input/Placeholder:0': X_scaled_testing})\n",
    "    Y_train_pred=sess.run('output/MatMul:0', feed_dict={'input/Placeholder:0': X_scaled_training})\n",
    "    #y_pred = sess.run('earnings:0', feed_dict={'input:0': X_scaled_testing})\n",
    "    #Y_predicted_scaled = sess.run('earnings:0', feed_dict={X: X_scaled_testing[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.get_default_graph()\n",
    "list_of_tuples = [op.values() for op in graph.get_operations()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor 'save_1/RestoreV2_31/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_31/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_30/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_30/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_29/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_29/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_28/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_28/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_27/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_27/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_26/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_26/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_25/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_25/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_24/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_24/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_23/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_23/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_22/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_22/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_21/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_21/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_20/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_20/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_19/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_19/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_18/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_18/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_17/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_17/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_16/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_16/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_15/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_15/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_14/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_14/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_13/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_13/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_12/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_12/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_11/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_11/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_10/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_10/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_9/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_9/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_8/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_8/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_7/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_7/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_6/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_6/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_5/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_5/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_4/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_4/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_3/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_3/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_2/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_2/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_1/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_1/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/SaveV2/shape_and_slices:0' shape=(32,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/SaveV2/tensor_names:0' shape=(32,) dtype=string>,),\n",
       " (<tf.Tensor 'save_1/ShardedFilename/shard:0' shape=() dtype=int32>,),\n",
       " (<tf.Tensor 'save_1/num_shards:0' shape=() dtype=int32>,),\n",
       " (<tf.Tensor 'save_1/StringJoin/inputs_1:0' shape=() dtype=string>,),\n",
       " (<tf.Tensor 'save_1/Const:0' shape=() dtype=string>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_31:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_30:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_29:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_28:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_27:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_26:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_25:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_24:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_23:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_22:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_21:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_20:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_19:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_18:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_17:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_16:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_15:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_14:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_13:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_12:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_11:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_10:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_9:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_8:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_7:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_6:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_5:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_4:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_3:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_2:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2_1:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/RestoreV2:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save_1/StringJoin:0' shape=() dtype=string>,),\n",
       " (<tf.Tensor 'save_1/ShardedFilename:0' shape=() dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_31/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_31/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_30/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_30/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_29/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_29/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_28/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_28/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_27/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_27/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_26/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_26/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_25/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_25/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_24/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_24/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_23/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_23/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_22/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_22/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_21/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_21/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_20/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_20/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_19/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_19/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_18/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_18/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_17/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_17/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_16/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_16/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_15/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_15/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_14/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_14/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_13/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_13/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_12/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_12/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_11/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_11/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_10/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_10/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_9/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_9/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_8/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_8/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_7/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_7/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_6/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_6/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_5/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_5/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_4/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_4/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_3/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_3/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_2/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_2/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_1/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_1/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2/shape_and_slices:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2/tensor_names:0' shape=(1,) dtype=string>,),\n",
       " (<tf.Tensor 'save/SaveV2/shape_and_slices:0' shape=(32,) dtype=string>,),\n",
       " (<tf.Tensor 'save/SaveV2/tensor_names:0' shape=(32,) dtype=string>,),\n",
       " (<tf.Tensor 'save/Const:0' shape=() dtype=string>,),\n",
       " (<tf.Tensor 'save/RestoreV2_31:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_30:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_29:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_28:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_27:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_26:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_25:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_24:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_23:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_22:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_21:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_20:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_19:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_18:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_17:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_16:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_15:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_14:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_13:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_12:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_11:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_10:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_9:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_8:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_7:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_6:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_5:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_4:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_3:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_2:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2_1:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'save/RestoreV2:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'logging/current_cost/tags:0' shape=() dtype=string>,),\n",
       " (<tf.Tensor 'train/Adam/epsilon:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'train/Adam/beta2:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'train/Adam/beta1:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'train/Adam/learning_rate:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'train/output/biases5/Adam_1:0' shape=(1,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_29:0' shape=(1,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_29:0' shape=(1,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/output/biases5/Adam_1/read:0' shape=(1,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/output/biases5/Adam_1/Initializer/zeros:0' shape=(1,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/output/biases5/Adam_1/Assign:0' shape=(1,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/output/biases5/Adam:0' shape=(1,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_28:0' shape=(1,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_28:0' shape=(1,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/output/biases5/Adam/read:0' shape=(1,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/output/biases5/Adam/Initializer/zeros:0' shape=(1,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/output/biases5/Adam/Assign:0' shape=(1,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/output/weights5/Adam_1:0' shape=(512, 1) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_31:0' shape=(512, 1) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_31:0' shape=(512, 1) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/output/weights5/Adam_1/read:0' shape=(512, 1) dtype=float32>,),\n",
       " (<tf.Tensor 'train/output/weights5/Adam_1/Initializer/zeros:0' shape=(512, 1) dtype=float32>,),\n",
       " (<tf.Tensor 'train/output/weights5/Adam_1/Assign:0' shape=(512, 1) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/output/weights5/Adam:0' shape=(512, 1) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_30:0' shape=(512, 1) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_30:0' shape=(512, 1) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/output/weights5/Adam/read:0' shape=(512, 1) dtype=float32>,),\n",
       " (<tf.Tensor 'train/output/weights5/Adam/Initializer/zeros:0' shape=(512, 1) dtype=float32>,),\n",
       " (<tf.Tensor 'train/output/weights5/Adam/Assign:0' shape=(512, 1) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_4/biases4/Adam_1:0' shape=(512,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_25:0' shape=(512,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_25:0' shape=(512,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_4/biases4/Adam_1/read:0' shape=(512,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_4/biases4/Adam_1/Initializer/zeros:0' shape=(512,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_4/biases4/Adam_1/Assign:0' shape=(512,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_4/biases4/Adam:0' shape=(512,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_24:0' shape=(512,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_24:0' shape=(512,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_4/biases4/Adam/read:0' shape=(512,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_4/biases4/Adam/Initializer/zeros:0' shape=(512,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_4/biases4/Adam/Assign:0' shape=(512,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_4/weights4/Adam_1:0' shape=(1024, 512) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_27:0' shape=(1024, 512) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_27:0' shape=(1024, 512) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_4/weights4/Adam_1/read:0' shape=(1024, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_4/weights4/Adam_1/Initializer/zeros:0' shape=(1024, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_4/weights4/Adam_1/Assign:0' shape=(1024, 512) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_4/weights4/Adam:0' shape=(1024, 512) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_26:0' shape=(1024, 512) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_26:0' shape=(1024, 512) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_4/weights4/Adam/read:0' shape=(1024, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_4/weights4/Adam/Initializer/zeros:0' shape=(1024, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_4/weights4/Adam/Assign:0' shape=(1024, 512) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_3/biases3/Adam_1:0' shape=(1024,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_21:0' shape=(1024,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_21:0' shape=(1024,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_3/biases3/Adam_1/read:0' shape=(1024,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_3/biases3/Adam_1/Initializer/zeros:0' shape=(1024,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_3/biases3/Adam_1/Assign:0' shape=(1024,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_3/biases3/Adam:0' shape=(1024,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_20:0' shape=(1024,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_20:0' shape=(1024,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_3/biases3/Adam/read:0' shape=(1024,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_3/biases3/Adam/Initializer/zeros:0' shape=(1024,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_3/biases3/Adam/Assign:0' shape=(1024,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_3/weights3/Adam_1:0' shape=(1024, 1024) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_23:0' shape=(1024, 1024) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_23:0' shape=(1024, 1024) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_3/weights3/Adam_1/read:0' shape=(1024, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_3/weights3/Adam_1/Initializer/zeros:0' shape=(1024, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_3/weights3/Adam_1/Assign:0' shape=(1024, 1024) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_3/weights3/Adam:0' shape=(1024, 1024) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_22:0' shape=(1024, 1024) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_22:0' shape=(1024, 1024) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_3/weights3/Adam/read:0' shape=(1024, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_3/weights3/Adam/Initializer/zeros:0' shape=(1024, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_3/weights3/Adam/Assign:0' shape=(1024, 1024) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_2/biases2/Adam_1:0' shape=(1024,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_17:0' shape=(1024,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_17:0' shape=(1024,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_2/biases2/Adam_1/read:0' shape=(1024,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_2/biases2/Adam_1/Initializer/zeros:0' shape=(1024,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_2/biases2/Adam_1/Assign:0' shape=(1024,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_2/biases2/Adam:0' shape=(1024,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_16:0' shape=(1024,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_16:0' shape=(1024,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_2/biases2/Adam/read:0' shape=(1024,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_2/biases2/Adam/Initializer/zeros:0' shape=(1024,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_2/biases2/Adam/Assign:0' shape=(1024,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_2/weights2/Adam_1:0' shape=(512, 1024) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_19:0' shape=(512, 1024) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_19:0' shape=(512, 1024) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_2/weights2/Adam_1/read:0' shape=(512, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_2/weights2/Adam_1/Initializer/zeros:0' shape=(512, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_2/weights2/Adam_1/Assign:0' shape=(512, 1024) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_2/weights2/Adam:0' shape=(512, 1024) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_18:0' shape=(512, 1024) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_18:0' shape=(512, 1024) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_2/weights2/Adam/read:0' shape=(512, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_2/weights2/Adam/Initializer/zeros:0' shape=(512, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_2/weights2/Adam/Assign:0' shape=(512, 1024) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_1/biases1/Adam_1:0' shape=(512,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_13:0' shape=(512,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_13:0' shape=(512,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_1/biases1/Adam_1/read:0' shape=(512,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_1/biases1/Adam_1/Initializer/zeros:0' shape=(512,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_1/biases1/Adam_1/Assign:0' shape=(512,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_1/biases1/Adam:0' shape=(512,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_12:0' shape=(512,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_12:0' shape=(512,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_1/biases1/Adam/read:0' shape=(512,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_1/biases1/Adam/Initializer/zeros:0' shape=(512,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_1/biases1/Adam/Assign:0' shape=(512,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_1/weights1/Adam_1:0' shape=(70, 512) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_15:0' shape=(70, 512) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_15:0' shape=(70, 512) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_1/weights1/Adam_1/read:0' shape=(70, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_1/weights1/Adam_1/Initializer/zeros:0' shape=(70, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_1/weights1/Adam_1/Assign:0' shape=(70, 512) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_1/weights1/Adam:0' shape=(70, 512) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_14:0' shape=(70, 512) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_14:0' shape=(70, 512) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/layer_1/weights1/Adam/read:0' shape=(70, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_1/weights1/Adam/Initializer/zeros:0' shape=(70, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'train/layer_1/weights1/Adam/Assign:0' shape=(70, 512) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/beta2_power:0' shape=() dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_11:0' shape=() dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_11:0' shape=() dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/beta2_power/read:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'train/beta2_power/initial_value:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'train/beta2_power/Assign:0' shape=() dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/beta1_power:0' shape=() dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_10:0' shape=() dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_10:0' shape=() dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/beta1_power/read:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'train/beta1_power/initial_value:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'train/beta1_power/Assign:0' shape=() dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/gradients/layer_1/add_grad/Shape_1:0' shape=(1,) dtype=int32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_2/add_grad/Shape_1:0' shape=(1,) dtype=int32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_3/add_grad/Shape_1:0' shape=(1,) dtype=int32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_4/add_grad/Shape_1:0' shape=(1,) dtype=int32>,),\n",
       " (<tf.Tensor 'train/gradients/output/add_grad/Shape_1:0' shape=(1,) dtype=int32>,),\n",
       " (<tf.Tensor 'train/gradients/cost/Mean_grad/Maximum/y:0' shape=() dtype=int32>,),\n",
       " (<tf.Tensor 'train/gradients/cost/Mean_grad/Const_1:0' shape=(1,) dtype=int32>,),\n",
       " (<tf.Tensor 'train/gradients/cost/Mean_grad/Const:0' shape=(1,) dtype=int32>,),\n",
       " (<tf.Tensor 'train/gradients/cost/Mean_grad/Shape_2:0' shape=(0,) dtype=int32>,),\n",
       " (<tf.Tensor 'train/gradients/cost/Mean_grad/Prod_1:0' shape=() dtype=int32>,),\n",
       " (<tf.Tensor 'train/gradients/cost/Mean_grad/Maximum:0' shape=() dtype=int32>,),\n",
       " (<tf.Tensor 'train/gradients/cost/Mean_grad/Reshape/shape:0' shape=(2,) dtype=int32>,),\n",
       " (<tf.Tensor 'train/gradients/Const:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/Shape:0' shape=(0,) dtype=int32>,),\n",
       " (<tf.Tensor 'train/gradients/Fill:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/cost/Mean_grad/Reshape:0' shape=(1, 1) dtype=float32>,),\n",
       " (<tf.Tensor 'cost/Const:0' shape=(2,) dtype=int32>,),\n",
       " (<tf.Tensor 'cost/Placeholder:0' shape=(?, 1) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/cost/SquaredDifference_grad/Shape_1:0' shape=(2,) dtype=int32>,),\n",
       " (<tf.Tensor 'output/biases5:0' shape=(1,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_8:0' shape=(1,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_8:0' shape=(1,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'output/biases5/read:0' shape=(1,) dtype=float32>,),\n",
       " (<tf.Tensor 'output/biases5/Initializer/zeros:0' shape=(1,) dtype=float32>,),\n",
       " (<tf.Tensor 'output/biases5/Assign:0' shape=(1,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'output/weights5:0' shape=(512, 1) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_9:0' shape=(512, 1) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_9:0' shape=(512, 1) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'output/weights5/read:0' shape=(512, 1) dtype=float32>,),\n",
       " (<tf.Tensor 'output/weights5/Initializer/random_uniform/max:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'output/weights5/Initializer/random_uniform/min:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'output/weights5/Initializer/random_uniform/sub:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'output/weights5/Initializer/random_uniform/shape:0' shape=(2,) dtype=int32>,),\n",
       " (<tf.Tensor 'output/weights5/Initializer/random_uniform/RandomUniform:0' shape=(512, 1) dtype=float32>,),\n",
       " (<tf.Tensor 'output/weights5/Initializer/random_uniform/mul:0' shape=(512, 1) dtype=float32>,),\n",
       " (<tf.Tensor 'output/weights5/Initializer/random_uniform:0' shape=(512, 1) dtype=float32>,),\n",
       " (<tf.Tensor 'output/weights5/Assign:0' shape=(512, 1) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'layer_4/biases4:0' shape=(512,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_6:0' shape=(512,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_6:0' shape=(512,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'layer_4/biases4/read:0' shape=(512,) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_4/biases4/Initializer/zeros:0' shape=(512,) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_4/biases4/Assign:0' shape=(512,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'layer_4/weights4:0' shape=(1024, 512) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_7:0' shape=(1024, 512) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_7:0' shape=(1024, 512) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'layer_4/weights4/read:0' shape=(1024, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_4/weights4/Initializer/random_uniform/max:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'layer_4/weights4/Initializer/random_uniform/min:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'layer_4/weights4/Initializer/random_uniform/sub:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'layer_4/weights4/Initializer/random_uniform/shape:0' shape=(2,) dtype=int32>,),\n",
       " (<tf.Tensor 'layer_4/weights4/Initializer/random_uniform/RandomUniform:0' shape=(1024, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_4/weights4/Initializer/random_uniform/mul:0' shape=(1024, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_4/weights4/Initializer/random_uniform:0' shape=(1024, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_4/weights4/Assign:0' shape=(1024, 512) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'layer_3/biases3:0' shape=(1024,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_4:0' shape=(1024,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_4:0' shape=(1024,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'layer_3/biases3/read:0' shape=(1024,) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_3/biases3/Initializer/zeros:0' shape=(1024,) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_3/biases3/Assign:0' shape=(1024,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'layer_3/weights3:0' shape=(1024, 1024) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_5:0' shape=(1024, 1024) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_5:0' shape=(1024, 1024) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'layer_3/weights3/read:0' shape=(1024, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_3/weights3/Initializer/random_uniform/max:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'layer_3/weights3/Initializer/random_uniform/min:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'layer_3/weights3/Initializer/random_uniform/sub:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'layer_3/weights3/Initializer/random_uniform/shape:0' shape=(2,) dtype=int32>,),\n",
       " (<tf.Tensor 'layer_3/weights3/Initializer/random_uniform/RandomUniform:0' shape=(1024, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_3/weights3/Initializer/random_uniform/mul:0' shape=(1024, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_3/weights3/Initializer/random_uniform:0' shape=(1024, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_3/weights3/Assign:0' shape=(1024, 1024) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'layer_2/biases2:0' shape=(1024,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_2:0' shape=(1024,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_2:0' shape=(1024,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'layer_2/biases2/read:0' shape=(1024,) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_2/biases2/Initializer/zeros:0' shape=(1024,) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_2/biases2/Assign:0' shape=(1024,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'layer_2/weights2:0' shape=(512, 1024) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_3:0' shape=(512, 1024) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign_3:0' shape=(512, 1024) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'layer_2/weights2/read:0' shape=(512, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_2/weights2/Initializer/random_uniform/max:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'layer_2/weights2/Initializer/random_uniform/min:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'layer_2/weights2/Initializer/random_uniform/sub:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'layer_2/weights2/Initializer/random_uniform/shape:0' shape=(2,) dtype=int32>,),\n",
       " (<tf.Tensor 'layer_2/weights2/Initializer/random_uniform/RandomUniform:0' shape=(512, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_2/weights2/Initializer/random_uniform/mul:0' shape=(512, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_2/weights2/Initializer/random_uniform:0' shape=(512, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_2/weights2/Assign:0' shape=(512, 1024) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'layer_1/biases1:0' shape=(512,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign:0' shape=(512,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save/Assign:0' shape=(512,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'layer_1/biases1/read:0' shape=(512,) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_1/biases1/Initializer/zeros:0' shape=(512,) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_1/biases1/Assign:0' shape=(512,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'layer_1/weights1:0' shape=(70, 512) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'save_1/Assign_1:0' shape=(70, 512) dtype=float32_ref>,),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (<tf.Tensor 'save_1/control_dependency:0' shape=() dtype=string>,),\n",
       " (<tf.Tensor 'save_1/MergeV2Checkpoints/checkpoint_prefixes:0' shape=(1,) dtype=string>,),\n",
       " (),\n",
       " (<tf.Tensor 'save_1/Identity:0' shape=() dtype=string>,),\n",
       " (<tf.Tensor 'save/Assign_1:0' shape=(70, 512) dtype=float32_ref>,),\n",
       " (),\n",
       " (),\n",
       " (<tf.Tensor 'save/control_dependency:0' shape=() dtype=string>,),\n",
       " (<tf.Tensor 'layer_1/weights1/read:0' shape=(70, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_1/weights1/Initializer/random_uniform/max:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'layer_1/weights1/Initializer/random_uniform/min:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'layer_1/weights1/Initializer/random_uniform/sub:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'layer_1/weights1/Initializer/random_uniform/shape:0' shape=(2,) dtype=int32>,),\n",
       " (<tf.Tensor 'layer_1/weights1/Initializer/random_uniform/RandomUniform:0' shape=(70, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_1/weights1/Initializer/random_uniform/mul:0' shape=(70, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_1/weights1/Initializer/random_uniform:0' shape=(70, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_1/weights1/Assign:0' shape=(70, 512) dtype=float32_ref>,),\n",
       " (),\n",
       " (<tf.Tensor 'input/Placeholder:0' shape=(?, 70) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_1/MatMul:0' shape=(?, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_1/add_grad/Shape:0' shape=(2,) dtype=int32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_1/add_grad/BroadcastGradientArgs:0' shape=(?,) dtype=int32>,\n",
       "  <tf.Tensor 'train/gradients/layer_1/add_grad/BroadcastGradientArgs:1' shape=(?,) dtype=int32>),\n",
       " (<tf.Tensor 'layer_1/add:0' shape=(?, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_1/Relu:0' shape=(?, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_2/MatMul:0' shape=(?, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_2/add_grad/Shape:0' shape=(2,) dtype=int32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_2/add_grad/BroadcastGradientArgs:0' shape=(?,) dtype=int32>,\n",
       "  <tf.Tensor 'train/gradients/layer_2/add_grad/BroadcastGradientArgs:1' shape=(?,) dtype=int32>),\n",
       " (<tf.Tensor 'layer_2/add:0' shape=(?, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_2/Relu:0' shape=(?, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_3/MatMul:0' shape=(?, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_3/add_grad/Shape:0' shape=(2,) dtype=int32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_3/add_grad/BroadcastGradientArgs:0' shape=(?,) dtype=int32>,\n",
       "  <tf.Tensor 'train/gradients/layer_3/add_grad/BroadcastGradientArgs:1' shape=(?,) dtype=int32>),\n",
       " (<tf.Tensor 'layer_3/add:0' shape=(?, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_3/Relu:0' shape=(?, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_4/MatMul:0' shape=(?, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_4/add_grad/Shape:0' shape=(2,) dtype=int32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_4/add_grad/BroadcastGradientArgs:0' shape=(?,) dtype=int32>,\n",
       "  <tf.Tensor 'train/gradients/layer_4/add_grad/BroadcastGradientArgs:1' shape=(?,) dtype=int32>),\n",
       " (<tf.Tensor 'layer_4/add:0' shape=(?, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'layer_4/Relu:0' shape=(?, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'output/MatMul:0' shape=(?, 1) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/output/add_grad/Shape:0' shape=(2,) dtype=int32>,),\n",
       " (<tf.Tensor 'train/gradients/output/add_grad/BroadcastGradientArgs:0' shape=(?,) dtype=int32>,\n",
       "  <tf.Tensor 'train/gradients/output/add_grad/BroadcastGradientArgs:1' shape=(?,) dtype=int32>),\n",
       " (<tf.Tensor 'output/add:0' shape=(?, 1) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/cost/SquaredDifference_grad/Shape:0' shape=(2,) dtype=int32>,),\n",
       " (<tf.Tensor 'train/gradients/cost/SquaredDifference_grad/BroadcastGradientArgs:0' shape=(?,) dtype=int32>,\n",
       "  <tf.Tensor 'train/gradients/cost/SquaredDifference_grad/BroadcastGradientArgs:1' shape=(?,) dtype=int32>),\n",
       " (<tf.Tensor 'cost/SquaredDifference:0' shape=(?, 1) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/cost/Mean_grad/Shape_1:0' shape=(2,) dtype=int32>,),\n",
       " (<tf.Tensor 'train/gradients/cost/Mean_grad/Prod:0' shape=() dtype=int32>,),\n",
       " (<tf.Tensor 'train/gradients/cost/Mean_grad/floordiv:0' shape=() dtype=int32>,),\n",
       " (<tf.Tensor 'train/gradients/cost/Mean_grad/Cast:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/cost/Mean_grad/Shape:0' shape=(2,) dtype=int32>,),\n",
       " (<tf.Tensor 'train/gradients/cost/Mean_grad/Tile:0' shape=(?, 1) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/cost/Mean_grad/truediv:0' shape=(?, 1) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/cost/SquaredDifference_grad/sub:0' shape=(?, 1) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/cost/SquaredDifference_grad/scalar:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/cost/SquaredDifference_grad/mul:0' shape=(?, 1) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/cost/SquaredDifference_grad/mul_1:0' shape=(?, 1) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/cost/SquaredDifference_grad/Sum_1:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/cost/SquaredDifference_grad/Reshape_1:0' shape=(?, 1) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/cost/SquaredDifference_grad/Neg:0' shape=(?, 1) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/cost/SquaredDifference_grad/Sum:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/cost/SquaredDifference_grad/Reshape:0' shape=(?, 1) dtype=float32>,),\n",
       " (),\n",
       " (<tf.Tensor 'train/gradients/cost/SquaredDifference_grad/tuple/control_dependency_1:0' shape=(?, 1) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/cost/SquaredDifference_grad/tuple/control_dependency:0' shape=(?, 1) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/output/add_grad/Sum_1:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/output/add_grad/Reshape_1:0' shape=(1,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/output/add_grad/Sum:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/output/add_grad/Reshape:0' shape=(?, 1) dtype=float32>,),\n",
       " (),\n",
       " (<tf.Tensor 'train/gradients/output/add_grad/tuple/control_dependency_1:0' shape=(1,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/Adam/update_output/biases5/ApplyAdam:0' shape=(1,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/gradients/output/add_grad/tuple/control_dependency:0' shape=(?, 1) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/output/MatMul_grad/MatMul_1:0' shape=(512, 1) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/output/MatMul_grad/MatMul:0' shape=(?, 512) dtype=float32>,),\n",
       " (),\n",
       " (<tf.Tensor 'train/gradients/output/MatMul_grad/tuple/control_dependency_1:0' shape=(512, 1) dtype=float32>,),\n",
       " (<tf.Tensor 'train/Adam/update_output/weights5/ApplyAdam:0' shape=(512, 1) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/gradients/output/MatMul_grad/tuple/control_dependency:0' shape=(?, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_4/Relu_grad/ReluGrad:0' shape=(?, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_4/add_grad/Sum_1:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_4/add_grad/Reshape_1:0' shape=(512,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_4/add_grad/Sum:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_4/add_grad/Reshape:0' shape=(?, 512) dtype=float32>,),\n",
       " (),\n",
       " (<tf.Tensor 'train/gradients/layer_4/add_grad/tuple/control_dependency_1:0' shape=(512,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/Adam/update_layer_4/biases4/ApplyAdam:0' shape=(512,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/gradients/layer_4/add_grad/tuple/control_dependency:0' shape=(?, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_4/MatMul_grad/MatMul_1:0' shape=(1024, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_4/MatMul_grad/MatMul:0' shape=(?, 1024) dtype=float32>,),\n",
       " (),\n",
       " (<tf.Tensor 'train/gradients/layer_4/MatMul_grad/tuple/control_dependency_1:0' shape=(1024, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'train/Adam/update_layer_4/weights4/ApplyAdam:0' shape=(1024, 512) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/gradients/layer_4/MatMul_grad/tuple/control_dependency:0' shape=(?, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_3/Relu_grad/ReluGrad:0' shape=(?, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_3/add_grad/Sum_1:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_3/add_grad/Reshape_1:0' shape=(1024,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_3/add_grad/Sum:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_3/add_grad/Reshape:0' shape=(?, 1024) dtype=float32>,),\n",
       " (),\n",
       " (<tf.Tensor 'train/gradients/layer_3/add_grad/tuple/control_dependency_1:0' shape=(1024,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/Adam/update_layer_3/biases3/ApplyAdam:0' shape=(1024,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/gradients/layer_3/add_grad/tuple/control_dependency:0' shape=(?, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_3/MatMul_grad/MatMul_1:0' shape=(1024, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_3/MatMul_grad/MatMul:0' shape=(?, 1024) dtype=float32>,),\n",
       " (),\n",
       " (<tf.Tensor 'train/gradients/layer_3/MatMul_grad/tuple/control_dependency_1:0' shape=(1024, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'train/Adam/update_layer_3/weights3/ApplyAdam:0' shape=(1024, 1024) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/gradients/layer_3/MatMul_grad/tuple/control_dependency:0' shape=(?, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_2/Relu_grad/ReluGrad:0' shape=(?, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_2/add_grad/Sum_1:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_2/add_grad/Reshape_1:0' shape=(1024,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_2/add_grad/Sum:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_2/add_grad/Reshape:0' shape=(?, 1024) dtype=float32>,),\n",
       " (),\n",
       " (<tf.Tensor 'train/gradients/layer_2/add_grad/tuple/control_dependency_1:0' shape=(1024,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/Adam/update_layer_2/biases2/ApplyAdam:0' shape=(1024,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/gradients/layer_2/add_grad/tuple/control_dependency:0' shape=(?, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_2/MatMul_grad/MatMul_1:0' shape=(512, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_2/MatMul_grad/MatMul:0' shape=(?, 512) dtype=float32>,),\n",
       " (),\n",
       " (<tf.Tensor 'train/gradients/layer_2/MatMul_grad/tuple/control_dependency_1:0' shape=(512, 1024) dtype=float32>,),\n",
       " (<tf.Tensor 'train/Adam/update_layer_2/weights2/ApplyAdam:0' shape=(512, 1024) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/gradients/layer_2/MatMul_grad/tuple/control_dependency:0' shape=(?, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_1/Relu_grad/ReluGrad:0' shape=(?, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_1/add_grad/Sum_1:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_1/add_grad/Reshape_1:0' shape=(512,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_1/add_grad/Sum:0' shape=<unknown> dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_1/add_grad/Reshape:0' shape=(?, 512) dtype=float32>,),\n",
       " (),\n",
       " (<tf.Tensor 'train/gradients/layer_1/add_grad/tuple/control_dependency_1:0' shape=(512,) dtype=float32>,),\n",
       " (<tf.Tensor 'train/Adam/update_layer_1/biases1/ApplyAdam:0' shape=(512,) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/gradients/layer_1/add_grad/tuple/control_dependency:0' shape=(?, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_1/MatMul_grad/MatMul_1:0' shape=(70, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'train/gradients/layer_1/MatMul_grad/MatMul:0' shape=(?, 70) dtype=float32>,),\n",
       " (),\n",
       " (<tf.Tensor 'train/gradients/layer_1/MatMul_grad/tuple/control_dependency_1:0' shape=(70, 512) dtype=float32>,),\n",
       " (<tf.Tensor 'train/Adam/update_layer_1/weights1/ApplyAdam:0' shape=(70, 512) dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/Adam/mul_1:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'train/Adam/Assign_1:0' shape=() dtype=float32_ref>,),\n",
       " (<tf.Tensor 'train/Adam/mul:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'train/Adam/Assign:0' shape=() dtype=float32_ref>,),\n",
       " (),\n",
       " (<tf.Tensor 'train/gradients/layer_1/MatMul_grad/tuple/control_dependency:0' shape=(?, 70) dtype=float32>,),\n",
       " (<tf.Tensor 'cost/Mean:0' shape=() dtype=float32>,),\n",
       " (<tf.Tensor 'logging/current_cost:0' shape=() dtype=string>,),\n",
       " (<tf.Tensor 'logging/Merge/MergeSummary:0' shape=() dtype=string>,)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_tuples.find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.01])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_scaled_testing[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.086184], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predicted_scaled[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.0794425],\n",
       "       [7.7880044],\n",
       "       [6.4178386],\n",
       "       ...,\n",
       "       [3.766553 ],\n",
       "       [4.324222 ],\n",
       "       [3.6537979]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Deep neural network Trained on Original Data\n",
      "Average absolute error: 0.9 degrees.\n",
      "Testing Accuracy: 76.23 %.\n"
     ]
    }
   ],
   "source": [
    "errors = abs(Y_predicted_scaled - Y_scaled_testing)\n",
    "\n",
    "print('Metrics for Deep neural network Trained on Original Data')\n",
    "print('Average absolute error:', round(np.mean(errors), 2), 'degrees.')\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / Y_scaled_testing)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Testing Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Deep neural network Trained on Original Data\n",
      "Average absolute error: 0.58 degrees.\n",
      "training Accuracy: 84.8 %.\n"
     ]
    }
   ],
   "source": [
    "errors = abs(Y_train_pred- Y_scaled_training)\n",
    "\n",
    "print('Metrics for Deep neural network Trained on Original Data')\n",
    "print('Average absolute error:', round(np.mean(errors), 2), 'degrees.')\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / Y_scaled_training)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('training Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circumplex_model=[{'High Arousal positive Valence':[{'Excited':},{'Delighted':},{'Happy':}]},{'Low Arousal positive Valence':[{'Content':},{'Relaxed':},{'Calm':}]},{'High Arousal negative Valence'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "twod_Model={'Excited':(6,8),'Delighted':(7,7),'Happy':(8,6),'Content':(8,4),'Relaxed':(7,3),'Calm':(6,2),'tired':(4,2),'bored':(3,3),'Depressed':(2,4),'Frustrated':(2,6),'Angry':(3,7),'Taste':(4,8)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(val): \n",
    "    for key, value in twod_Model.items(): \n",
    "         if val == value: \n",
    "             return key \n",
    "  \n",
    "    return \"key doesn't exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.7958307, 8.486149 ]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "zbc=loaded_model.predict(X_valid[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sav=np.round_(zbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_val_score(x):\n",
    "    return x[0][0]\n",
    "def extract_arousal_score(x):\n",
    "    return x[0][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "val1=int(extract_val_score(np.round_(zbc)))\n",
    "arous1=int(extract_arousal_score(np.round_(zbc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib.ticker import AutoLocator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"./images/123.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy2imgxy = lambda x,y: (img.size[0] * x / np.max(9),\\\n",
    "                        img.size[1] * (np.max(9) - y) / np.max(tickly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticklx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticklx = np.linspace(1,10,10)\n",
    "tickly = np.linspace(1,11,10)\n",
    "tickpx,tickpy = xy2imgxy(ticklx,tickly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "xAxis=list(np.linspace(1,576,10))\n",
    "yAxis=list(np.linspace(1,388,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576, 388)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.size # returns (x, y) of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 64.88888888888889,\n",
       " 128.77777777777777,\n",
       " 192.66666666666666,\n",
       " 256.55555555555554,\n",
       " 320.44444444444446,\n",
       " 384.3333333333333,\n",
       " 448.2222222222222,\n",
       " 512.1111111111111,\n",
       " 576.0]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xAxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 44.0, 87.0, 130.0, 173.0, 216.0, 259.0, 302.0, 345.0, 388.0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yAxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20, 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,az = plt.subplots()\n",
    "az.imshow(np.flipud(img), cmap='gray', origin='lower')\n",
    "\n",
    "# Rewrite x,y ticks\n",
    "\n",
    "az.set_xticks(ticklx)\n",
    "az.set_yticks(tickly)\n",
    "az.set_xticklabels(ticklx.astype('int'))\n",
    "az.set_yticklabels(tickly.astype('int'))\n",
    "az.yaxis.set_major_locator(AutoLocator())\n",
    "az.xaxis.set_major_locator(AutoLocator())\n",
    "plt.scatter(xAxis[val1], yAxis[arous1],s=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
