{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walkthrough of Data Science - Traveler\n",
    "\n",
    "### * Goal: Predict the country that users will make their first booking in, based on some basic user profile data.\n",
    "\n",
    "#### * Training data, set of users with correct category (i.e. what country they made their first booking in).\n",
    "\n",
    "#### * Build a model to accurately predict the country of first booking.\n",
    "\n",
    "#### * Test data, set of users without the knowledge of outcome.\n",
    "\n",
    "#### * Task in hand: Preprocessing, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walkthrough process\n",
    "\n",
    "#### [1] Pre-processing: Assessing and analyzing data, cleaning, transforming and adding new features\n",
    "#### [2] Learning model: Constructing and testing learning model\n",
    "#### [3] Post-processing: Creating final predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAB 1 CODE - DATA PREPROCESSING (Data Cleaning and Data Transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Exploring Traveler data\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline \n",
    "\n",
    "print(\"Reading data...\")\n",
    "train_file = \"./traveler_dataset/train_users_2.csv\"\n",
    "df_train = pd.read_csv(train_file, header = 0,index_col=None)\n",
    "\n",
    "test_file = \"./traveler_dataset/test_users.csv\"\n",
    "df_test = pd.read_csv(test_file, header = 0,index_col=None)\n",
    "\n",
    "# Combining into one dataset for cleaning\n",
    "df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True, sort=False)\n",
    "print(\"Reading data...completed\")\n",
    "\n",
    "# Fixing date formats in Pandas - to_datetime\n",
    "## Change dates to specific format\n",
    "print(\"Fixing timestamps...\")\n",
    "df_all['date_account_created'] = pd.to_datetime(df_all['date_account_created'], format='%Y-%m-%d')\n",
    "df_all['timestamp_first_active'] = pd.to_datetime(df_all['timestamp_first_active'], format='%Y%m%d%H%M%S')\n",
    "print(\"Fixing timestamps...completed\")\n",
    "\n",
    "## Removing date_first_booking column\n",
    "df_all.drop('date_first_booking', axis = 1, inplace = True)\n",
    "print(\"Droped date_first_booking column...\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "## Remove outliers function - [1]\n",
    "def remove_outliers(df, column, min_val, max_val):\n",
    "    col_values = df[column].values\n",
    "    df[column] = np.where(np.logical_or(col_values<=min_val, col_values>=max_val), np.NaN, col_values)\n",
    "    return df\n",
    "\n",
    "## Fixing age column - [2]\n",
    "print(\"Fixing age column...\")\n",
    "df_all = remove_outliers(df = df_all, column = 'age', min_val = 15, max_val = 90)\n",
    "df_all['age'].fillna(-1, inplace = True)\n",
    "print(\"Fixing age column...completed\")\n",
    "\n",
    "# Other column missing value - Fill first_affiliate_tracked column\n",
    "print(\"Filling first_affiliate_tracked column...\")\n",
    "df_all['first_affiliate_tracked'].fillna(-1, inplace=True)\n",
    "print(\"Filling first_affiliate_tracked column...completed\")\n",
    "\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Transformation and Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Own implementation of One Hot Encoding - Data Transformation\n",
    "def convert_to_binary(df, column_to_convert):\n",
    "    categories = list(df[column_to_convert].drop_duplicates())\n",
    "\n",
    "    for category in categories:\n",
    "        cat_name = str(category).replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"/\", \"_\").replace(\"-\", \"\").lower()\n",
    "        col_name = column_to_convert[:5] + '_' + cat_name[:10]\n",
    "        df[col_name] = 0\n",
    "        df.loc[(df[column_to_convert] == category), col_name] = 1\n",
    "\n",
    "    return df\n",
    "\n",
    "# One Hot Encoding\n",
    "print(\"One Hot Encoding categorical data...\")\n",
    "columns_to_convert = ['gender', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser']\n",
    "\n",
    "for column in columns_to_convert:\n",
    "    df_all = convert_to_binary(df=df_all, column_to_convert=column)\n",
    "    df_all.drop(column, axis=1, inplace=True)\n",
    "print(\"One Hot Encoding categorical data...completed\")\n",
    "\n",
    "# Add new date related fields - Creating New Features\n",
    "print(\"Adding new fields...\")\n",
    "df_all['day_account_created'] = df_all['date_account_created'].dt.weekday\n",
    "df_all['month_account_created'] = df_all['date_account_created'].dt.month\n",
    "df_all['quarter_account_created'] = df_all['date_account_created'].dt.quarter\n",
    "df_all['year_account_created'] = df_all['date_account_created'].dt.year\n",
    "df_all['hour_first_active'] = df_all['timestamp_first_active'].dt.hour\n",
    "df_all['day_first_active'] = df_all['timestamp_first_active'].dt.weekday\n",
    "df_all['month_first_active'] = df_all['timestamp_first_active'].dt.month\n",
    "df_all['quarter_first_active'] = df_all['timestamp_first_active'].dt.quarter\n",
    "df_all['year_first_active'] = df_all['timestamp_first_active'].dt.year\n",
    "df_all['created_less_active'] = (df_all['date_account_created'] - df_all['timestamp_first_active']).dt.days\n",
    "print(\"Adding new fields...completed\")\n",
    "\n",
    "\n",
    "# Drop unnecessary columns\n",
    "print(\"Droping fields...\")\n",
    "columns_to_drop = ['date_account_created', 'timestamp_first_active', 'date_first_booking', 'country_destination']\n",
    "for column in columns_to_drop:\n",
    "    if column in df_all.columns:\n",
    "        df_all.drop(column, axis=1, inplace=True)\n",
    "print(\"Droping fields...completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Next? - Data Integration \n",
    "- Adding new data **sessions.csv** to **df_all** dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Milestone: Understanding the sessions.csv data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading sessions.csv data\n",
    "print(\"Reading sessions data...\")\n",
    "sessions_file = \"./traveler_dataset/sessions.csv\"\n",
    "df_sessions = pd.read_csv(sessions_file, header = 0,index_col=False)\n",
    "print(\"Reading sessions data...completed\")\n",
    "df_sessions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sessions.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "\n",
    "   - While combining sessions data with the training data, we need to aggregate the sessions data so that there is one row per user (as opposed to many rows for each user)\n",
    "   \n",
    "   - Two attributes are striking: **device_type** and **secs_elapsed**\n",
    "      - These attributes provide important information that could help to more accurately predict which country a user will make a first booking in.\n",
    "      \n",
    "   - For example, assume that people spending relatively little time to make a booking on a phone are likely to be making bookings in locations closer to home (i.e. US) than someone spending more time to make a booking on a desktop computer. \n",
    "   \n",
    "   - Remember this is just a theory that needs to be proven, but it is a good reason to ensure it helps in final training dataset.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Milestone: Cleaning and Transforming the Data\n",
    "\n",
    "- **Goal:** We need to get the final data into a format that can be merged with the data created for **df_all** (i.e., training data + test data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1] Extract the primary and secondary devices for each user\n",
    "\n",
    "- How do we determine what a userâ€™s primary and secondary devices are?\n",
    "\n",
    "- **Solution:** Look at how much time user's spent on each device\n",
    "\n",
    "<img src=\"./images/primary_secondary_device_type.jpg\" height=\"500\" width=\"600\"/>\n",
    "\n",
    "- **Note:** By aggregating the data this way, we are also implicitly removing the missing values. (**Hint:** ref. Pandas Groupby) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine primary device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Determing primary device...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection of all rows for 'user_id', 'device_type', 'secs_elapsed' using .loc operation\n",
    "sessions_device = df_sessions.loc[:, ['user_id', 'device_type', 'secs_elapsed']]\n",
    "sessions_device.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping based on 'user_id', 'device_type' the sum of 'secs_elapsed' \n",
    "aggregated_lvl1 = sessions_device.groupby(['user_id', 'device_type'], as_index=False, sort=False).aggregate(np.sum)\n",
    "aggregated_lvl1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the index which is true or false based on first largest device type used by users as matching condition\n",
    "idx = aggregated_lvl1.groupby(['user_id'], sort=False)['secs_elapsed'].transform(max) == aggregated_lvl1['secs_elapsed']\n",
    "idx.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the rows of first largest device type used by users based on index\n",
    "df_sessions_primary = pd.DataFrame(aggregated_lvl1.loc[idx, ['user_id', 'device_type', 'secs_elapsed']])\n",
    "df_sessions_primary.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the attributes and modify in the df_sessions_primary dataframe\n",
    "df_sessions_primary.rename(columns = {'device_type':'primary_device','secs_elapsed':'primary_secs'}, inplace=True)\n",
    "df_sessions_primary.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation: \n",
    "- Now convert the primary_device attribute (**df_sessions_primary['primary_device']**) from categorical to numeric form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call user defined One Hot Encoding function\n",
    "df_sessions_primary = convert_to_binary(df=df_sessions_primary, column_to_convert='primary_device')\n",
    "df_sessions_primary.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the 'primary_device' attribute after one-hot encoding\n",
    "df_sessions_primary.drop('primary_device', axis=1, inplace=True)\n",
    "df_sessions_primary.head(10)\n",
    "print(\"Determing primary device...completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking ...\n",
    "df_sessions_primary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1: Determine Secondary device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Determing secondary device...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initially drop the primary device index before selecting the remaining device (eg: secondary device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start code\n",
    "# Obtaining the index which is true or false based on second largest device type used by users as matching condition\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the rows of second largest device type used by users based on index\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the attributes and modify in the df_sessions_secondary dataframe\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### End code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2: Transformation \n",
    "- Now convert the secondary_device attribute from categorical into numeric form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call user defined One Hot Encoding function\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the 'secondary_device' attribute after one-hot encoding\n",
    "\n",
    "\n",
    "\n",
    "print(\"Determing secondary device...completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking...\n",
    "df_sessions_secondary.head()\n",
    "### End code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sessions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3: \n",
    "- Display number of rows and columns in \n",
    "   - df_sessions_primary\n",
    "   - df_sessions_secondary\n",
    "   - df_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### End code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [2] Determine Counts of Actions\n",
    "- What? Counts of how many times each action was taken by each user\n",
    "\n",
    "- **Solution:** Two-step process\n",
    "\n",
    "- **Step1:** Determine the count of each action for each user\n",
    "\n",
    "<img src=\"./images/Count_of_actions_Step1.jpg\" height=\"500\" width=\"600\"/>\n",
    "\n",
    "- **Step2:** pivot table\n",
    "\n",
    "<img src=\"./images/Count_of_actions_Step2.jpg\" height=\"500\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [3] Looping Through the action attributes\n",
    "\n",
    "- **Problem:** Transformation works for one action attribute at a time, but in the data we have three action attributes: **action, action_type** and **action_detail**\n",
    "\n",
    "\n",
    "- **Solution:** Repeat **determine counts of actions ([2])** steps for each attribute individually, effectively by creating three separate tables.\n",
    "\n",
    "\n",
    "- **Effective Approach:** Since this now creates tables where each row represents one user, *JOIN* (SQL concept) these three tables together on the basis of the **user-id**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sessions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of step 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of value in a column\n",
    "def convert_to_counts(df, id_col, column_to_convert):\n",
    "    #id_list = df[id_col].drop_duplicates()\n",
    "    #print (id_list.head())\n",
    "    #Step1\n",
    "    df_counts = df.loc[:,[id_col, column_to_convert]]  \n",
    "    df_counts['count'] = 1\n",
    "    df_counts = df_counts.groupby(by=[id_col, column_to_convert], as_index=False, sort=False).sum()\n",
    "    print('Step1')\n",
    "    print (df_counts.head())\n",
    "    #Step2\n",
    "    new_df = df_counts.pivot(index=id_col, columns=column_to_convert, values='count') \n",
    "    new_df = new_df.fillna(0)\n",
    "    print ('Step2')\n",
    "    print (new_df.head())\n",
    "\n",
    "    # Rename Columns\n",
    "    categories = list(df[column_to_convert].drop_duplicates())\n",
    "    print('categories')\n",
    "    print (categories)\n",
    "    for category in categories:\n",
    "        cat_name = str(category).replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"/\", \"_\").replace(\"-\", \"\").lower()\n",
    "        col_name = column_to_convert + '_' + cat_name\n",
    "        new_df.rename(columns = {category:col_name}, inplace=True)\n",
    "        \n",
    "    return new_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate and combine actions taken columns\n",
    "print(\"Aggregating actions taken...\")\n",
    "session_actions = df_sessions.loc[:,['user_id', 'action', 'action_type', 'action_detail']]\n",
    "#session_actions = df_sessions.loc[:,['user_id', 'action']]\n",
    "session_actions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns_to_convert = ['action']\n",
    "columns_to_convert = ['action', 'action_type', 'action_detail']\n",
    "session_actions = session_actions.fillna('not provided')\n",
    "session_actions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = True\n",
    "for column in columns_to_convert:\n",
    "    print(\"Converting \" + column + \" attribute...\")\n",
    "    current_data = convert_to_counts(df=session_actions, id_col='user_id', column_to_convert=column)\n",
    "    print(\"Converting \" + column + \" attribute... finished\")\n",
    "\n",
    "    # If first loop, current data becomes existing data, otherwise merge existing and current\n",
    "    if first:\n",
    "        first = False\n",
    "        actions_data = current_data\n",
    "    else:\n",
    "        actions_data = pd.concat([actions_data, current_data], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Checking...\n",
    "actions_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### [4] Finally, combine datasets (Data Integration)\n",
    "\n",
    "- [4.1] First, combine the two device dataframes (**df_primary** and **df_secondary**) to create a **device** dataframe. \n",
    "\n",
    "- [4.2] Then, combine the **device** dataframe with the **actions** dataframe to create a **sessions** dataframe with all the features extracted from *sessions.csv*\n",
    "\n",
    "- [4.3] Finally, combine the **sessions** dataframe with the **users**(train + test) dataframe computed earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4.1] Merge device datasets\n",
    "print(\"Combining df_primary and df_secondary to device dataframe...\")\n",
    "df_sessions_primary.set_index('user_id', inplace=True)\n",
    "df_sessions_secondary.set_index('user_id', inplace=True)\n",
    "device_data = pd.concat([df_sessions_primary, df_sessions_secondary], axis=1, join=\"outer\", sort=False)\n",
    "print(\"Combining df_primary and df_secondary to device dataframe...finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Note on Outer Join\n",
    "\n",
    "- [4.1*] Requires outer join because not all users have a secondary device. Doing an outer join here ensures that the dataset includes all users regardless of this fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4.2] Merge device and actions datasets\n",
    "print(\"Combining device and actions to sessions dataframe...\")\n",
    "combined_results = pd.concat([device_data, actions_data], axis=1, join='outer', sort=False)\n",
    "df_sessions_complete = combined_results.fillna(0)\n",
    "print(\"Combining device and actions to sessions dataframe...finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sessions_complete.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Note on Outer Join\n",
    "\n",
    "- [4.2*] Requires outer join just to ensure that if a user is missing from one of the datasets (for whatever reason), we will still capture them.\n",
    "\n",
    "- Fill any missing values with 0's to ensure not to have any NULL values that may have been generated by these outer joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4.3] Merge user and session datasets\n",
    "print(\"Combining sessions and users to get final dataframe...\")\n",
    "df_all.set_index('id', inplace=True)\n",
    "df_all = pd.concat([df_all, df_sessions_complete], axis=1, join='inner', sort = False)\n",
    "print(\"Combining sessions and users to get final dataframe...finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Note on Inner Join\n",
    "\n",
    "- [4.3*] Requires inner join i.e., want to have final training dataset to only include users that also have sessions data (filters others).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4:\n",
    " - Perform data pre-processing on your CP dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
