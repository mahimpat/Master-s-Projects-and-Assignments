{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [COM6513] Assignment 1: Sentiment Analysis with Logistic Regression\n",
    "\n",
    "### Instructor: Nikos Aletras\n",
    "\n",
    "\n",
    "The goal of this assignment is to develop and test a **text classification** system for **sentiment analysis**, in particular to predict the sentiment of movie reviews, i.e. positive or negative (binary classification).\n",
    "\n",
    "\n",
    "\n",
    "For that purpose, you will implement:\n",
    "\n",
    "\n",
    "- Text processing methods for extracting Bag-Of-Word features, using \n",
    "    - n-grams (BOW), i.e. unigrams, bigrams and trigrams to obtain vector representations of documents where n=1,2,3 respectively. Two vector weighting schemes should be tested: (1) raw frequencies (**1 mark**); (2) tf.idf (**1 mark**). \n",
    "    - character n-grams (BOCN). A character n-gram is a contiguous sequence of characters given a word, e.g. for n=2, 'coffee' is split into {'co', 'of', 'ff', 'fe', 'ee'}. Two vector weighting schemes should be tested: (1) raw frequencies (**1 mark**); (2) tf.idf (**1 mark**). **Tip: Note the large vocabulary size!** \n",
    "    - a combination of the two vector spaces (n-grams and character n-grams) choosing your best performing wighting respectively (i.e. raw or tfidf). (**1 mark**) **Tip: you should merge the two representations**\n",
    "\n",
    "\n",
    "\n",
    "- Binary Logistic Regression (LR) classifiers that will be able to accurately classify movie reviews trained with: \n",
    "    - (1) BOW-count (raw frequencies) \n",
    "    - (2) BOW-tfidf (tf.idf weighted)\n",
    "    - (3) BOCN-count\n",
    "    - (4) BOCN-tfidf\n",
    "    - (5) BOW+BOCN (best performing weighting; raw or tfidf)\n",
    "\n",
    "\n",
    "\n",
    "- The Stochastic Gradient Descent (SGD) algorithm to estimate the parameters of your Logistic Regression models. Your SGD algorithm should:\n",
    "    - Minimise the Binary Cross-entropy loss function (**1 mark**)\n",
    "    - Use L2 regularisation (**1 mark**)\n",
    "    - Perform multiple passes (epochs) over the training data (**1 mark**)\n",
    "    - Randomise the order of training data after each pass (**1 mark**)\n",
    "    - Stop training if the difference between the current and previous development loss is smaller than a threshold (**1 mark**)\n",
    "    - After each epoch print the training and development loss (**1 mark**)\n",
    "\n",
    "\n",
    "\n",
    "- Discuss how did you choose hyperparameters (e.g. learning rate and regularisation strength) for each LR model? You should use a table showing model performance using different set of hyperparameter values. (**2 marks). **Tip: Instead of using all possible combinations, you could perform a random sampling of combinations.**\n",
    "\n",
    "\n",
    "- After training each LR model, plot the learning process (i.e. training and validation loss in each epoch) using a line plot. Does your model underfit, overfit or is it about right? Explain why. (**1 mark**). \n",
    "\n",
    "\n",
    "- Identify and show the most important features (model interpretability) for each class (i.e. top-10 most positive and top-10 negative weights). Give the top 10 for each class and comment on whether they make sense (if they don't you might have a bug!). If you were to apply the classifier into a different domain such laptop reviews or restaurant reviews, do you think these features would generalise well? Can you propose what features the classifier could pick up as important in the new domain? (**2 marks**)\n",
    "\n",
    "\n",
    "- Provide well documented and commented code describing all of your choices. In general, you are free to make decisions about text processing (e.g. punctuation, numbers, vocabulary size) and hyperparameter values. We expect to see justifications and discussion for all of your choices (**2 marks**). \n",
    "\n",
    "\n",
    "- Provide efficient solutions by using Numpy arrays when possible (you can find tips in Lab 1 sheet). Executing the whole notebook with your code should not take more than 5 minutes on a any standard computer (e.g. Intel Core i5 CPU, 8 or 16GB RAM) excluding hyperparameter tuning runs (**2 marks**). \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Data \n",
    "\n",
    "The data you will use are taken from here: [http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/) and you can find it in the `./data_sentiment` folder in CSV format:\n",
    "\n",
    "- `data_sentiment/train.csv`: contains 1,400 reviews, 700 positive (label: 1) and 700 negative (label: 0) to be used for training.\n",
    "- `data_sentiment/dev.csv`: contains 200 reviews, 100 positive and 100 negative to be used for hyperparameter selection and monitoring the training process.\n",
    "- `data_sentiment/test.csv`: contains 400 reviews, 200 positive and 200 negative to be used for testing.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Submission Instructions\n",
    "\n",
    "You should submit a Jupyter Notebook file (assignment1.ipynb) and an exported PDF version (you can do it from Jupyter: `File->Download as->PDF via Latex` or you can print it as PDF using your browser).\n",
    "\n",
    "You are advised to follow the code structure given in this notebook by completing all given funtions. You can also write any auxilliary/helper functions (and arguments for the functions) that you might need but note that you can provide a full solution without any such functions. Similarly, you can just use only the packages imported below but you are free to use any functionality from the [Python Standard Library](https://docs.python.org/2/library/index.html), NumPy, SciPy (excluding built-in softmax funtcions) and Pandas. You are not allowed to use any third-party library such as Scikit-learn (apart from metric functions already provided), NLTK, Spacy, Keras etc.. \n",
    "\n",
    "There is no single correct answer on what your accuracy should be, but correct implementations usually achieve F1-scores around 80\\% or higher. The quality of the analysis of the results is as important as the accuracy itself. \n",
    "\n",
    "This assignment will be marked out of 20. It is worth 20\\% of your final grade in the module.\n",
    "\n",
    "The deadline for this assignment is **23:59 on Mon, 14 Mar 2022** and it needs to be submitted via Blackboard. Standard departmental penalties for lateness will be applied. We use a range of strategies to **detect [unfair means](https://www.sheffield.ac.uk/ssid/unfair-means/index)**, including Turnitin which helps detect plagiarism. Use of unfair means would result in getting a failing grade.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PATH=/Library/TeX/texbin:$PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T13:41:17.642162Z",
     "start_time": "2020-03-27T13:41:16.891940Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "\n",
    "# fixing random seed for reproducibility\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw texts and labels into arrays\n",
    "\n",
    "First, you need to load the training, development and test sets from their corresponding CSV files (tip: you can use Pandas dataframes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:28.145788Z",
     "start_time": "2020-02-15T14:17:28.066100Z"
    }
   },
   "outputs": [],
   "source": [
    "#load data into dataframe using pandas\n",
    "train_data = pd.read_csv('data_sentiment/train.csv',header=None)\n",
    "\n",
    "train_data.columns=['raw_text','label']\n",
    "\n",
    "test_data = pd.read_csv('data_sentiment/test.csv',header=None)\n",
    "\n",
    "test_data.columns=['raw_text','label']\n",
    "\n",
    "dev_data = pd.read_csv('data_sentiment/dev.csv',header=None)\n",
    "\n",
    "dev_data.columns=['raw_text','label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>note : some may consider portions of the follo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>note : some may consider portions of the follo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>every once in a while you see a film that is s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when i was growing up in 1970s , boys in my sc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the muppet movie is the first , and the best m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            raw_text  label\n",
       "0  note : some may consider portions of the follo...      1\n",
       "1  note : some may consider portions of the follo...      1\n",
       "2  every once in a while you see a film that is s...      1\n",
       "3  when i was growing up in 1970s , boys in my sc...      1\n",
       "4  the muppet movie is the first , and the best m...      1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wild things is a suspenseful thriller starring...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i know it already opened in december , but i f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what's shocking about \" carlito's way \" is how...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uncompromising french director robert bresson'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aggressive , bleak , and unrelenting film abou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            raw_text  label\n",
       "0  wild things is a suspenseful thriller starring...      1\n",
       "1  i know it already opened in december , but i f...      1\n",
       "2  what's shocking about \" carlito's way \" is how...      1\n",
       "3  uncompromising french director robert bresson'...      1\n",
       "4  aggressive , bleak , and unrelenting film abou...      1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>if he doesn=92t watch out , mel gibson is in d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wong kar-wei's \" fallen angels \" is , on a pur...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is nothing like american history x in th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>an unhappy italian housewife , a lonely waiter...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when people are talking about good old times ,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            raw_text  label\n",
       "0  if he doesn=92t watch out , mel gibson is in d...      1\n",
       "1  wong kar-wei's \" fallen angels \" is , on a pur...      1\n",
       "2  there is nothing like american history x in th...      1\n",
       "3  an unhappy italian housewife , a lonely waiter...      1\n",
       "4  when people are talking about good old times ,...      1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use Pandas you can see a sample of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to put the raw texts into Python lists and their corresponding labels into NumPy arrays:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:31.115577Z",
     "start_time": "2020-02-15T14:17:31.108038Z"
    }
   },
   "outputs": [],
   "source": [
    "#converting the data to list and labels to numpy array\n",
    "\n",
    "train_text = train_data['raw_text'].tolist()\n",
    "train_labels = train_data['label'].to_numpy()\n",
    "test_text = test_data['raw_text'].tolist()\n",
    "test_labels = test_data['label'].to_numpy()\n",
    "dev_text = dev_data['raw_text'].tolist()\n",
    "dev_labels = dev_data['label'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Representations of Text \n",
    "\n",
    "\n",
    "To train and test Logisitc Regression models, you first need to obtain vector representations for all documents given a vocabulary of features (unigrams, bigrams, trigrams).\n",
    "\n",
    "\n",
    "## Text Pre-Processing Pipeline\n",
    "\n",
    "To obtain a vocabulary of features, you should: \n",
    "- tokenise all texts into a list of unigrams (tip: using a regular expression) \n",
    "- remove stop words (using the one provided or one of your preference) \n",
    "- compute bigrams, trigrams given the remaining unigrams (or character ngrams from the unigrams)\n",
    "- remove ngrams appearing in less than K documents\n",
    "- use the remaining to create a vocabulary of unigrams, bigrams and trigrams (or character n-grams). You can keep top N if you encounter memory issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:31.860420Z",
     "start_time": "2020-02-15T14:17:31.855439Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = ['a','in','on','at','and','or', \n",
    "              'to', 'the', 'of', 'an', 'by', \n",
    "              'as', 'is', 'was', 'were', 'been', 'be', \n",
    "              'are','for', 'this', 'that', 'these', 'those', 'you', 'i',\n",
    "             'it', 'he', 'she', 'we', 'they', 'will', 'have', 'has',\n",
    "              'do', 'did', 'can', 'could', 'who', 'which', 'what', \n",
    "             'his', 'her', 'they', 'them', 'from', 'with', 'its']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Data preprocessing\n",
    "x_raw_train=[]\n",
    "x_raw_test=[]\n",
    "x_raw_dev=[]\n",
    "x_raw_train_char=[]\n",
    "x_raw_test_char=[]\n",
    "x_raw_dev_char=[]\n",
    "for i in range(0,len(train_data)):\n",
    "    y=train_data['raw_text'][i]\n",
    "#replace special characters with space for word ngrams\n",
    "    x_raw = re.sub(r'[^a-zA-Z0-9\\s]', ' ', y)\n",
    "#just remove \\ symbol because it has no meaning (i have left the other special characters as \n",
    "#they have some meaning in char ngrams)\n",
    "    x_raw_char = re.sub(r'\\+','',y)\n",
    "    x_raw_train_char.append(x_raw_char)\n",
    "#replace mulitple spaces with single space for word ngrams\n",
    "    x_raw_t = re.sub(r' +',' ',x_raw)\n",
    "    x_raw_train.append(x_raw_t)\n",
    "   \n",
    "\n",
    "\n",
    " #Dev Data preprocessing\n",
    "for i in range(0,len(dev_data)):\n",
    "    y=dev_data['raw_text'][i]\n",
    "\n",
    "#replace special characters with space\n",
    "    x_raw = re.sub(r'[^a-zA-Z0-9\\s]', ' ', y)\n",
    "#just remove \\ symbol because it has no meaning (i have left the other special characters as \n",
    "#they have some meaning in char ngrams)\n",
    "    x_raw_char = re.sub(r'\\+','',y)\n",
    "    x_raw_test_char.append(x_raw_char)\n",
    "#replace mulitple spaces with single space\n",
    "    x_raw_d = re.sub(r' +',' ',x_raw)\n",
    "    x_raw_dev.append(x_raw_d)\n",
    "\n",
    "    \n",
    "    \n",
    "#test Data preprocessing\n",
    "for i in range(0,len(test_data)):\n",
    "    y=test_data['raw_text'][i]\n",
    "\n",
    "#replace special characters with space\n",
    "    x_raw = re.sub(r'[^a-zA-Z0-9\\s]', ' ', y)\n",
    "#just remove \\ symbol because it has no meaning (i have left the other special characters as \n",
    "#they have some meaning in char ngrams)\n",
    "    x_raw_char = re.sub(r'\\+','',y)\n",
    "    x_raw_dev_char.append(x_raw_char)\n",
    "#replace mulitple spaces with single space\n",
    "    x_raw_t = re.sub(r' +',' ',x_raw)\n",
    "    x_raw_test.append(x_raw_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram extraction from a document\n",
    "\n",
    "You first need to implement the `extract_ngrams` function. It takes as input:\n",
    "- `x_raw`: a string corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
    "- `char_ngrams`: boolean. If true the function extracts character n-grams\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `x': a list of all extracted features.\n",
    "\n",
    "See the examples below to see how this function should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:33.169090Z",
     "start_time": "2020-02-15T14:17:33.161268Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_ngrams(x_raw, ngram_range=(1,3), token_pattern=r' ', \n",
    "                   stop_words=[], vocab=set(), char_ngrams=False):\n",
    "    #convert to lowercase\n",
    "    x_raw = x_raw.lower()\n",
    "    tokens=[]\n",
    "    tokens_new=[]\n",
    "    tuple_list=[]\n",
    "    new_token_list=[]\n",
    "    sep=' '\n",
    "  \n",
    "    \n",
    "    #Extract character ngrams \n",
    "    if char_ngrams==True:\n",
    "        \n",
    "        x_raw =re.sub(r' +','',x_raw) \n",
    "        for i in range(ngram_range[0],ngram_range[1]+1):\n",
    "            tokens.extend(x_raw[z:z+i] for z in range(len(x_raw)-i+1))\n",
    "        extracted_ngrams=tokens\n",
    "        \n",
    "    #Extract word ngrams\n",
    "    else:\n",
    "        tokens = [token for token in x_raw.split(token_pattern) if token != \"\"]\n",
    "    \n",
    "        x_raw_new = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "        for i in range(ngram_range[0],ngram_range[1]+1):\n",
    "            tokens_new.extend(x_raw_new[z:z+i] for z in range(len(x_raw_new)-i+1))\n",
    "\n",
    "        for i in tokens_new:\n",
    "            tuple_list.append(tuple(i))\n",
    "        extracted_ngrams=tuple_list\n",
    "        \n",
    "    #return extracted ngrams from given vocabulary or return the full list of ngrams\n",
    "    return [ngram for ngram in extracted_ngrams if ngram in vocab] if vocab else extracted_ngrams\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is OK to represent n-grams using lists instead of tuples: e.g. `['great', ['great', 'movie']]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For extracting character n-grams the function should work as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=extract_ngrams(x_raw_test[0],\n",
    "               ngram_range=(1,3),\n",
    "               stop_words=stop_words,\n",
    "                vocab=train_vocab,\n",
    "               char_ngrams=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=extract_ngrams(\"-- movie\", \n",
    "               ngram_range=(2,4), \n",
    "               stop_words=[],\n",
    "                 vocab=set(),\n",
    "               char_ngrams=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--',\n",
       " '- ',\n",
       " ' m',\n",
       " 'mo',\n",
       " 'ov',\n",
       " 'vi',\n",
       " 'ie',\n",
       " '-- ',\n",
       " '- m',\n",
       " ' mo',\n",
       " 'mov',\n",
       " 'ovi',\n",
       " 'vie',\n",
       " '-- m',\n",
       " '- mo',\n",
       " ' mov',\n",
       " 'movi',\n",
       " 'ovie']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocabulary \n",
    "\n",
    "The `get_vocab` function will be used to (1) create a vocabulary of ngrams; (2) count the document frequencies of ngrams; (3) their raw frequency. It takes as input:\n",
    "- `X_raw`: a list of strings each corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `min_df`: keep ngrams with a minimum document frequency.\n",
    "- `keep_topN`: keep top-N more frequent ngrams.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `vocab`: a set of the n-grams that will be used as features.\n",
    "- `df`: a Counter (or dict) that contains ngrams as keys and their corresponding document frequency as values.\n",
    "- `ngram_counts`: counts of each ngram in vocab\n",
    "\n",
    "Hint: it should make use of the `extract_ngrams` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:35.821240Z",
     "start_time": "2020-02-15T14:17:35.814722Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_vocab(X_raw, ngram_range=(1,3), token_pattern=r' ', \n",
    "              min_df=0, keep_topN=0, \n",
    "              stop_words=[],char_ngrams=False):\n",
    "    \n",
    "    df = Counter()\n",
    "    ngram_counter = Counter()\n",
    "    #iterate over the whole list of documents\n",
    "    for doc in X_raw:\n",
    "        vocab_list = extract_ngrams(doc,ngram_range=ngram_range,stop_words=stop_words,vocab=set(),char_ngrams=char_ngrams)\n",
    "        #vocab=vocab_list\n",
    "        \n",
    "    \n",
    "        #calculate document frequency of ngram using counter\n",
    "        df.update(vocab_list)\n",
    "\n",
    "        #calculate count of each ngram in vocab\n",
    "        ngram_counter.update(ngram for ngram in vocab_list if df[ngram]>=min_df)\n",
    "        \n",
    "        #Get topN ngrams\n",
    "        vocab_temp=ngram_counter.most_common(keep_topN)\n",
    "        \n",
    "        #create a set of vocabulary of all top N ngrams \n",
    "        vocab = {i[0] for i in vocab_temp}\n",
    "        \n",
    "    \n",
    "    #return vocabulary, document frequency, ngram count\n",
    "    return vocab, df, ngram_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should use `get_vocab` to create your vocabulary and get document and raw frequencies of n-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:39.319793Z",
     "start_time": "2020-02-15T14:17:36.836545Z"
    }
   },
   "outputs": [],
   "source": [
    "train_vocab,train_df,train_ngram_counts = get_vocab(x_raw_train,ngram_range=(1,3),stop_words=stop_words,min_df=1,keep_topN=10000,char_ngrams=False)\n",
    "_,dev_df, _ = get_vocab(x_raw_dev, keep_topN=10000)\n",
    "_,test_df, _ = get_vocab(x_raw_test, keep_topN=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create vocab and get document frequencies for char n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_char_vocab,train_char_df,train_char_counts = get_vocab(x_raw_train_char,ngram_range=(2,6),stop_words=stop_words,min_df=5,keep_topN=10000,char_ngrams=True)\n",
    "_,dev_char_df, _ = get_vocab(x_raw_dev_char, keep_topN=10000,char_ngrams=True)\n",
    "_,test_char_df, _ = get_vocab(x_raw_test_char, keep_topN=10000,char_ngrams=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you need to create 2 dictionaries: (1) vocabulary id -> word; and  (2) word -> vocabulary id so you can use them for reference:\n",
    "\n",
    "i have created another 2 dictionaries that have vocabulary id->chars; and chars-> vocabulary id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function returns 2 dictionaries with vocab_id -> word and word -> vocab_id \n",
    "def vocab2word(vocab):\n",
    "    v2w={}\n",
    "    w2v={}\n",
    "    for idx,word in enumerate(vocab):\n",
    "        v2w[idx]=word\n",
    "        w2v[word]=idx\n",
    "    #print(v2w)\n",
    "    \n",
    "    return v2w,w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:39.326811Z",
     "start_time": "2020-02-15T14:17:39.322256Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_word = dict()\n",
    "word_vocab = dict()\n",
    "vocab_char={}\n",
    "char_vocab={}\n",
    "\n",
    "vocab_word,word_vocab =vocab2word(train_vocab)\n",
    "vocab_char,char_vocab =vocab2word(train_char_vocab)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should be able to extract n-grams for each text in the training, development and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:40.213253Z",
     "start_time": "2020-02-15T14:17:39.329147Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ngrams_list=[]\n",
    "train_char_list=[]\n",
    "test_ngrams_list=[]\n",
    "test_char_list=[]\n",
    "dev_ngrams_list=[]\n",
    "dev_char_list=[]\n",
    "for doc in x_raw_train:\n",
    "    train_ngrams_list.append(extract_ngrams(doc,ngram_range=(1,3),token_pattern=r' ',stop_words=stop_words,vocab=train_vocab,char_ngrams=False))\n",
    "    train_char_list.append(extract_ngrams(doc,ngram_range=(2,6),token_pattern=r' ',stop_words=stop_words,vocab=train_char_vocab,char_ngrams=True))\n",
    "for doc in x_raw_test:\n",
    "    test_ngrams_list.append(extract_ngrams(doc,ngram_range=(1,3),token_pattern=r' ',stop_words=stop_words,vocab=train_vocab,char_ngrams=False))\n",
    "    test_char_list.append(extract_ngrams(doc,ngram_range=(2,6),token_pattern=r' ',stop_words=stop_words,vocab=train_char_vocab,char_ngrams=True))\n",
    "for doc in x_raw_dev:\n",
    "    dev_ngrams_list.append(extract_ngrams(doc,ngram_range=(1,3),token_pattern=r' ',stop_words=stop_words,vocab=train_vocab,char_ngrams=False))\n",
    "    dev_char_list.append(extract_ngrams(doc,ngram_range=(2,6),token_pattern=r' ',stop_words=stop_words,vocab=train_char_vocab,char_ngrams=True))\n",
    "\n",
    "\n",
    "    \n",
    "                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorise documents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, write a function `vectoriser` to obtain Bag-of-ngram representations for a list of documents. The function should take as input:\n",
    "- `X_ngram`: a list of texts (documents), where each text is represented as list of n-grams in the `vocab`\n",
    "- `vocab`: a set of n-grams to be used for representing the documents\n",
    "\n",
    "and return:\n",
    "- `X_vec`: an array with dimensionality Nx|vocab| where N is the number of documents and |vocab| is the size of the vocabulary. Each element of the array should represent the frequency of a given n-gram in a document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:40.219201Z",
     "start_time": "2020-02-15T14:17:40.215129Z"
    }
   },
   "outputs": [],
   "source": [
    "def vectorise(X_ngram, vocab ):\n",
    "    final_vector=[]\n",
    "    print('number of documents',len(X_ngram))\n",
    "    \n",
    "    #iterate through the list of ngrams_list\n",
    "    for doc_ngram in X_ngram:\n",
    "        #use counter to get count of vocab word if present in document\n",
    "        count = Counter(doc_ngram)\n",
    "        temp_vector=[]\n",
    "        for ngram in vocab:\n",
    "            temp_vector.extend([count[ngram]])\n",
    "        final_vector.append(temp_vector)\n",
    "    X_vec = np.array(final_vector)\n",
    "    return X_vec\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use `vectorise` to obtain document vectors for each document in the train, development and test set. You should extract both count and tf.idf vectors respectively:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:41.999574Z",
     "start_time": "2020-02-15T14:17:40.376534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents 1400\n",
      "number of documents 1400\n",
      "number of documents 400\n",
      "number of documents 400\n",
      "number of documents 200\n",
      "number of documents 200\n"
     ]
    }
   ],
   "source": [
    "#create \n",
    "train_count_vector = vectorise(train_ngrams_list,train_vocab)\n",
    "train_char_count_vector = vectorise(train_char_list,train_char_vocab)\n",
    "test_count_vector = vectorise(test_ngrams_list,train_vocab)\n",
    "test_char_count_vector = vectorise(test_char_list,train_char_vocab)\n",
    "dev_count_vector = vectorise(dev_ngrams_list,train_vocab)\n",
    "dev_char_count_vector = vectorise(dev_char_list,train_char_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train data vector - (1400, 10000)  shape of test data vector - (400, 10000) \n",
      "shape of validation data vector - (200, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of train data vector -\",train_count_vector.shape,\" shape of test data vector -\",test_count_vector.shape,\"\\nshape of validation data vector -\",dev_count_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF.IDF vectors\n",
    "\n",
    "First compute `idfs` an array containing inverted document frequencies (Note: its elements should correspond to your `vocab`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:42.022692Z",
     "start_time": "2020-02-15T14:17:42.012315Z"
    }
   },
   "outputs": [],
   "source": [
    "#compute the IDF of training,test and validation set of BOW\n",
    "\n",
    "temp_arr=[]\n",
    "temp_arr2=[]\n",
    "temp_arr3=[]\n",
    "for v in train_vocab:\n",
    "    temp_arr.append(np.log10(len(x_raw_train)/(train_df[v]+1)))\n",
    "    temp_arr2.append(np.log10(len(x_raw_test)/(test_df[v]+1)))\n",
    "    temp_arr3.append(np.log10(len(x_raw_dev)/(dev_df[v]+1)))\n",
    "\n",
    "\n",
    "train_idf=np.array(temp_arr)\n",
    "test_idf = np.array(temp_arr2)\n",
    "dev_idf = np.array(temp_arr3)\n",
    "\n",
    "#compute the IDF of training,test and validation set of BOCN\n",
    "temp_arr=[]\n",
    "temp_arr2=[]\n",
    "temp_arr3=[]\n",
    "for v in train_char_vocab:\n",
    "    temp_arr.append(np.log10(len(x_raw_train)/(train_char_df[v]+1)))\n",
    "    temp_arr2.append(np.log10(len(x_raw_test)/(test_char_df[v]+1)))\n",
    "    temp_arr3.append(np.log10(len(x_raw_dev)/(dev_char_df[v]+1)))\n",
    "\n",
    "train_char_idf=np.array(temp_arr)\n",
    "test_char_idf = np.array(temp_arr2)\n",
    "dev_char_idf = np.array(temp_arr3)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i have decided to do use smoothing to avoid division by zero in cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then transform your count vectors to tf.idf vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_tf_idf = np.log10(1+train_count_vector) * train_idf\n",
    "test_tf_idf = np.log10(1+test_count_vector) * test_idf\n",
    "dev_tf_idf = np.log10(1+dev_count_vector) * dev_idf\n",
    "\n",
    "train_char_tf_idf = np.log10(1+train_char_count_vector) * train_char_idf\n",
    "test_char_tf_idf = np.log10(1+test_char_count_vector) * test_char_idf\n",
    "dev_char_tf_idf = np.log10(1+dev_char_count_vector) * dev_char_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 10000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_char_tf_idf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Logistic Regression\n",
    "\n",
    "After obtaining vector representations of the data, now you are ready to implement Binary Logistic Regression for classifying sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to implement the `sigmoid` function. It takes as input:\n",
    "\n",
    "- `z`: a real number or an array of real numbers \n",
    "\n",
    "and returns:\n",
    "\n",
    "- `sig`: the sigmoid of `z`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:44.160661Z",
     "start_time": "2020-02-15T14:17:44.157902Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def sigmoid(z):\n",
    "    sig = 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    return sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, implement the `predict_proba` function to obtain prediction probabilities. It takes as input:\n",
    "\n",
    "- `X`: an array of inputs, i.e. documents represented by bag-of-ngram vectors ($N \\times |vocab|$)\n",
    "- `weights`: a 1-D array of the model's weights $(1, |vocab|)$\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `preds_proba`: the prediction probabilities of X given the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:44.718566Z",
     "start_time": "2020-02-15T14:17:44.715017Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_proba(X, weights):\n",
    "    \n",
    "    z = np.dot(X,weights)\n",
    "    \n",
    "    preds_proba=sigmoid(z)\n",
    "    \n",
    "    return preds_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, implement the `predict_class` function to obtain the most probable class for each vector in an array of input vectors. It takes as input:\n",
    "\n",
    "- `X`: an array of documents represented by bag-of-ngram vectors ($N \\times |vocab|$)\n",
    "- `weights`: a 1-D array of the model's weights $(1, |vocab|)$\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `preds_class`: the predicted class for each x in X given the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:45.002125Z",
     "start_time": "2020-02-15T14:17:44.998668Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_class(X, weights):\n",
    "    \n",
    "    preds_class = predict_proba(X,weights)\n",
    "    \n",
    "    return [0 if prob<0.5 else 1 for prob in preds_class]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn the weights from data, we need to minimise the binary cross-entropy loss. Implement `binary_loss` that takes as input:\n",
    "\n",
    "- `X`: input vectors\n",
    "- `Y`: labels\n",
    "- `weights`: model weights\n",
    "- `alpha`: regularisation strength\n",
    "\n",
    "and return:\n",
    "\n",
    "- `l`: the loss score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:45.455533Z",
     "start_time": "2020-02-15T14:17:45.451475Z"
    }
   },
   "outputs": [],
   "source": [
    "epsilon = 1e-5   \n",
    "def binary_loss(X, Y, weights, alpha=0.00001):\n",
    "    '''\n",
    "    Binary Cross-entropy Loss\n",
    "\n",
    "    X:(len(X),len(vocab))\n",
    "    Y: array len(Y)\n",
    "    weights: array len(X)\n",
    "    '''\n",
    "    predicted_prob = predict_proba(X, weights)\n",
    "\n",
    "    l = -Y * np.log(predicted_prob) - (1 - Y) * np.log(1 - predicted_prob + epsilon  )\n",
    "    \n",
    "    l += alpha * weights.dot(weights)\n",
    "    \n",
    "    return np.mean(l)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can implement Stochastic Gradient Descent to learn the weights of your sentiment classifier. The `SGD` function takes as input:\n",
    "\n",
    "- `X_tr`: array of training data (vectors)\n",
    "- `Y_tr`: labels of `X_tr`\n",
    "- `X_dev`: array of development (i.e. validation) data (vectors)\n",
    "- `Y_dev`: labels of `X_dev`\n",
    "- `lr`: learning rate\n",
    "- `alpha`: regularisation strength\n",
    "- `epochs`: number of full passes over the training data\n",
    "- `tolerance`: stop training if the difference between the current and previous validation loss is smaller than a threshold\n",
    "- `print_progress`: flag for printing the training progress (train/validation loss)\n",
    "\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `weights`: the weights learned\n",
    "- `training_loss_history`: an array with the average losses of the whole training set after each epoch\n",
    "- `validation_loss_history`: an array with the average losses of the whole development set after each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Stochastic Gradient Descent (SGD) algorithm to estimate the parameters of your Logistic Regression models. Your SGD algorithm should:\n",
    "    - Minimise the Binary Cross-entropy loss function (**1 mark**)\n",
    "    - Use L2 regularisation (**1 mark**)\n",
    "    - Perform multiple passes (epochs) over the training data (**1 mark**)\n",
    "    - Randomise the order of training data after each pass (**1 mark**)\n",
    "    - Stop training if the difference between the current and previous development loss is smaller than a threshold (**1 mark**)\n",
    "    - After each epoch print the training and development loss (**1 mark**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to compute gradient with respect to weights\n",
    "def gradient_dw(x,y,w,alpha,N):\n",
    "  \n",
    "    dw = x * (y - (predict_proba(x,w)) - (alpha / N) * w)\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:45.968510Z",
     "start_time": "2020-02-15T14:17:45.958185Z"
    }
   },
   "outputs": [],
   "source": [
    "def SGD(X_tr, Y_tr, X_dev=[], Y_dev=[], lr=0.1, \n",
    "        alpha=0.00001, epochs=5, \n",
    "        tolerance=0.0001, print_progress=True):\n",
    "    np.random.seed(123)\n",
    "    \n",
    "    n=X_tr.shape[0] #Samples\n",
    "    m=X_tr.shape[1] #Features\n",
    "    training_loss_history = []\n",
    "    validation_loss_history = []\n",
    "    \n",
    "\n",
    "    #initialize weights randomly from standard normal distribution \n",
    "    #weights = np.random.randn(X_tr.shape[1])\n",
    "    weights = np.zeros(X_tr.shape[1])\n",
    "    #print(weights)\n",
    "    train_docs = list(zip(X_tr, Y_tr))\n",
    "    \n",
    "    #print(train_docs[0][1])\n",
    "    for current_iteration in np.arange(epochs):\n",
    "\n",
    "        temp=[]\n",
    "        np.random.shuffle(train_docs)\n",
    "        \n",
    " \n",
    "        for x,y in train_docs:\n",
    "            #calculate gradient for x and y\n",
    "            dw=gradient_dw(x,y,weights,alpha,n)\n",
    "            weights = weights + (lr * dw)\n",
    "            \n",
    "            \n",
    "        cur_loss_tr = binary_loss(X_tr, Y_tr, weights, alpha)\n",
    "        cur_loss_dev = binary_loss(X_dev, Y_dev, weights, alpha)\n",
    "        \n",
    "        #stop SGD if the difference in validation loss is lower than tolerance set\n",
    "        if current_iteration > 0 and validation_loss_history[-1] - cur_loss_dev < tolerance:\n",
    "            break\n",
    "        else:\n",
    "            training_loss_history.append(cur_loss_tr)\n",
    "            validation_loss_history.append(cur_loss_dev)\n",
    "        \n",
    "        if print_progress:\n",
    "            print(f'Epoch: {current_iteration:{10}} | Training loss: {cur_loss_tr:{15}} | Validation loss: {cur_loss_dev:{15}}')\n",
    "        \n",
    "\n",
    "    return weights, training_loss_history, validation_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Logistic Regression with Count vectors\n",
    "\n",
    "First train the model using SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate hyperparameters using logspace\n",
    "lr = np.linspace(0.00001, 0.01, num = 5,endpoint=False)\n",
    "alpha = np.logspace(-5, -1, num = 5,endpoint=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.000e-05, 2.008e-03, 4.006e-03, 6.004e-03, 8.002e-03])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e-05, 6.30957344e-05, 3.98107171e-04, 2.51188643e-03,\n",
       "       1.58489319e-02])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper Parameters -- Learning Rate - 1e-05  Regularization Parameter- 1e-05\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1e-05  Regularization Parameter- 6.309573444801929e-05\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1e-05  Regularization Parameter- 0.00039810717055349735\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1e-05  Regularization Parameter- 0.002511886431509582\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1e-05  Regularization Parameter- 0.01584893192461114\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.0020080000000000002  Regularization Parameter- 1e-05\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.0020080000000000002  Regularization Parameter- 6.309573444801929e-05\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.0020080000000000002  Regularization Parameter- 0.00039810717055349735\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.0020080000000000002  Regularization Parameter- 0.002511886431509582\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.0020080000000000002  Regularization Parameter- 0.01584893192461114\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.004006  Regularization Parameter- 1e-05\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.004006  Regularization Parameter- 6.309573444801929e-05\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.004006  Regularization Parameter- 0.00039810717055349735\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.004006  Regularization Parameter- 0.002511886431509582\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.004006  Regularization Parameter- 0.01584893192461114\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.006004000000000001  Regularization Parameter- 1e-05\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.006004000000000001  Regularization Parameter- 6.309573444801929e-05\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.006004000000000001  Regularization Parameter- 0.00039810717055349735\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.006004000000000001  Regularization Parameter- 0.002511886431509582\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.006004000000000001  Regularization Parameter- 0.01584893192461114\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.008002  Regularization Parameter- 1e-05\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.008002  Regularization Parameter- 6.309573444801929e-05\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.008002  Regularization Parameter- 0.00039810717055349735\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.008002  Regularization Parameter- 0.002511886431509582\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.008002  Regularization Parameter- 0.01584893192461114\n"
     ]
    }
   ],
   "source": [
    "#hyper paramater tuning for BOW-Count\n",
    "train_loss_history =[]\n",
    "validation_loss_history=[]\n",
    "hyperparam_history=[]\n",
    "#\n",
    "for lr_i in lr:\n",
    "    for alpha_i in alpha:\n",
    "        print('\\nHyper Parameters -- Learning Rate -',lr_i,' Regularization Parameter-',alpha_i)\n",
    "        weight, train_loss_count, dev_loss_count = SGD(X_tr=train_count_vector,\n",
    "                                             Y_tr=train_labels,\n",
    "                                             X_dev=dev_count_vector,\n",
    "                                             Y_dev=dev_labels,\n",
    "                                             lr=lr_i,\n",
    "                                             alpha=alpha_i,\n",
    "                                             tolerance=0.0001,\n",
    "                                             epochs=400,\n",
    "                                             print_progress=False)\n",
    "        if len(train_loss_count) > 40:\n",
    "            train_loss_history.append(train_loss_count[-1])\n",
    "            validation_loss_history.append(dev_loss_count[-1])\n",
    "            hyperparam_history.append([lr_i,alpha_i])\n",
    "     \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Training loss - 0.23281265732113487,best_validation_loss-0.41329350790053354,\n",
      "best_hyperparams-[1e-05, 1e-05]\n"
     ]
    }
   ],
   "source": [
    "#get the best hyper parameters\n",
    "temp = np.array(train_loss_history)\n",
    "train_loss_history_sortidx = np.argsort(temp)\n",
    "\n",
    "best_train_loss = temp[train_loss_history_sortidx[0]]\n",
    "best_validation_loss = validation_loss_history[train_loss_history_sortidx[0]]\n",
    "best_hyperparams=hyperparam_history[train_loss_history_sortidx[0]]\n",
    "\n",
    "print(f'Best Training loss - {best_train_loss},best_validation_loss-{best_validation_loss},\\nbest_hyperparams-{best_hyperparams}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:          0 | Training loss: 0.6837664802935786 | Validation loss: 0.6860666266246024\n",
      "Epoch:          1 | Training loss: 0.6751537941301965 | Validation loss: 0.6800242919507538\n",
      "Epoch:          2 | Training loss: 0.6670410843190909 | Validation loss: 0.6741665937070213\n",
      "Epoch:          3 | Training loss: 0.6593693831021286 | Validation loss: 0.6688284403248014\n",
      "Epoch:          4 | Training loss: 0.6520789507389164 | Validation loss: 0.6642044718900759\n",
      "Epoch:          5 | Training loss: 0.6451418867290613 | Validation loss: 0.6592676317437978\n",
      "Epoch:          6 | Training loss: 0.6385578999155843 | Validation loss: 0.6546807321249714\n",
      "Epoch:          7 | Training loss: 0.6322596420878759 | Validation loss: 0.6507759296929874\n",
      "Epoch:          8 | Training loss: 0.6262314087264625 | Validation loss: 0.646674831896937\n",
      "Epoch:          9 | Training loss: 0.620460854628425 | Validation loss: 0.64272630178654\n",
      "Epoch:         10 | Training loss: 0.6149881618632875 | Validation loss: 0.6394738548143387\n",
      "Epoch:         11 | Training loss: 0.6096263303307669 | Validation loss: 0.6356584498778969\n",
      "Epoch:         12 | Training loss: 0.6045043018078649 | Validation loss: 0.6322572826186653\n",
      "Epoch:         13 | Training loss: 0.5996144292432117 | Validation loss: 0.6293492459800534\n",
      "Epoch:         14 | Training loss: 0.5948162723563388 | Validation loss: 0.6258787119313933\n",
      "Epoch:         15 | Training loss: 0.5902174540550508 | Validation loss: 0.6229724126694056\n",
      "Epoch:         16 | Training loss: 0.5858409993538789 | Validation loss: 0.6205727203610901\n",
      "Epoch:         17 | Training loss: 0.5814696132641515 | Validation loss: 0.6175430515652522\n",
      "Epoch:         18 | Training loss: 0.5772660634103965 | Validation loss: 0.6146913686402145\n",
      "Epoch:         19 | Training loss: 0.573203239868719 | Validation loss: 0.6120757106076501\n",
      "Epoch:         20 | Training loss: 0.5692533401304347 | Validation loss: 0.609465451686768\n",
      "Epoch:         21 | Training loss: 0.565418961747484 | Validation loss: 0.6071622629742548\n",
      "Epoch:         22 | Training loss: 0.5616801579878159 | Validation loss: 0.6046943304173754\n",
      "Epoch:         23 | Training loss: 0.5580456726258316 | Validation loss: 0.6022680736754554\n",
      "Epoch:         24 | Training loss: 0.5545520121760678 | Validation loss: 0.6006037103567777\n",
      "Epoch:         25 | Training loss: 0.5510367585348915 | Validation loss: 0.5979420185980028\n",
      "Epoch:         26 | Training loss: 0.5476865083427509 | Validation loss: 0.596128864112069\n",
      "Epoch:         27 | Training loss: 0.5443924556120864 | Validation loss: 0.5940815370083644\n",
      "Epoch:         28 | Training loss: 0.5411402012734088 | Validation loss: 0.5918049348792468\n",
      "Epoch:         29 | Training loss: 0.5380223918391238 | Validation loss: 0.5901352532689339\n",
      "Epoch:         30 | Training loss: 0.5349035346458563 | Validation loss: 0.5879064173990801\n",
      "Epoch:         31 | Training loss: 0.5318869420725559 | Validation loss: 0.5860325171763313\n",
      "Epoch:         32 | Training loss: 0.5289719737508198 | Validation loss: 0.5845000056060266\n",
      "Epoch:         33 | Training loss: 0.5260395933891074 | Validation loss: 0.5822160556529584\n",
      "Epoch:         34 | Training loss: 0.5232012126512148 | Validation loss: 0.5805510208618871\n",
      "Epoch:         35 | Training loss: 0.5204200399022954 | Validation loss: 0.5787348355126516\n",
      "Epoch:         36 | Training loss: 0.5176919364011832 | Validation loss: 0.5770930718379378\n",
      "Epoch:         37 | Training loss: 0.5150336240849595 | Validation loss: 0.5756605902730182\n",
      "Epoch:         38 | Training loss: 0.5123937823554481 | Validation loss: 0.5739013257967216\n",
      "Epoch:         39 | Training loss: 0.509812115063446 | Validation loss: 0.5721273367399778\n",
      "Epoch:         40 | Training loss: 0.50728973068448 | Validation loss: 0.5704208804294065\n",
      "Epoch:         41 | Training loss: 0.5048061870437035 | Validation loss: 0.5688381889547951\n",
      "Epoch:         42 | Training loss: 0.5023526571599741 | Validation loss: 0.5673625681109469\n",
      "Epoch:         43 | Training loss: 0.4999495815903 | Validation loss: 0.5658515724516266\n",
      "Epoch:         44 | Training loss: 0.49756792858969884 | Validation loss: 0.5646093091599969\n",
      "Epoch:         45 | Training loss: 0.4952405210849046 | Validation loss: 0.5631584819637063\n",
      "Epoch:         46 | Training loss: 0.4929495823818077 | Validation loss: 0.5617385648972377\n",
      "Epoch:         47 | Training loss: 0.4907052406130593 | Validation loss: 0.5605148072784047\n",
      "Epoch:         48 | Training loss: 0.48847718354110614 | Validation loss: 0.5589147833835034\n",
      "Epoch:         49 | Training loss: 0.48631595490055785 | Validation loss: 0.5573487114953168\n",
      "Epoch:         50 | Training loss: 0.48415099012918256 | Validation loss: 0.5560928143646193\n",
      "Epoch:         51 | Training loss: 0.4820267580317835 | Validation loss: 0.5548089734127464\n",
      "Epoch:         52 | Training loss: 0.4799531560891506 | Validation loss: 0.5534212394390322\n",
      "Epoch:         53 | Training loss: 0.4778715239330672 | Validation loss: 0.5523197650202973\n",
      "Epoch:         54 | Training loss: 0.4758583903519518 | Validation loss: 0.550923406670577\n",
      "Epoch:         55 | Training loss: 0.47385802271210575 | Validation loss: 0.5496875054123193\n",
      "Epoch:         56 | Training loss: 0.4718681295381277 | Validation loss: 0.548613084374457\n",
      "Epoch:         57 | Training loss: 0.46991966274925095 | Validation loss: 0.5474829695359023\n",
      "Epoch:         58 | Training loss: 0.4680049722151188 | Validation loss: 0.5464390469432487\n",
      "Epoch:         59 | Training loss: 0.4661056300765147 | Validation loss: 0.5450870805694685\n",
      "Epoch:         60 | Training loss: 0.4642353057270652 | Validation loss: 0.5439437184317902\n",
      "Epoch:         61 | Training loss: 0.46238963126834726 | Validation loss: 0.5427980778893151\n",
      "Epoch:         62 | Training loss: 0.4605677091500393 | Validation loss: 0.5417392248113186\n",
      "Epoch:         63 | Training loss: 0.45876931457710496 | Validation loss: 0.5405703181534346\n",
      "Epoch:         64 | Training loss: 0.4569957814670056 | Validation loss: 0.5394118632507483\n",
      "Epoch:         65 | Training loss: 0.45525567083657764 | Validation loss: 0.5382181738733991\n",
      "Epoch:         66 | Training loss: 0.45350693965796374 | Validation loss: 0.5373449221244526\n",
      "Epoch:         67 | Training loss: 0.4517972310911083 | Validation loss: 0.5363856244619263\n",
      "Epoch:         68 | Training loss: 0.45010537742194817 | Validation loss: 0.5352418058418439\n",
      "Epoch:         69 | Training loss: 0.44843390998528515 | Validation loss: 0.5342143588566688\n",
      "Epoch:         70 | Training loss: 0.4467864345514428 | Validation loss: 0.5333387689769278\n",
      "Epoch:         71 | Training loss: 0.4451541229710335 | Validation loss: 0.53208973855215\n",
      "Epoch:         72 | Training loss: 0.44353724671552713 | Validation loss: 0.5311363596198899\n",
      "Epoch:         73 | Training loss: 0.4419436689976153 | Validation loss: 0.5303511529307477\n",
      "Epoch:         74 | Training loss: 0.44036265764022897 | Validation loss: 0.5292788789961118\n",
      "Epoch:         75 | Training loss: 0.43880524132660653 | Validation loss: 0.5283948624752359\n",
      "Epoch:         76 | Training loss: 0.4372603678646285 | Validation loss: 0.5274045270345552\n",
      "Epoch:         77 | Training loss: 0.4357380559904594 | Validation loss: 0.5265435372786134\n",
      "Epoch:         78 | Training loss: 0.4342271454320785 | Validation loss: 0.5255983716061824\n",
      "Epoch:         79 | Training loss: 0.4327375884245382 | Validation loss: 0.5247380006049372\n",
      "Epoch:         80 | Training loss: 0.43125832339600356 | Validation loss: 0.5238182689310932\n",
      "Epoch:         81 | Training loss: 0.4297900551815661 | Validation loss: 0.5228247023817667\n",
      "Epoch:         82 | Training loss: 0.4283518810233583 | Validation loss: 0.5220613935123488\n",
      "Epoch:         83 | Training loss: 0.4269265876611975 | Validation loss: 0.5212362568522544\n",
      "Epoch:         84 | Training loss: 0.42550311120505496 | Validation loss: 0.5199889692115025\n",
      "Epoch:         85 | Training loss: 0.42410125587166264 | Validation loss: 0.5191150987011416\n",
      "Epoch:         86 | Training loss: 0.42270846707060045 | Validation loss: 0.5185971321153982\n",
      "Epoch:         87 | Training loss: 0.42132776064686667 | Validation loss: 0.5177003748719688\n",
      "Epoch:         88 | Training loss: 0.41996120999150743 | Validation loss: 0.516747525008293\n",
      "Epoch:         89 | Training loss: 0.4186275736309935 | Validation loss: 0.5157430577343197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:         90 | Training loss: 0.4172757794162526 | Validation loss: 0.5151131850544521\n",
      "Epoch:         91 | Training loss: 0.4159762832232861 | Validation loss: 0.5140810545242347\n",
      "Epoch:         92 | Training loss: 0.41464340331998195 | Validation loss: 0.5134520424238438\n",
      "Epoch:         93 | Training loss: 0.4133468123738284 | Validation loss: 0.5128286890702466\n",
      "Epoch:         94 | Training loss: 0.4120585401784717 | Validation loss: 0.5118623067546063\n",
      "Epoch:         95 | Training loss: 0.4107899160380053 | Validation loss: 0.5110190277209037\n",
      "Epoch:         96 | Training loss: 0.4095229318181874 | Validation loss: 0.5105047904426872\n",
      "Epoch:         97 | Training loss: 0.40826756001931724 | Validation loss: 0.5095879179310288\n",
      "Epoch:         98 | Training loss: 0.4070266603437674 | Validation loss: 0.5088495815583657\n",
      "Epoch:         99 | Training loss: 0.40580129879378796 | Validation loss: 0.5082373370727296\n",
      "Epoch:        100 | Training loss: 0.4045811907223099 | Validation loss: 0.5073068644241903\n",
      "Epoch:        101 | Training loss: 0.4033708860588333 | Validation loss: 0.5066510957252813\n",
      "Epoch:        102 | Training loss: 0.4021737142323049 | Validation loss: 0.5058962069390738\n",
      "Epoch:        103 | Training loss: 0.4009868699255227 | Validation loss: 0.5052480378871065\n",
      "Epoch:        104 | Training loss: 0.3998105320332447 | Validation loss: 0.5044421897435346\n",
      "Epoch:        105 | Training loss: 0.39864337152841883 | Validation loss: 0.5038227036901928\n",
      "Epoch:        106 | Training loss: 0.39749217950254906 | Validation loss: 0.5032219159790792\n",
      "Epoch:        107 | Training loss: 0.39633839108272556 | Validation loss: 0.502394469121143\n",
      "Epoch:        108 | Training loss: 0.3952003948948108 | Validation loss: 0.5017042072427088\n",
      "Epoch:        109 | Training loss: 0.39408021017633094 | Validation loss: 0.50088691692697\n",
      "Epoch:        110 | Training loss: 0.3929551844944264 | Validation loss: 0.5002717780832359\n",
      "Epoch:        111 | Training loss: 0.3918424925389611 | Validation loss: 0.4996949738951304\n",
      "Epoch:        112 | Training loss: 0.39074235881939934 | Validation loss: 0.4989667625670596\n",
      "Epoch:        113 | Training loss: 0.3896507254594083 | Validation loss: 0.4982936372009222\n",
      "Epoch:        114 | Training loss: 0.38856577839811474 | Validation loss: 0.4977236576057825\n",
      "Epoch:        115 | Training loss: 0.3874912587487948 | Validation loss: 0.497092210414007\n",
      "Epoch:        116 | Training loss: 0.3864245058441865 | Validation loss: 0.49639514844708915\n",
      "Epoch:        117 | Training loss: 0.38536621832108536 | Validation loss: 0.49579208574398304\n",
      "Epoch:        118 | Training loss: 0.3843175961350903 | Validation loss: 0.49509806489595914\n",
      "Epoch:        119 | Training loss: 0.3832761101869947 | Validation loss: 0.4945980545743065\n",
      "Epoch:        120 | Training loss: 0.3822431351101441 | Validation loss: 0.49399528013576244\n",
      "Epoch:        121 | Training loss: 0.3812198618511562 | Validation loss: 0.49341686473156166\n",
      "Epoch:        122 | Training loss: 0.38019700909799203 | Validation loss: 0.49265280746861395\n",
      "Epoch:        123 | Training loss: 0.37918637405184313 | Validation loss: 0.4921199129806377\n",
      "Epoch:        124 | Training loss: 0.3781830213035372 | Validation loss: 0.4914571494940989\n",
      "Epoch:        125 | Training loss: 0.37718782077397384 | Validation loss: 0.4909382254841343\n",
      "Epoch:        126 | Training loss: 0.3762010919228765 | Validation loss: 0.49038305322243275\n",
      "Epoch:        127 | Training loss: 0.37521745495581926 | Validation loss: 0.48971506875948606\n",
      "Epoch:        128 | Training loss: 0.37424387402819387 | Validation loss: 0.48910710029759175\n",
      "Epoch:        129 | Training loss: 0.3732769285856317 | Validation loss: 0.4885392655074986\n",
      "Epoch:        130 | Training loss: 0.3723166583632724 | Validation loss: 0.4879730632345186\n",
      "Epoch:        131 | Training loss: 0.3713641008337482 | Validation loss: 0.487378681738184\n",
      "Epoch:        132 | Training loss: 0.3704165479403739 | Validation loss: 0.48685780270798495\n",
      "Epoch:        133 | Training loss: 0.3694787611742333 | Validation loss: 0.48624909090034096\n",
      "Epoch:        134 | Training loss: 0.36854427604964013 | Validation loss: 0.48571717053841934\n",
      "Epoch:        135 | Training loss: 0.367617447551719 | Validation loss: 0.4851747551705035\n",
      "Epoch:        136 | Training loss: 0.3666988185612402 | Validation loss: 0.4845954720943099\n",
      "Epoch:        137 | Training loss: 0.3657823020599932 | Validation loss: 0.4841340518317611\n",
      "Epoch:        138 | Training loss: 0.36487720696773046 | Validation loss: 0.48351948615726514\n",
      "Epoch:        139 | Training loss: 0.3639770230485703 | Validation loss: 0.48316868363794596\n",
      "Epoch:        140 | Training loss: 0.3630771836196717 | Validation loss: 0.48255331134807916\n",
      "Epoch:        141 | Training loss: 0.3621879412017794 | Validation loss: 0.4819849300908874\n",
      "Epoch:        142 | Training loss: 0.3613131043358653 | Validation loss: 0.48135875654667154\n",
      "Epoch:        143 | Training loss: 0.36043164837788605 | Validation loss: 0.4808750458407627\n",
      "Epoch:        144 | Training loss: 0.3595603669371584 | Validation loss: 0.4803566282554021\n",
      "Epoch:        145 | Training loss: 0.35868919508036623 | Validation loss: 0.4799138336732156\n",
      "Epoch:        146 | Training loss: 0.35782773389332767 | Validation loss: 0.47948397072265253\n",
      "Epoch:        147 | Training loss: 0.35697236820113565 | Validation loss: 0.4789521401224263\n",
      "Epoch:        148 | Training loss: 0.3561232624581765 | Validation loss: 0.47848903725237946\n",
      "Epoch:        149 | Training loss: 0.3552789495491082 | Validation loss: 0.4779530252895735\n",
      "Epoch:        150 | Training loss: 0.3544446139721459 | Validation loss: 0.47757635267120546\n",
      "Epoch:        151 | Training loss: 0.35362023402704046 | Validation loss: 0.47717529632732564\n",
      "Epoch:        152 | Training loss: 0.3527792258499768 | Validation loss: 0.4765087429996908\n",
      "Epoch:        153 | Training loss: 0.3519567366894854 | Validation loss: 0.47605160488496323\n",
      "Epoch:        154 | Training loss: 0.35113978870596857 | Validation loss: 0.4756038160862036\n",
      "Epoch:        155 | Training loss: 0.35032605388671406 | Validation loss: 0.47508210274007345\n",
      "Epoch:        156 | Training loss: 0.34951843544699124 | Validation loss: 0.4746071712777294\n",
      "Epoch:        157 | Training loss: 0.34871603472216606 | Validation loss: 0.47413578067771944\n",
      "Epoch:        158 | Training loss: 0.34791942036625095 | Validation loss: 0.47364102772769856\n",
      "Epoch:        159 | Training loss: 0.3471295332823663 | Validation loss: 0.4731380770914423\n",
      "Epoch:        160 | Training loss: 0.3463381998065525 | Validation loss: 0.47273382440140005\n",
      "Epoch:        161 | Training loss: 0.345554218566849 | Validation loss: 0.47231736813332575\n",
      "Epoch:        162 | Training loss: 0.34478024841307064 | Validation loss: 0.4719752848419785\n",
      "Epoch:        163 | Training loss: 0.3440021560793176 | Validation loss: 0.47144730377500155\n",
      "Epoch:        164 | Training loss: 0.34323692981262605 | Validation loss: 0.4708934783438385\n",
      "Epoch:        165 | Training loss: 0.342468076305001 | Validation loss: 0.47052241099472575\n",
      "Epoch:        166 | Training loss: 0.341707655134254 | Validation loss: 0.4701120375278542\n",
      "Epoch:        167 | Training loss: 0.3409592786909664 | Validation loss: 0.4695549272699273\n",
      "Epoch:        168 | Training loss: 0.3402119277923956 | Validation loss: 0.469097496921235\n",
      "Epoch:        169 | Training loss: 0.3394532304418111 | Validation loss: 0.4688064676161413\n",
      "Epoch:        170 | Training loss: 0.3387193153988854 | Validation loss: 0.46826182222140617\n",
      "Epoch:        171 | Training loss: 0.337972573017819 | Validation loss: 0.46793741316463544\n",
      "Epoch:        172 | Training loss: 0.3372380530027844 | Validation loss: 0.4675228925721659\n",
      "Epoch:        173 | Training loss: 0.3365088972153167 | Validation loss: 0.46718129154621324\n",
      "Epoch:        174 | Training loss: 0.3357879888220526 | Validation loss: 0.46683534931068876\n",
      "Epoch:        175 | Training loss: 0.33506411731170416 | Validation loss: 0.4664010990597667\n",
      "Epoch:        176 | Training loss: 0.33434973525181466 | Validation loss: 0.46602830178329085\n",
      "Epoch:        177 | Training loss: 0.33362816376952426 | Validation loss: 0.465508050069186\n",
      "Epoch:        178 | Training loss: 0.3329206905966826 | Validation loss: 0.4651603280407606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:        179 | Training loss: 0.33221217814564225 | Validation loss: 0.4646831356370877\n",
      "Epoch:        180 | Training loss: 0.33151099935248435 | Validation loss: 0.4643300041296846\n",
      "Epoch:        181 | Training loss: 0.33081210232163055 | Validation loss: 0.4638747885329552\n",
      "Epoch:        182 | Training loss: 0.33011805846000997 | Validation loss: 0.4635279741292131\n",
      "Epoch:        183 | Training loss: 0.3294272036932456 | Validation loss: 0.46309147136694834\n",
      "Epoch:        184 | Training loss: 0.32874422459478614 | Validation loss: 0.4628154395085077\n",
      "Epoch:        185 | Training loss: 0.32805709022503043 | Validation loss: 0.4623556638684647\n",
      "Epoch:        186 | Training loss: 0.32737747565202324 | Validation loss: 0.461940278786045\n",
      "Epoch:        187 | Training loss: 0.32670557530677463 | Validation loss: 0.4614888647627559\n",
      "Epoch:        188 | Training loss: 0.3260353060054206 | Validation loss: 0.4610928508119131\n",
      "Epoch:        189 | Training loss: 0.3253700076164875 | Validation loss: 0.46069379519061937\n",
      "Epoch:        190 | Training loss: 0.32469939811023496 | Validation loss: 0.4603619337468763\n",
      "Epoch:        191 | Training loss: 0.3240404177149908 | Validation loss: 0.46019173218447484\n",
      "Epoch:        192 | Training loss: 0.323375462716851 | Validation loss: 0.45971808113545903\n",
      "Epoch:        193 | Training loss: 0.32272050285827336 | Validation loss: 0.4593517241599492\n",
      "Epoch:        194 | Training loss: 0.32206953512295705 | Validation loss: 0.45893612843053744\n",
      "Epoch:        195 | Training loss: 0.32142179912782315 | Validation loss: 0.45856682603386134\n",
      "Epoch:        196 | Training loss: 0.3207761135070402 | Validation loss: 0.4582643333647355\n",
      "Epoch:        197 | Training loss: 0.32013539395873586 | Validation loss: 0.45793776255217006\n",
      "Epoch:        198 | Training loss: 0.3194963562881158 | Validation loss: 0.45752488975725647\n",
      "Epoch:        199 | Training loss: 0.31886139082229314 | Validation loss: 0.4571746746889178\n",
      "Epoch:        200 | Training loss: 0.31823343920021396 | Validation loss: 0.45675616731951757\n",
      "Epoch:        201 | Training loss: 0.3176026336108145 | Validation loss: 0.4564356528359507\n",
      "Epoch:        202 | Training loss: 0.31697586594662347 | Validation loss: 0.4561232978672811\n",
      "Epoch:        203 | Training loss: 0.31635354048571507 | Validation loss: 0.45577870249066565\n",
      "Epoch:        204 | Training loss: 0.31574323525768355 | Validation loss: 0.45558912172238913\n",
      "Epoch:        205 | Training loss: 0.31512046253664283 | Validation loss: 0.45517867799584294\n",
      "Epoch:        206 | Training loss: 0.314507032207403 | Validation loss: 0.45471252109198884\n",
      "Epoch:        207 | Training loss: 0.3138960911284605 | Validation loss: 0.4543896538358052\n",
      "Epoch:        208 | Training loss: 0.31328873673344787 | Validation loss: 0.45406241714483725\n",
      "Epoch:        209 | Training loss: 0.31268678845709247 | Validation loss: 0.4536968093436164\n",
      "Epoch:        210 | Training loss: 0.3120930840308591 | Validation loss: 0.4533111586850921\n",
      "Epoch:        211 | Training loss: 0.31148689020278425 | Validation loss: 0.45304208893432785\n",
      "Epoch:        212 | Training loss: 0.310889034907078 | Validation loss: 0.45276093211752455\n",
      "Epoch:        213 | Training loss: 0.31029713533413217 | Validation loss: 0.45241670563134484\n",
      "Epoch:        214 | Training loss: 0.30970746344897643 | Validation loss: 0.4521761444555202\n",
      "Epoch:        215 | Training loss: 0.30912236127767795 | Validation loss: 0.4518853734867697\n",
      "Epoch:        216 | Training loss: 0.3085359673552447 | Validation loss: 0.45146156719937863\n",
      "Epoch:        217 | Training loss: 0.30796291909199697 | Validation loss: 0.4510619013801721\n",
      "Epoch:        218 | Training loss: 0.3073777333634862 | Validation loss: 0.45079927754932464\n",
      "Epoch:        219 | Training loss: 0.30680210556578225 | Validation loss: 0.45048288247201784\n",
      "Epoch:        220 | Training loss: 0.30623420470349505 | Validation loss: 0.4501263782347033\n",
      "Epoch:        221 | Training loss: 0.30565780120263486 | Validation loss: 0.44986489950137226\n",
      "Epoch:        222 | Training loss: 0.30508776590640835 | Validation loss: 0.44961406174787727\n",
      "Epoch:        223 | Training loss: 0.30452496279694996 | Validation loss: 0.44924677611805897\n",
      "Epoch:        224 | Training loss: 0.303960054421738 | Validation loss: 0.4489867042579285\n",
      "Epoch:        225 | Training loss: 0.3034016628748526 | Validation loss: 0.4487573663649006\n",
      "Epoch:        226 | Training loss: 0.3028440420485588 | Validation loss: 0.44845398432703737\n",
      "Epoch:        227 | Training loss: 0.30228733474043223 | Validation loss: 0.4481040430180663\n",
      "Epoch:        228 | Training loss: 0.30173903734859786 | Validation loss: 0.44772468501939444\n",
      "Epoch:        229 | Training loss: 0.3011862962017453 | Validation loss: 0.44746240065830156\n",
      "Epoch:        230 | Training loss: 0.3006397332590611 | Validation loss: 0.4472700133123325\n",
      "Epoch:        231 | Training loss: 0.3000950948197428 | Validation loss: 0.44685733996666455\n",
      "Epoch:        232 | Training loss: 0.299557102061021 | Validation loss: 0.44652678348187946\n",
      "Epoch:        233 | Training loss: 0.29901425873087273 | Validation loss: 0.44625592099647915\n",
      "Epoch:        234 | Training loss: 0.2984747267924936 | Validation loss: 0.4459855184504905\n",
      "Epoch:        235 | Training loss: 0.2979410208992743 | Validation loss: 0.4456818025643885\n",
      "Epoch:        236 | Training loss: 0.29740391584600634 | Validation loss: 0.44547272974400676\n",
      "Epoch:        237 | Training loss: 0.2968746417776504 | Validation loss: 0.4451444432164614\n",
      "Epoch:        238 | Training loss: 0.2963451747906574 | Validation loss: 0.44492971595338177\n",
      "Epoch:        239 | Training loss: 0.29581890030085234 | Validation loss: 0.44463569527742375\n",
      "Epoch:        240 | Training loss: 0.2952951851198195 | Validation loss: 0.4443383998265434\n",
      "Epoch:        241 | Training loss: 0.29477429013412126 | Validation loss: 0.44403853408037564\n",
      "Epoch:        242 | Training loss: 0.29425580756804526 | Validation loss: 0.4438408341138683\n",
      "Epoch:        243 | Training loss: 0.29374150183319564 | Validation loss: 0.44360268325182184\n",
      "Epoch:        244 | Training loss: 0.2932233287798214 | Validation loss: 0.4432254818580042\n",
      "Epoch:        245 | Training loss: 0.2927128436002478 | Validation loss: 0.4429176048092433\n",
      "Epoch:        246 | Training loss: 0.29220030793922774 | Validation loss: 0.44269688855023914\n",
      "Epoch:        247 | Training loss: 0.2916927365680737 | Validation loss: 0.4424667519540168\n",
      "Epoch:        248 | Training loss: 0.2911873017848136 | Validation loss: 0.44221132067406527\n",
      "Epoch:        249 | Training loss: 0.29068448473297326 | Validation loss: 0.4419617055601474\n",
      "Epoch:        250 | Training loss: 0.29018592028510526 | Validation loss: 0.44173287718063264\n",
      "Epoch:        251 | Training loss: 0.28968231972626435 | Validation loss: 0.4414221099744773\n",
      "Epoch:        252 | Training loss: 0.289185744633953 | Validation loss: 0.4411751772137923\n",
      "Epoch:        253 | Training loss: 0.28869266507690483 | Validation loss: 0.4409417372659828\n",
      "Epoch:        254 | Training loss: 0.2881966360464299 | Validation loss: 0.4406517100173884\n",
      "Epoch:        255 | Training loss: 0.28770306525312644 | Validation loss: 0.44033999372111454\n",
      "Epoch:        256 | Training loss: 0.2872170773859687 | Validation loss: 0.4401493822679325\n",
      "Epoch:        257 | Training loss: 0.28672681362811775 | Validation loss: 0.4398435675818761\n",
      "Epoch:        258 | Training loss: 0.28624117295593204 | Validation loss: 0.4395677808767671\n",
      "Epoch:        259 | Training loss: 0.2857583528304201 | Validation loss: 0.4393409563747055\n",
      "Epoch:        260 | Training loss: 0.2852784419417516 | Validation loss: 0.43911650572563377\n",
      "Epoch:        261 | Training loss: 0.2847996582006983 | Validation loss: 0.4388753185982695\n",
      "Epoch:        262 | Training loss: 0.2843209907791069 | Validation loss: 0.43860769816819295\n",
      "Epoch:        263 | Training loss: 0.28384475905915485 | Validation loss: 0.4383444806919913\n",
      "Epoch:        264 | Training loss: 0.2833766150637667 | Validation loss: 0.4381691724445799\n",
      "Epoch:        265 | Training loss: 0.28290557747517137 | Validation loss: 0.43792990382843755\n",
      "Epoch:        266 | Training loss: 0.28242913868999325 | Validation loss: 0.4375736568142176\n",
      "Epoch:        267 | Training loss: 0.28196117177409274 | Validation loss: 0.4373445390106905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:        268 | Training loss: 0.28149609523660174 | Validation loss: 0.43705884675505446\n",
      "Epoch:        269 | Training loss: 0.2810311151075112 | Validation loss: 0.4368383014770363\n",
      "Epoch:        270 | Training loss: 0.28056912719671784 | Validation loss: 0.43658948063331693\n",
      "Epoch:        271 | Training loss: 0.28011015575631687 | Validation loss: 0.4364321913654545\n",
      "Epoch:        272 | Training loss: 0.27964962055397224 | Validation loss: 0.4361481172851967\n",
      "Epoch:        273 | Training loss: 0.2791927882589598 | Validation loss: 0.4359207019412642\n",
      "Epoch:        274 | Training loss: 0.27873755087120894 | Validation loss: 0.43567979429809156\n",
      "Epoch:        275 | Training loss: 0.27828454033736877 | Validation loss: 0.43541548329062274\n",
      "Epoch:        276 | Training loss: 0.27783341316269955 | Validation loss: 0.4352428184030165\n",
      "Epoch:        277 | Training loss: 0.2773835561583845 | Validation loss: 0.4349407220671337\n",
      "Epoch:        278 | Training loss: 0.2769346053615126 | Validation loss: 0.43473758879036245\n",
      "Epoch:        279 | Training loss: 0.2764882579823728 | Validation loss: 0.434525474678895\n",
      "Epoch:        280 | Training loss: 0.2760447717837533 | Validation loss: 0.43433011927051796\n",
      "Epoch:        281 | Training loss: 0.27560091441758056 | Validation loss: 0.43404588671249145\n",
      "Epoch:        282 | Training loss: 0.27515956117681817 | Validation loss: 0.43385211297805015\n",
      "Epoch:        283 | Training loss: 0.27472013718102506 | Validation loss: 0.4335934538575373\n",
      "Epoch:        284 | Training loss: 0.27428571088847253 | Validation loss: 0.43332310586854517\n",
      "Epoch:        285 | Training loss: 0.27384758580060165 | Validation loss: 0.43311902519112144\n",
      "Epoch:        286 | Training loss: 0.2734133657476954 | Validation loss: 0.4328938423769166\n",
      "Epoch:        287 | Training loss: 0.2729783734089405 | Validation loss: 0.4327467318705543\n",
      "Epoch:        288 | Training loss: 0.27254678730167703 | Validation loss: 0.43250474468872274\n",
      "Epoch:        289 | Training loss: 0.27211717837124283 | Validation loss: 0.4322791053832443\n",
      "Epoch:        290 | Training loss: 0.27168931714155814 | Validation loss: 0.4320507683390275\n",
      "Epoch:        291 | Training loss: 0.27126310081081745 | Validation loss: 0.43182870979253024\n",
      "Epoch:        292 | Training loss: 0.2708376148901278 | Validation loss: 0.43164420590729613\n",
      "Epoch:        293 | Training loss: 0.270414820995472 | Validation loss: 0.43139898652306335\n",
      "Epoch:        294 | Training loss: 0.26999301535104 | Validation loss: 0.4311871003815604\n",
      "Epoch:        295 | Training loss: 0.26957243729672276 | Validation loss: 0.4309854660766739\n",
      "Epoch:        296 | Training loss: 0.26915434766055624 | Validation loss: 0.4307594961520286\n",
      "Epoch:        297 | Training loss: 0.2687364786213496 | Validation loss: 0.4305732168945346\n",
      "Epoch:        298 | Training loss: 0.26832086439550756 | Validation loss: 0.4303645760664692\n",
      "Epoch:        299 | Training loss: 0.2679070238155448 | Validation loss: 0.43014383756606234\n",
      "Epoch:        300 | Training loss: 0.2674945662571926 | Validation loss: 0.4299327342293184\n",
      "Epoch:        301 | Training loss: 0.2670830823098649 | Validation loss: 0.4297583656108707\n",
      "Epoch:        302 | Training loss: 0.26667360370222315 | Validation loss: 0.42956031663696337\n",
      "Epoch:        303 | Training loss: 0.2662657075020615 | Validation loss: 0.4293175648390725\n",
      "Epoch:        304 | Training loss: 0.2658592861308371 | Validation loss: 0.42910705939202176\n",
      "Epoch:        305 | Training loss: 0.26545521935813887 | Validation loss: 0.4288876855302513\n",
      "Epoch:        306 | Training loss: 0.265049791409723 | Validation loss: 0.42872025306955885\n",
      "Epoch:        307 | Training loss: 0.2646483464458197 | Validation loss: 0.42849657929195567\n",
      "Epoch:        308 | Training loss: 0.26424660678663897 | Validation loss: 0.42831987824214335\n",
      "Epoch:        309 | Training loss: 0.26384720165744213 | Validation loss: 0.4281187779385575\n",
      "Epoch:        310 | Training loss: 0.26344909205352957 | Validation loss: 0.42793406315306326\n",
      "Epoch:        311 | Training loss: 0.26305248777656515 | Validation loss: 0.4277382895928163\n",
      "Epoch:        312 | Training loss: 0.2626577524253129 | Validation loss: 0.4275172004965311\n",
      "Epoch:        313 | Training loss: 0.2622647304426624 | Validation loss: 0.4273075420841991\n",
      "Epoch:        314 | Training loss: 0.26187214857366264 | Validation loss: 0.42718944341781473\n",
      "Epoch:        315 | Training loss: 0.2614799744369903 | Validation loss: 0.42695951708391744\n",
      "Epoch:        316 | Training loss: 0.26109052360573054 | Validation loss: 0.42674827119056064\n",
      "Epoch:        317 | Training loss: 0.26070332718759526 | Validation loss: 0.42653329119606487\n",
      "Epoch:        318 | Training loss: 0.26031543096135734 | Validation loss: 0.42635712713370055\n",
      "Epoch:        319 | Training loss: 0.2599300288797742 | Validation loss: 0.42616201461778586\n",
      "Epoch:        320 | Training loss: 0.2595465119998346 | Validation loss: 0.4259622187480059\n",
      "Epoch:        321 | Training loss: 0.259162021978057 | Validation loss: 0.42580102393707375\n",
      "Epoch:        322 | Training loss: 0.25878102674300774 | Validation loss: 0.42559489621988933\n",
      "Epoch:        323 | Training loss: 0.2583998525658513 | Validation loss: 0.42545520710869916\n",
      "Epoch:        324 | Training loss: 0.25802064116605533 | Validation loss: 0.4252476206181201\n",
      "Epoch:        325 | Training loss: 0.2576436257548547 | Validation loss: 0.4250402122292205\n",
      "Epoch:        326 | Training loss: 0.2572664441475887 | Validation loss: 0.42489829012761215\n",
      "Epoch:        327 | Training loss: 0.25689109994580045 | Validation loss: 0.42470117684171216\n",
      "Epoch:        328 | Training loss: 0.25651735954087707 | Validation loss: 0.42450353655457135\n",
      "Epoch:        329 | Training loss: 0.25614445518707385 | Validation loss: 0.424337779222284\n",
      "Epoch:        330 | Training loss: 0.25577301071238473 | Validation loss: 0.4241506559701778\n",
      "Epoch:        331 | Training loss: 0.2554041897640804 | Validation loss: 0.4239382583282817\n",
      "Epoch:        332 | Training loss: 0.2550346811380546 | Validation loss: 0.4237672132367234\n",
      "Epoch:        333 | Training loss: 0.2546666015111085 | Validation loss: 0.4235959397518333\n",
      "Epoch:        334 | Training loss: 0.2543006958938394 | Validation loss: 0.4234056030676824\n",
      "Epoch:        335 | Training loss: 0.25393825019071115 | Validation loss: 0.4232001157742092\n",
      "Epoch:        336 | Training loss: 0.2535780459417035 | Validation loss: 0.4230020009022137\n",
      "Epoch:        337 | Training loss: 0.25320951104676404 | Validation loss: 0.4228656526362492\n",
      "Epoch:        338 | Training loss: 0.25284885840949534 | Validation loss: 0.4226812431103983\n",
      "Epoch:        339 | Training loss: 0.2524885006806433 | Validation loss: 0.42250629512935334\n",
      "Epoch:        340 | Training loss: 0.25213102981133173 | Validation loss: 0.4223196749492793\n",
      "Epoch:        341 | Training loss: 0.25176883295065305 | Validation loss: 0.42218553508655604\n",
      "Epoch:        342 | Training loss: 0.25141355378719593 | Validation loss: 0.42199277967677473\n",
      "Epoch:        343 | Training loss: 0.2510555795509604 | Validation loss: 0.42185516280830315\n",
      "Epoch:        344 | Training loss: 0.25070074775306656 | Validation loss: 0.42169790361153386\n",
      "Epoch:        345 | Training loss: 0.25034725115205153 | Validation loss: 0.4215330183867472\n",
      "Epoch:        346 | Training loss: 0.24999558358674123 | Validation loss: 0.4213341384153104\n",
      "Epoch:        347 | Training loss: 0.24964366585315567 | Validation loss: 0.42119529736037203\n",
      "Epoch:        348 | Training loss: 0.24929600146133687 | Validation loss: 0.42097849218655525\n",
      "Epoch:        349 | Training loss: 0.24894596519415768 | Validation loss: 0.42082361100200444\n",
      "Epoch:        350 | Training loss: 0.24859717938803474 | Validation loss: 0.4206775473984057\n",
      "Epoch:        351 | Training loss: 0.24825076200241564 | Validation loss: 0.4205056121667856\n",
      "Epoch:        352 | Training loss: 0.24790490817913785 | Validation loss: 0.42035080557504884\n",
      "Epoch:        353 | Training loss: 0.24756100803650918 | Validation loss: 0.420222437306324\n",
      "Epoch:        354 | Training loss: 0.24721777297460318 | Validation loss: 0.42006204676663034\n",
      "Epoch:        355 | Training loss: 0.24687476233718744 | Validation loss: 0.41987014485291396\n",
      "Epoch:        356 | Training loss: 0.24653380570704053 | Validation loss: 0.4197233666008504\n",
      "Epoch:        357 | Training loss: 0.24619357739540773 | Validation loss: 0.41953147346104364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:        358 | Training loss: 0.2458558698624229 | Validation loss: 0.4193471182911409\n",
      "Epoch:        359 | Training loss: 0.2455165804589985 | Validation loss: 0.4192126506882012\n",
      "Epoch:        360 | Training loss: 0.24517955844786798 | Validation loss: 0.41906090865577406\n",
      "Epoch:        361 | Training loss: 0.24484481539464428 | Validation loss: 0.4188719641108787\n",
      "Epoch:        362 | Training loss: 0.2445101590099968 | Validation loss: 0.41871188342840426\n",
      "Epoch:        363 | Training loss: 0.24417653031448425 | Validation loss: 0.4185537598580524\n",
      "Epoch:        364 | Training loss: 0.24384283170198784 | Validation loss: 0.41841451014062003\n",
      "Epoch:        365 | Training loss: 0.24351089762153735 | Validation loss: 0.41827779821206845\n",
      "Epoch:        366 | Training loss: 0.24318093854283263 | Validation loss: 0.4181452851955499\n",
      "Epoch:        367 | Training loss: 0.2428508526968094 | Validation loss: 0.4179770220448924\n",
      "Epoch:        368 | Training loss: 0.24252224267584455 | Validation loss: 0.4177963705432299\n",
      "Epoch:        369 | Training loss: 0.24219550823199615 | Validation loss: 0.4176868432177186\n",
      "Epoch:        370 | Training loss: 0.24186824928921882 | Validation loss: 0.4174824918015028\n",
      "Epoch:        371 | Training loss: 0.2415423675858444 | Validation loss: 0.417341127320461\n",
      "Epoch:        372 | Training loss: 0.24121802451596416 | Validation loss: 0.417178320048798\n",
      "Epoch:        373 | Training loss: 0.2408950554438425 | Validation loss: 0.4170141121398922\n",
      "Epoch:        374 | Training loss: 0.24057155724182913 | Validation loss: 0.4168924434223904\n",
      "Epoch:        375 | Training loss: 0.24024995653786183 | Validation loss: 0.416735255539405\n",
      "Epoch:        376 | Training loss: 0.23992966281820097 | Validation loss: 0.41661272682447786\n",
      "Epoch:        377 | Training loss: 0.23960980406826862 | Validation loss: 0.41645909329195846\n",
      "Epoch:        378 | Training loss: 0.23929089959867947 | Validation loss: 0.4162853140382665\n",
      "Epoch:        379 | Training loss: 0.23897318884460142 | Validation loss: 0.4161560019740349\n",
      "Epoch:        380 | Training loss: 0.23865629671746028 | Validation loss: 0.41599114412605004\n",
      "Epoch:        381 | Training loss: 0.2383408229563539 | Validation loss: 0.4158699587510695\n",
      "Epoch:        382 | Training loss: 0.23802557770797994 | Validation loss: 0.41571127196663454\n",
      "Epoch:        383 | Training loss: 0.23771200687868363 | Validation loss: 0.4155789658399173\n",
      "Epoch:        384 | Training loss: 0.23739913648081135 | Validation loss: 0.4154376899314753\n",
      "Epoch:        385 | Training loss: 0.2370871056514189 | Validation loss: 0.41524330947404076\n",
      "Epoch:        386 | Training loss: 0.23677565925504268 | Validation loss: 0.41510565499131885\n",
      "Epoch:        387 | Training loss: 0.23646556333241955 | Validation loss: 0.41495879030473065\n",
      "Epoch:        388 | Training loss: 0.23615587452076295 | Validation loss: 0.41484639806437884\n",
      "Epoch:        389 | Training loss: 0.23584749244114472 | Validation loss: 0.4146852184535048\n",
      "Epoch:        390 | Training loss: 0.2355399787006666 | Validation loss: 0.41456053043791846\n",
      "Epoch:        391 | Training loss: 0.23523360634533666 | Validation loss: 0.4143948319710631\n",
      "Epoch:        392 | Training loss: 0.23492765449119876 | Validation loss: 0.41427613815700665\n",
      "Epoch:        393 | Training loss: 0.23462344698241655 | Validation loss: 0.4141546233955462\n",
      "Epoch:        394 | Training loss: 0.23431890846330272 | Validation loss: 0.41399306378821565\n",
      "Epoch:        395 | Training loss: 0.23401591894025922 | Validation loss: 0.41385769423764673\n",
      "Epoch:        396 | Training loss: 0.2337143151992898 | Validation loss: 0.4137364355717328\n",
      "Epoch:        397 | Training loss: 0.2334125424684175 | Validation loss: 0.41356758861573967\n",
      "Epoch:        398 | Training loss: 0.23311232233817641 | Validation loss: 0.41342268734349574\n",
      "Epoch:        399 | Training loss: 0.23281265732113487 | Validation loss: 0.41329350790053354\n",
      "Epoch:        400 | Training loss: 0.23251446500338943 | Validation loss: 0.4131847479190516\n",
      "Epoch:        401 | Training loss: 0.23221623289499915 | Validation loss: 0.4130196856327241\n",
      "Epoch:        402 | Training loss: 0.2319194123476981 | Validation loss: 0.4128783104910542\n",
      "Epoch:        403 | Training loss: 0.23162315568338251 | Validation loss: 0.4127564292318081\n",
      "Epoch:        404 | Training loss: 0.23132945894095577 | Validation loss: 0.4126597333270198\n",
      "Epoch:        405 | Training loss: 0.23103523233871032 | Validation loss: 0.41252746813821645\n",
      "Epoch:        406 | Training loss: 0.23074005762609492 | Validation loss: 0.41235966222840603\n",
      "Epoch:        407 | Training loss: 0.23044734202679965 | Validation loss: 0.4122254854172502\n",
      "Epoch:        408 | Training loss: 0.2301564450463919 | Validation loss: 0.4121178945154149\n",
      "Epoch:        409 | Training loss: 0.22986534544613435 | Validation loss: 0.4119845336347991\n",
      "Epoch:        410 | Training loss: 0.2295750899821175 | Validation loss: 0.41185211077054246\n",
      "Epoch:        411 | Training loss: 0.2292850512989462 | Validation loss: 0.4116783544298918\n",
      "Epoch:        412 | Training loss: 0.228996299155636 | Validation loss: 0.4115561017141144\n",
      "Epoch:        413 | Training loss: 0.22870856035946097 | Validation loss: 0.41143780456917667\n",
      "Epoch:        414 | Training loss: 0.22842213796317618 | Validation loss: 0.41132509828976765\n",
      "Epoch:        415 | Training loss: 0.2281353725185468 | Validation loss: 0.4111774242308684\n",
      "Epoch:        416 | Training loss: 0.22784998061953296 | Validation loss: 0.4110446136903045\n",
      "Epoch:        417 | Training loss: 0.2275655564290059 | Validation loss: 0.41090459972845195\n",
      "Epoch:        418 | Training loss: 0.22728165725265262 | Validation loss: 0.41078602793471913\n",
      "Epoch:        419 | Training loss: 0.2269999507399692 | Validation loss: 0.41069448062851477\n",
      "Epoch:        420 | Training loss: 0.22671691685556808 | Validation loss: 0.41055226849547843\n",
      "Epoch:        421 | Training loss: 0.22643535165882867 | Validation loss: 0.41042142805645343\n",
      "Epoch:        422 | Training loss: 0.22615458028231267 | Validation loss: 0.4102896127518934\n",
      "Epoch:        423 | Training loss: 0.2258766040221865 | Validation loss: 0.41019906753522206\n",
      "Epoch:        424 | Training loss: 0.22559632450638933 | Validation loss: 0.41005746714587893\n",
      "Epoch:        425 | Training loss: 0.22531729485843677 | Validation loss: 0.4099121894487227\n",
      "Epoch:        426 | Training loss: 0.22504082579429602 | Validation loss: 0.4098148042578583\n",
      "Epoch:        427 | Training loss: 0.22476292303494924 | Validation loss: 0.4096565357196637\n",
      "Epoch:        428 | Training loss: 0.22448809712417683 | Validation loss: 0.409505294802411\n",
      "Epoch:        429 | Training loss: 0.2242132952781394 | Validation loss: 0.40937769142823965\n",
      "Epoch:        430 | Training loss: 0.22393768823377358 | Validation loss: 0.4092708365499367\n",
      "Epoch:        431 | Training loss: 0.22366352389753238 | Validation loss: 0.4091590973771582\n",
      "Epoch:        432 | Training loss: 0.22339043447321397 | Validation loss: 0.40905389980115786\n",
      "Epoch:        433 | Training loss: 0.2231181106068127 | Validation loss: 0.40892317347863405\n",
      "Epoch:        434 | Training loss: 0.2228467986987103 | Validation loss: 0.40879108261364877\n",
      "Epoch:        435 | Training loss: 0.22257570380091632 | Validation loss: 0.4086867698210348\n",
      "Epoch:        436 | Training loss: 0.22230571226701493 | Validation loss: 0.40857504242099213\n",
      "Epoch:        437 | Training loss: 0.22203624272313757 | Validation loss: 0.4084469023348704\n",
      "Epoch:        438 | Training loss: 0.22176904716287385 | Validation loss: 0.4083633529987958\n",
      "Epoch:        439 | Training loss: 0.22150015483384988 | Validation loss: 0.4082282170460054\n",
      "Epoch:        440 | Training loss: 0.2212328098628687 | Validation loss: 0.408105948109107\n",
      "Epoch:        441 | Training loss: 0.22096693810609863 | Validation loss: 0.4080009156357633\n",
      "Epoch:        442 | Training loss: 0.220700399708208 | Validation loss: 0.4078651903572045\n",
      "Epoch:        443 | Training loss: 0.22043521937609176 | Validation loss: 0.4077371894985217\n",
      "Epoch:        444 | Training loss: 0.22017185827724728 | Validation loss: 0.4075950301320391\n",
      "Epoch:        445 | Training loss: 0.21990728801701112 | Validation loss: 0.40750055127956286\n",
      "Epoch:        446 | Training loss: 0.21964437223668085 | Validation loss: 0.4073974417503937\n",
      "Epoch:        447 | Training loss: 0.21938233034850121 | Validation loss: 0.40729106579573143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:        448 | Training loss: 0.21912091496029498 | Validation loss: 0.40718057663716634\n",
      "Epoch:        449 | Training loss: 0.21885999419925334 | Validation loss: 0.40706495675263\n",
      "Epoch:        450 | Training loss: 0.21859952828562224 | Validation loss: 0.40694336945652976\n",
      "Epoch:        451 | Training loss: 0.21834029345426975 | Validation loss: 0.4068377678530383\n",
      "Epoch:        452 | Training loss: 0.21808134076779062 | Validation loss: 0.406691713827346\n",
      "Epoch:        453 | Training loss: 0.21782310367567237 | Validation loss: 0.4065816102267918\n",
      "Epoch:        454 | Training loss: 0.2175656833106895 | Validation loss: 0.40646754676537133\n",
      "Epoch:        455 | Training loss: 0.217308975111601 | Validation loss: 0.40638406021940626\n",
      "Epoch:        456 | Training loss: 0.2170546879049753 | Validation loss: 0.4062178157201805\n",
      "Epoch:        457 | Training loss: 0.21679780830955442 | Validation loss: 0.406122533552396\n",
      "Epoch:        458 | Training loss: 0.21654235153805432 | Validation loss: 0.4060297893990572\n",
      "Epoch:        459 | Training loss: 0.21628826169531626 | Validation loss: 0.4059321815045657\n",
      "Epoch:        460 | Training loss: 0.21603497903166827 | Validation loss: 0.40582921585326526\n",
      "Epoch:        461 | Training loss: 0.21578452337228538 | Validation loss: 0.40575360167351954\n",
      "Epoch:        462 | Training loss: 0.21552985925759616 | Validation loss: 0.40560570190782785\n",
      "Epoch:        463 | Training loss: 0.21527866028822612 | Validation loss: 0.40546666893646716\n",
      "Epoch:        464 | Training loss: 0.21502779997228305 | Validation loss: 0.4053574184458678\n",
      "Epoch:        465 | Training loss: 0.21477704088410238 | Validation loss: 0.40526742571201174\n",
      "Epoch:        466 | Training loss: 0.2145277155093855 | Validation loss: 0.40514644479554823\n",
      "Epoch:        467 | Training loss: 0.21427883044166085 | Validation loss: 0.40503666812230976\n",
      "Epoch:        468 | Training loss: 0.21403012713098787 | Validation loss: 0.4049490213929041\n",
      "Epoch:        469 | Training loss: 0.21378302797452328 | Validation loss: 0.404861865928374\n",
      "Epoch:        470 | Training loss: 0.2135355850945529 | Validation loss: 0.40474668008452674\n",
      "Epoch:        471 | Training loss: 0.21329115521001785 | Validation loss: 0.40467110847791815\n",
      "Epoch:        472 | Training loss: 0.21304348509055784 | Validation loss: 0.40453909683895434\n",
      "Epoch:        473 | Training loss: 0.21279797875886536 | Validation loss: 0.4044249470927188\n",
      "Epoch:        474 | Training loss: 0.21255326509891273 | Validation loss: 0.4043112070855892\n",
      "Epoch:        475 | Training loss: 0.21230945759936243 | Validation loss: 0.4041944306728286\n",
      "Epoch:        476 | Training loss: 0.21206594407610685 | Validation loss: 0.4040990130970394\n",
      "Epoch:        477 | Training loss: 0.21182334642530057 | Validation loss: 0.40398611667733986\n",
      "Epoch:        478 | Training loss: 0.21158124458590985 | Validation loss: 0.40388059664934345\n",
      "Epoch:        479 | Training loss: 0.21134230198474288 | Validation loss: 0.4037470988091057\n",
      "Epoch:        480 | Training loss: 0.21109994650584202 | Validation loss: 0.40365635354536245\n",
      "Epoch:        481 | Training loss: 0.21085920074112663 | Validation loss: 0.40355892173743824\n",
      "Epoch:        482 | Training loss: 0.21061878865550285 | Validation loss: 0.4034691336682451\n",
      "Epoch:        483 | Training loss: 0.21037991407754703 | Validation loss: 0.4033990321574138\n",
      "Epoch:        484 | Training loss: 0.21014467292762118 | Validation loss: 0.4033349054703899\n",
      "Epoch:        485 | Training loss: 0.20990482458604617 | Validation loss: 0.40321496127267026\n",
      "Epoch:        486 | Training loss: 0.20966614337327721 | Validation loss: 0.40309413790252696\n",
      "Epoch:        487 | Training loss: 0.20943071132361998 | Validation loss: 0.4030129848616822\n",
      "Epoch:        488 | Training loss: 0.20919345671230677 | Validation loss: 0.40289792767882304\n",
      "Epoch:        489 | Training loss: 0.2089574813503881 | Validation loss: 0.4027905531892129\n",
      "Epoch:        490 | Training loss: 0.20872253739508823 | Validation loss: 0.40265675362381054\n",
      "Epoch:        491 | Training loss: 0.2084877810535611 | Validation loss: 0.40256586857786614\n",
      "Epoch:        492 | Training loss: 0.20825387315981012 | Validation loss: 0.40246925609676687\n",
      "Epoch:        493 | Training loss: 0.20802115104961932 | Validation loss: 0.4023561213256771\n",
      "Epoch:        494 | Training loss: 0.2077884911416652 | Validation loss: 0.40225708315537545\n",
      "Epoch:        495 | Training loss: 0.20755592284852165 | Validation loss: 0.4021929651274633\n",
      "Epoch:        496 | Training loss: 0.2073262520493661 | Validation loss: 0.4021247120528588\n",
      "Epoch:        497 | Training loss: 0.20709420304959614 | Validation loss: 0.40201468929635453\n",
      "Epoch:        498 | Training loss: 0.2068627555530405 | Validation loss: 0.40189749156180526\n",
      "Epoch:        499 | Training loss: 0.206632745655962 | Validation loss: 0.40179490798290757\n"
     ]
    }
   ],
   "source": [
    "weight, train_loss_count, dev_loss_count = SGD(X_tr=train_count_vector,\n",
    "                                             Y_tr=train_labels,\n",
    "                                             X_dev=dev_count_vector,\n",
    "                                             Y_dev=dev_labels,\n",
    "                                             lr=best_hyperparams[0],\n",
    "                                             alpha=best_hyperparams[1],\n",
    "                                             tolerance=0.00001,\n",
    "                                             epochs=500,\n",
    "                                             print_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the training and validation history per epoch for the best hyperparameter combination. Does your model underfit, overfit or is it about right? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:51.598911Z",
     "start_time": "2020-02-15T14:17:51.482307Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_count, label='Training loss')\n",
    "plt.plot(dev_loss_count, label='Validation loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.title('Training loss vs Validation loss (BOW - Count)')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:52:26.583150Z",
     "start_time": "2020-01-21T16:52:26.578754Z"
    }
   },
   "source": [
    "Explain here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "Compute accuracy, precision, recall and F1-scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:51.607940Z",
     "start_time": "2020-02-15T14:17:51.600272Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.845\n",
      "Precision: 0.8349514563106796\n",
      "Recall: 0.86\n",
      "F1-Score: 0.8472906403940887\n"
     ]
    }
   ],
   "source": [
    "preds_te_count = predict_class(test_count_vector, weight)\n",
    "\n",
    "print('Accuracy:', accuracy_score(test_labels,preds_te_count))\n",
    "print('Precision:', precision_score(test_labels,preds_te_count))\n",
    "print('Recall:', recall_score(test_labels,preds_te_count))\n",
    "print('F1-Score:', f1_score(test_labels,preds_te_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, print the top-10 words for the negative and positive class respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:51.613935Z",
     "start_time": "2020-02-15T14:17:51.610660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('bad',)\n",
      "('only',)\n",
      "('plot',)\n",
      "('worst',)\n",
      "('unfortunately',)\n",
      "('script',)\n",
      "('why',)\n",
      "('boring',)\n",
      "('nothing',)\n",
      "('any',)\n"
     ]
    }
   ],
   "source": [
    "top_negative_words = weight.argsort()[:10]\n",
    "for i in top_negative_words:\n",
    "    print(vocab_word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:51.624122Z",
     "start_time": "2020-02-15T14:17:51.615674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('great',)\n",
      "('well',)\n",
      "('also',)\n",
      "('seen',)\n",
      "('life',)\n",
      "('world',)\n",
      "('many',)\n",
      "('see',)\n",
      "('very',)\n",
      "('fun',)\n"
     ]
    }
   ],
   "source": [
    "top_positive_words = weight.argsort()[::-1][:10]\n",
    "for i in top_positive_words:\n",
    "    print(vocab_word[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to apply the classifier we've learned into a different domain such laptop reviews or restaurant reviews, do you think these features would generalise well? Can you propose what features the classifier could pick up as important in the new domain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This classifier is trained on movie reviews data. Word count are the features we used to train this classfier and therefore if we were to apply this classifer in different domain such as laptop review or restaurant review, Most probably it would classify reviews correctly as reviews are either positive or negative, given that reviews are positive if the words used are among the top positive words and negative reviews contain words from top negative words of this classifier.\n",
    "\n",
    "#### But, Restaurant and laptop reviews use different words to express positive and negative. In case of restaurant reviews,positive words like 'Delicious','Amazing','Welcomed','Cozy','Luxurious','Atmosphere',etc. And negative words like 'Unequipped','Snappy','mediocre',etc. And in case of Laptop reviews,positive words like 'Budget-friendly','fast','productivity','Gaming','Performance',etc. And negative words like 'Unsatisfied','Unhygenic',etc.\n",
    "#### The above mentioned words are features that can be important in the new domain. I think that this classifier would not generalise well in different domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters (e.g. learning rate and regularisation strength)? What is the relation between training epochs and learning rate? How the regularisation strength affects performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generated 5 different values using logspace and linspace and trained the model for all 25 different variations and selected the best hyperparameter from the results. From online review i found out that learning rate range is between 0.0001 to 0.01.\n",
    "\n",
    "#### If learning rate is too high then the cost function will converge very fast or in less epochs which may result in underfitting. if learning rate is too low then the cost function will converge very slow or takes higher epochs. Each epoch results in a larger weight update. Consequently, a higher learning rate will result in fewer convergence epochs.\n",
    "\n",
    "#### from the above trails, it can observed that tiny changes in the regularization parameter affects the number of epochs required for convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Logistic Regression with TF.IDF vectors\n",
    "\n",
    "Follow the same steps as above (i.e. evaluating count n-gram representations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper Parameters -- Learning Rate - 1e-05  Regularization Parameter- 1e-05\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1e-05  Regularization Parameter- 6.309573444801929e-05\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1e-05  Regularization Parameter- 0.00039810717055349735\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1e-05  Regularization Parameter- 0.002511886431509582\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1e-05  Regularization Parameter- 0.01584893192461114\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.0020080000000000002  Regularization Parameter- 1e-05\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.0020080000000000002  Regularization Parameter- 6.309573444801929e-05\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.0020080000000000002  Regularization Parameter- 0.00039810717055349735\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.0020080000000000002  Regularization Parameter- 0.002511886431509582\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.0020080000000000002  Regularization Parameter- 0.01584893192461114\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.004006  Regularization Parameter- 1e-05\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.004006  Regularization Parameter- 6.309573444801929e-05\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.004006  Regularization Parameter- 0.00039810717055349735\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.004006  Regularization Parameter- 0.002511886431509582\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.004006  Regularization Parameter- 0.01584893192461114\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.006004000000000001  Regularization Parameter- 1e-05\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.006004000000000001  Regularization Parameter- 6.309573444801929e-05\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.006004000000000001  Regularization Parameter- 0.00039810717055349735\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.006004000000000001  Regularization Parameter- 0.002511886431509582\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.006004000000000001  Regularization Parameter- 0.01584893192461114\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.008002  Regularization Parameter- 1e-05\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.008002  Regularization Parameter- 6.309573444801929e-05\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.008002  Regularization Parameter- 0.00039810717055349735\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.008002  Regularization Parameter- 0.002511886431509582\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.008002  Regularization Parameter- 0.01584893192461114\n"
     ]
    }
   ],
   "source": [
    "#hyper paramater tuning for BOW-tfidf\n",
    "train_loss_history =[]\n",
    "validation_loss_history=[]\n",
    "hyperparam_history=[]\n",
    "#\n",
    "for lr_i in lr:\n",
    "    for alpha_i in alpha:\n",
    "        print('\\nHyper Parameters -- Learning Rate -',lr_i,' Regularization Parameter-',alpha_i)\n",
    "        weight, train_loss_count, dev_loss_count = SGD(X_tr=train_tf_idf,\n",
    "                                             Y_tr=train_labels,\n",
    "                                             X_dev=dev_tf_idf,\n",
    "                                             Y_dev=dev_labels,\n",
    "                                             lr=lr_i,\n",
    "                                             alpha=alpha_i,\n",
    "                                             tolerance=0.00001,\n",
    "                                             epochs=800,\n",
    "                                             print_progress=False)\n",
    "        if len(train_loss_count) > 40:\n",
    "            train_loss_history.append(train_loss_count[-1])\n",
    "            validation_loss_history.append(dev_loss_count[-1])\n",
    "            hyperparam_history.append([lr_i,alpha_i])\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW-TFIDF\n",
      "Best Training loss - 0.01744929454307362,best_validation_loss-0.3313827620476839,\n",
      "best_hyperparams-[0.008002, 1e-05]\n"
     ]
    }
   ],
   "source": [
    "#get the best hyper parameters\n",
    "temp = np.array(train_loss_history)\n",
    "train_loss_history_sortidx = np.argsort(temp)\n",
    "\n",
    "best_train_loss_BOW_tfidf = temp[train_loss_history_sortidx[0]]\n",
    "best_validation_loss_BOW_tfidf = validation_loss_history[train_loss_history_sortidx[0]]\n",
    "best_hyperparams_BOW_tfidf=hyperparam_history[train_loss_history_sortidx[0]]\n",
    "\n",
    "print(\"BOW-TFIDF\")\n",
    "print(f'Best Training loss - {best_train_loss_BOW_tfidf},best_validation_loss-{best_validation_loss_BOW_tfidf},\\nbest_hyperparams-{best_hyperparams_BOW_tfidf}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:          0 | Training loss: 0.5000687628863478 | Validation loss: 0.5943528148320568\n",
      "Epoch:          1 | Training loss: 0.39920986884994153 | Validation loss: 0.5428902906558546\n",
      "Epoch:          2 | Training loss: 0.33499138039352355 | Validation loss: 0.508580032542277\n",
      "Epoch:          3 | Training loss: 0.29000179667592907 | Validation loss: 0.4845671543225173\n",
      "Epoch:          4 | Training loss: 0.25627014471622933 | Validation loss: 0.46684129906035027\n",
      "Epoch:          5 | Training loss: 0.2297458089909241 | Validation loss: 0.45205353662995096\n",
      "Epoch:          6 | Training loss: 0.20825335037158837 | Validation loss: 0.440253446789454\n",
      "Epoch:          7 | Training loss: 0.19063950647322533 | Validation loss: 0.4307245702563378\n",
      "Epoch:          8 | Training loss: 0.17571415147854116 | Validation loss: 0.4223184849980601\n",
      "Epoch:          9 | Training loss: 0.16313514118760217 | Validation loss: 0.4154286345989037\n",
      "Epoch:         10 | Training loss: 0.15233302691037892 | Validation loss: 0.4095581931210676\n",
      "Epoch:         11 | Training loss: 0.14248769777094802 | Validation loss: 0.4033856303816895\n",
      "Epoch:         12 | Training loss: 0.13402776573644645 | Validation loss: 0.3985700575132637\n",
      "Epoch:         13 | Training loss: 0.12659288795590481 | Validation loss: 0.3944502399601706\n",
      "Epoch:         14 | Training loss: 0.11984430551756174 | Validation loss: 0.3903455132172136\n",
      "Epoch:         15 | Training loss: 0.11383599085876578 | Validation loss: 0.3868023405782895\n",
      "Epoch:         16 | Training loss: 0.10840242385149411 | Validation loss: 0.3837186499239702\n",
      "Epoch:         17 | Training loss: 0.10347369225721091 | Validation loss: 0.3808620681655126\n",
      "Epoch:         18 | Training loss: 0.09894391240847246 | Validation loss: 0.3779948549001507\n",
      "Epoch:         19 | Training loss: 0.09479263724454472 | Validation loss: 0.37558441628371925\n",
      "Epoch:         20 | Training loss: 0.09098871924656077 | Validation loss: 0.3732604072060216\n",
      "Epoch:         21 | Training loss: 0.08748261549740667 | Validation loss: 0.3711773375441949\n",
      "Epoch:         22 | Training loss: 0.08427194176593678 | Validation loss: 0.36940727045842037\n",
      "Epoch:         23 | Training loss: 0.08123619623984052 | Validation loss: 0.36742806644112663\n",
      "Epoch:         24 | Training loss: 0.0784718329233569 | Validation loss: 0.365931564740213\n",
      "Epoch:         25 | Training loss: 0.0758300103311477 | Validation loss: 0.3642032708495659\n",
      "Epoch:         26 | Training loss: 0.07340995328355328 | Validation loss: 0.36284864239845566\n",
      "Epoch:         27 | Training loss: 0.07111099828057926 | Validation loss: 0.3613707816523887\n",
      "Epoch:         28 | Training loss: 0.06896805999201652 | Validation loss: 0.36005571733005015\n",
      "Epoch:         29 | Training loss: 0.06696821305405734 | Validation loss: 0.3589500726377199\n",
      "Epoch:         30 | Training loss: 0.06505900340137155 | Validation loss: 0.3576963856805789\n",
      "Epoch:         31 | Training loss: 0.06327138240390888 | Validation loss: 0.35662330068380554\n",
      "Epoch:         32 | Training loss: 0.06158389751687704 | Validation loss: 0.35562200620074175\n",
      "Epoch:         33 | Training loss: 0.05998304066038968 | Validation loss: 0.3545915982541497\n",
      "Epoch:         34 | Training loss: 0.05846767805412077 | Validation loss: 0.3537046399996238\n",
      "Epoch:         35 | Training loss: 0.057030512281680615 | Validation loss: 0.35283445001143626\n",
      "Epoch:         36 | Training loss: 0.05566514411310914 | Validation loss: 0.35198189105898847\n",
      "Epoch:         37 | Training loss: 0.05436655515442238 | Validation loss: 0.35121852830235784\n",
      "Epoch:         38 | Training loss: 0.05313005394056189 | Validation loss: 0.35045231369952434\n",
      "Epoch:         39 | Training loss: 0.05195298579768281 | Validation loss: 0.3497083703444011\n",
      "Epoch:         40 | Training loss: 0.05082831246401901 | Validation loss: 0.3490283060562943\n",
      "Epoch:         41 | Training loss: 0.04975474675976106 | Validation loss: 0.34836949714068877\n",
      "Epoch:         42 | Training loss: 0.048725894556434356 | Validation loss: 0.3477654647826584\n",
      "Epoch:         43 | Training loss: 0.047744689540984465 | Validation loss: 0.3471533286132313\n",
      "Epoch:         44 | Training loss: 0.04680148307650878 | Validation loss: 0.34661173051075067\n",
      "Epoch:         45 | Training loss: 0.04589910487292043 | Validation loss: 0.3460751346028658\n",
      "Epoch:         46 | Training loss: 0.04503588589591633 | Validation loss: 0.3456048042068136\n",
      "Epoch:         47 | Training loss: 0.044207019153829595 | Validation loss: 0.34513449972913707\n",
      "Epoch:         48 | Training loss: 0.04340392498276445 | Validation loss: 0.3445985532478361\n",
      "Epoch:         49 | Training loss: 0.042637540722205304 | Validation loss: 0.34407803320067815\n",
      "Epoch:         50 | Training loss: 0.041897683744451686 | Validation loss: 0.343644673588846\n",
      "Epoch:         51 | Training loss: 0.041186406606258635 | Validation loss: 0.3432171744362746\n",
      "Epoch:         52 | Training loss: 0.04050214149427618 | Validation loss: 0.34280107332000886\n",
      "Epoch:         53 | Training loss: 0.039838472686686846 | Validation loss: 0.3424492154248247\n",
      "Epoch:         54 | Training loss: 0.03920083530317034 | Validation loss: 0.34207726694126506\n",
      "Epoch:         55 | Training loss: 0.038586243896331 | Validation loss: 0.3416829828891649\n",
      "Epoch:         56 | Training loss: 0.037990995766305054 | Validation loss: 0.3413623338245357\n",
      "Epoch:         57 | Training loss: 0.037416608888609725 | Validation loss: 0.3410251916844907\n",
      "Epoch:         58 | Training loss: 0.036862197232481526 | Validation loss: 0.34074158648316233\n",
      "Epoch:         59 | Training loss: 0.03632418577979033 | Validation loss: 0.3404001600189022\n",
      "Epoch:         60 | Training loss: 0.03580426195842193 | Validation loss: 0.3400921748685252\n",
      "Epoch:         61 | Training loss: 0.03530095823851452 | Validation loss: 0.3398081961741603\n",
      "Epoch:         62 | Training loss: 0.03481335658487021 | Validation loss: 0.3395307243473487\n",
      "Epoch:         63 | Training loss: 0.03434078716764366 | Validation loss: 0.3392590047153974\n",
      "Epoch:         64 | Training loss: 0.033882648325428456 | Validation loss: 0.3389668115105515\n",
      "Epoch:         65 | Training loss: 0.03343850865205626 | Validation loss: 0.3387050553369445\n",
      "Epoch:         66 | Training loss: 0.033006930826666184 | Validation loss: 0.3385074418202704\n",
      "Epoch:         67 | Training loss: 0.03258920513097954 | Validation loss: 0.3383038340574391\n",
      "Epoch:         68 | Training loss: 0.03218163261075827 | Validation loss: 0.338046363280083\n",
      "Epoch:         69 | Training loss: 0.03178629337640348 | Validation loss: 0.33780109422050436\n",
      "Epoch:         70 | Training loss: 0.031402514070550426 | Validation loss: 0.3376093395616946\n",
      "Epoch:         71 | Training loss: 0.031029560074560487 | Validation loss: 0.33735488880841347\n",
      "Epoch:         72 | Training loss: 0.03066609809750166 | Validation loss: 0.3371646563922275\n",
      "Epoch:         73 | Training loss: 0.030312745844919605 | Validation loss: 0.33700183932381633\n",
      "Epoch:         74 | Training loss: 0.029968846446398203 | Validation loss: 0.33680381273570686\n",
      "Epoch:         75 | Training loss: 0.0296340976847317 | Validation loss: 0.3366198013180812\n",
      "Epoch:         76 | Training loss: 0.02930830664275227 | Validation loss: 0.3364621001448229\n",
      "Epoch:         77 | Training loss: 0.028990597794295865 | Validation loss: 0.3362849941568476\n",
      "Epoch:         78 | Training loss: 0.028681079741042836 | Validation loss: 0.33611683807384296\n",
      "Epoch:         79 | Training loss: 0.028379361638838807 | Validation loss: 0.3359514616461552\n",
      "Epoch:         80 | Training loss: 0.02808527811401247 | Validation loss: 0.33580265218258326\n",
      "Epoch:         81 | Training loss: 0.027798322924846183 | Validation loss: 0.3356434873278126\n",
      "Epoch:         82 | Training loss: 0.027518510138858504 | Validation loss: 0.33550830805641896\n",
      "Epoch:         83 | Training loss: 0.02724573828754203 | Validation loss: 0.33538803309271953\n",
      "Epoch:         84 | Training loss: 0.026978785788872795 | Validation loss: 0.3352062187187219\n",
      "Epoch:         85 | Training loss: 0.026718982460657854 | Validation loss: 0.33505200424436715\n",
      "Epoch:         86 | Training loss: 0.02646421128843187 | Validation loss: 0.3349557961906654\n",
      "Epoch:         87 | Training loss: 0.02621588230609095 | Validation loss: 0.33482626147780936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:         88 | Training loss: 0.025973306720391248 | Validation loss: 0.334695588240145\n",
      "Epoch:         89 | Training loss: 0.025736477503264833 | Validation loss: 0.3345627173753212\n",
      "Epoch:         90 | Training loss: 0.025504439328551183 | Validation loss: 0.33445741962174685\n",
      "Epoch:         91 | Training loss: 0.025278057005136287 | Validation loss: 0.33433456728305166\n",
      "Epoch:         92 | Training loss: 0.025055991256853295 | Validation loss: 0.334248096385748\n",
      "Epoch:         93 | Training loss: 0.02483917047448617 | Validation loss: 0.3341666762470636\n",
      "Epoch:         94 | Training loss: 0.024627020160449292 | Validation loss: 0.334040682245154\n",
      "Epoch:         95 | Training loss: 0.024419490226773043 | Validation loss: 0.3339347650191625\n",
      "Epoch:         96 | Training loss: 0.024216176467809953 | Validation loss: 0.33386107894527284\n",
      "Epoch:         97 | Training loss: 0.02401729117559786 | Validation loss: 0.3337511055608183\n",
      "Epoch:         98 | Training loss: 0.023822449933726477 | Validation loss: 0.3336680807760572\n",
      "Epoch:         99 | Training loss: 0.023631693085793673 | Validation loss: 0.3335947989473549\n",
      "Epoch:        100 | Training loss: 0.023444789830848127 | Validation loss: 0.3335013088546437\n",
      "Epoch:        101 | Training loss: 0.023261700463329465 | Validation loss: 0.33342095818581724\n",
      "Epoch:        102 | Training loss: 0.023082305451976218 | Validation loss: 0.33334607710168085\n",
      "Epoch:        103 | Training loss: 0.022906521582143832 | Validation loss: 0.3332759015791804\n",
      "Epoch:        104 | Training loss: 0.0227340970849175 | Validation loss: 0.33318702999270644\n",
      "Epoch:        105 | Training loss: 0.022565143720884504 | Validation loss: 0.33312356323495407\n",
      "Epoch:        106 | Training loss: 0.022399469327307974 | Validation loss: 0.33305655261512557\n",
      "Epoch:        107 | Training loss: 0.0222368847679602 | Validation loss: 0.3329729849120309\n",
      "Epoch:        108 | Training loss: 0.02207749680342985 | Validation loss: 0.33289469335810784\n",
      "Epoch:        109 | Training loss: 0.02192113061749038 | Validation loss: 0.33282530716963094\n",
      "Epoch:        110 | Training loss: 0.021767645850881773 | Validation loss: 0.3327662542743807\n",
      "Epoch:        111 | Training loss: 0.02161702908963536 | Validation loss: 0.33271608733312436\n",
      "Epoch:        112 | Training loss: 0.021469228297094942 | Validation loss: 0.33264829330888546\n",
      "Epoch:        113 | Training loss: 0.021324147223538807 | Validation loss: 0.33258670277555824\n",
      "Epoch:        114 | Training loss: 0.02118166049449371 | Validation loss: 0.33254272246460137\n",
      "Epoch:        115 | Training loss: 0.021041774047631835 | Validation loss: 0.33249063420634306\n",
      "Epoch:        116 | Training loss: 0.020904391487915893 | Validation loss: 0.3324353822707095\n",
      "Epoch:        117 | Training loss: 0.02076947421132589 | Validation loss: 0.3323918789259984\n",
      "Epoch:        118 | Training loss: 0.02063692156264473 | Validation loss: 0.33234279928875377\n",
      "Epoch:        119 | Training loss: 0.020506699687832364 | Validation loss: 0.3322961673250691\n",
      "Epoch:        120 | Training loss: 0.020378755118865943 | Validation loss: 0.33225394356891913\n",
      "Epoch:        121 | Training loss: 0.02025301591277404 | Validation loss: 0.33221187694082394\n",
      "Epoch:        122 | Training loss: 0.020129367463060432 | Validation loss: 0.33215894402804735\n",
      "Epoch:        123 | Training loss: 0.020007895082795914 | Validation loss: 0.33212110423885854\n",
      "Epoch:        124 | Training loss: 0.019888429092080002 | Validation loss: 0.3320748502490112\n",
      "Epoch:        125 | Training loss: 0.019770983660456536 | Validation loss: 0.33203317401255716\n",
      "Epoch:        126 | Training loss: 0.0196555055539041 | Validation loss: 0.3319959486378686\n",
      "Epoch:        127 | Training loss: 0.01954193074632353 | Validation loss: 0.33195592261116164\n",
      "Epoch:        128 | Training loss: 0.019430230777789517 | Validation loss: 0.331910043437494\n",
      "Epoch:        129 | Training loss: 0.01932034696614566 | Validation loss: 0.3318776262994879\n",
      "Epoch:        130 | Training loss: 0.019212250420639027 | Validation loss: 0.33184257203303985\n",
      "Epoch:        131 | Training loss: 0.019105897003508832 | Validation loss: 0.3318090286991676\n",
      "Epoch:        132 | Training loss: 0.019001256499751953 | Validation loss: 0.33177267734265853\n",
      "Epoch:        133 | Training loss: 0.018898267193748496 | Validation loss: 0.3317421755855795\n",
      "Epoch:        134 | Training loss: 0.01879688897932875 | Validation loss: 0.33171568541427576\n",
      "Epoch:        135 | Training loss: 0.018697114552796813 | Validation loss: 0.3316863788727208\n",
      "Epoch:        136 | Training loss: 0.0185989094413352 | Validation loss: 0.33165491499624183\n",
      "Epoch:        137 | Training loss: 0.01850219165944013 | Validation loss: 0.33163282782057507\n",
      "Epoch:        138 | Training loss: 0.018407008644148185 | Validation loss: 0.3316006614351423\n",
      "Epoch:        139 | Training loss: 0.018313214790264962 | Validation loss: 0.3315875633466498\n",
      "Epoch:        140 | Training loss: 0.018220891429279976 | Validation loss: 0.3315590507014351\n",
      "Epoch:        141 | Training loss: 0.018129993139736483 | Validation loss: 0.3315282265213715\n",
      "Epoch:        142 | Training loss: 0.01804050603452024 | Validation loss: 0.3314962386870355\n",
      "Epoch:        143 | Training loss: 0.017952196458610216 | Validation loss: 0.3314833432620698\n",
      "Epoch:        144 | Training loss: 0.017865256602693468 | Validation loss: 0.3314647096970907\n",
      "Epoch:        145 | Training loss: 0.017779596364837884 | Validation loss: 0.3314479889361393\n",
      "Epoch:        146 | Training loss: 0.017695192070307474 | Validation loss: 0.3314326987869932\n",
      "Epoch:        147 | Training loss: 0.017612022061507505 | Validation loss: 0.33141926216947515\n",
      "Epoch:        148 | Training loss: 0.017530066464825536 | Validation loss: 0.3314015231148633\n",
      "Epoch:        149 | Training loss: 0.01744929454307362 | Validation loss: 0.3313827620476839\n"
     ]
    }
   ],
   "source": [
    "weight_tfidf, train_loss_count_tfidf, dev_loss_count_tfidf = SGD(X_tr=train_tf_idf,\n",
    "                                             Y_tr=train_labels,\n",
    "                                             X_dev=dev_tf_idf,\n",
    "                                             Y_dev=dev_labels,\n",
    "                                             lr=best_hyperparams_BOW_tfidf[0],\n",
    "                                             alpha=best_hyperparams_BOW_tfidf[1],\n",
    "                                             tolerance=0.00001,\n",
    "                                             epochs=1000,\n",
    "                                             print_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA5cElEQVR4nO3deXxU5b348c83M9kXAiSsAQIKIshqwAVE3H51qwtqlVoV9bq11qq3Vrtqb6+tvVJrbbVeautWLXprpWpxqSvgyqIiIChLgABCCJAEsiff3x/PSRiGSTIJmcwk832/XvOaOc855znfmWTme57nnPMcUVWMMcbEr4RoB2CMMSa6LBEYY0ycs0RgjDFxzhKBMcbEOUsExhgT5ywRGGNMnLNEEEUi8rKIXNHRy7YxhukiUtTR9cYqEXlbRP7De32piLwWzrLt2M5gEdkrIr72xtpC3Soih3d0vS1s7zoRub+zttddiEhfEflcRJKjHUtrLBG0kfflbnw0iEhlwPSlbalLVc9Q1cc7etnuTER+KCILQpTniEiNiBwVbl2q+pSq/r8OiqtQRE4NqHuTqmaoan1H1B8tIpIE/AS415vO9xJR4//8dhF5SEQSA9ZJFpFficgm7/vxpYjcJiLizZ8pIquCtvPvZsruaEfMKwPiqxeRqoDpH4nILK888Lv8B2/dx0Tkv1t4ry+JyGlB2ysM+h3YKyIDVHU78BZwbVvfQ2ezRNBG3pc7Q1UzgE3A1wPKnmpcTkT80YuyW3sSOF5EhgaVXwJ8pqorohBTd3YusFpVtwSVZ3vfgTHAccB3Aub9H3AKcCaQCVyG+zH8nTf/HeBIEcmFpu/KOCAtqOw44KCk3xpVHR3wHV0I3BjwHf2lt9j7gd9lVb2xhSob3+s44N/A8yIyK2iZrwfVt9Urfwq4rq3vobNZIuggjV0sInK7iHwFPCoiPb09iGIR2e29zgtYJ7CbYpaILBKR2d6yG0TkjHYuO1REFohIuYi8LiIPishfw3wfR3rb2uPtWZ0TMO9MEVnl1btFRL7vled4722PiOwSkYUictD/log8LCKzg8r+KSK3eq9v9+otF5E1InJKcB2qWgS8iftxCXQ58Hhrn3nQtmeJyKKA6dNEZLWIlHp7iBIw7zAReVNESkRkp4g8JSLZ3rwngcHAi97e4A8C9ib93jIDROQF7/NZKyLXBNR9l4g8KyJPeO99pYgUhP4LHfQeenjrFYvIRhH5SeNnLyKHi8g73vvZKSLPeOUiIr8VkR3evOXSfEvqDNwPd0iqugP34zjKq/sU4P8BF6jqClWtU9UPgG8B3xGRw70fyfXANK+aicBKbzuBZQnAknA+h86gql+p6u+Au4Bfh/ofD+FDYJiIDIlocIfIEkHH6gf0Aobg9oASgEe96cFAJfCHFtY/BlgD5AD/A/xZRKQdyz4NfAT0xv3TBv9ohiSuef8i8BrQB/gu8JSIHOEt8mfgOlXNBI7C/SAD/CdQBOQCfYEfAaHGLnkauLgxThHpifvRmOtt40Zgklf/14DCZkJ9PPA9eeuOB/5G2z/zxjpygOdw3SA5wDpgSuAiwK+AAcCRwCDcZ4uqXsaBrcP/CbGJv+E+owHAhcAvgxLdOcBcIBt4IZyYPb8HegDDgBNxCfFKb94vcH/LnkCetyy4z3waMMLb3sVASTP1j8H9n4UkIgNwf6sPvKLTgA9VdXPgcqr6Ie79N77nBez/0Z+G23NfFFT2garWNLftKPoH7vtxRGsLqmodsBbXmohZlgg6VgNwp6pWq2qlqpao6nOqWqGq5cDduC9rczaq6p+8fuXHgf64H9awlxWRwcAk4GeqWqOqi3A/LOE4FsgA7vHWfRN4CZjpza8FRolIlqruVtVlAeX9gSGqWquqCzX0IFYLcQniBG/6QlwTfStQDyR79SeqaqGqrmsmzue993q8N3058LKqFrfjM290JrBKVf+uqrXA/cBXjTNVda2q/tv72xYD94VZLyIyCJgK3K6qVar6CfAIByboRao63/t7PkkYPxziDkRfDPxQVctVtRD4TUC9tbiEOMDb7qKA8kxgJCCq+rmqbmtmM9lAeYjynSKyB9gC7AP+7pXnAM3Vtc2bDwfu/Z+A+99YGFTWbEukAxzrtWAbH8e2Yd3Gbp9eAWXzAuqaF7R8Oe5zjFmWCDpWsapWNU6ISJqI/K/XZC/D7QVlS/NnkgT+8FR4LzPauOwAYFdAGcABe2ctGABsVtWGgLKNwEDv9QW4H8yNXpfDcV75vbi9ntdEZL00c4DPSw5z2Z9YvonrQ0VV1wI34/ayd4jIXG9vM1Q9Fbh+6Mu91sWluGTYns/8gPceFGvTtIj08WLa4tX7V/b/qLWm8W8S+IMa+LlCwN8TqABSpPXjTDlAkldXqHp/gGvJfOR1N13lvbc3cS2OB4HtIjJHRLKa2cZuXNI4aNuqmg2kAe8Cr3jlO3E7BaH09+aD+7uM9VqFx+J2CFYD/b2yqTRzfEDcAd/Gg7IPN7Ot1nygqtkBjw9aX6VJ4+e7K6DsvIC6zgtaPhPY0844O4Ulgo4VvBf8n7jm4zGqmsX+vZ3muns6wjagl4ikBZQNCnPdrcCgoL7Pwbi9PlR1saqei2sWzwOe9crLVfU/VXUY8HXgVgnRv+/5G3Ch12d6DK47Bq+ep1V1Km4vVoFftxDr48A3cF0RmbiWC7T/M99GwOfkJZjAz+1XXkxjvXq/FVRnS8P4bsX9TQJ/UJs+10Owk/17/QfV6/VpX6OqA3AHLB8S77RTVX1AVY8GRuO6iG5rZhvLvfkhqWol8BhwnNe99jpwjNcKaiIik3Gf55veeutxn8u1wCZV3est+r5XlsH+7qbgbf4y4KDs9c3FFkHnAztoocuskZfMDwc+jXRQh8ISQWRl4vqo94hIL+DOSG9QVTfiDrDdJSJJ3l7718Nc/UNcM/8HIpIoItO9ded6dV0qIj28rpMyXHcOInK2d2BSAspDnjapqh8DxbiukVdVdY9XxxEicrK4c66rcJ9bS6deLsTtZc0B5gb0Jbf3M/8XMFpEZnhf3ptwx3waZQJ7vXoHcvAP53ZcP/1BvP7y94BfiUiKiIwFrsZrDbWX1430LHC3iGR6yfVWXGsFEblI9h8o341LVvUiMklEjvGOCe3Dfd7NfdbzaaELzPt7XYZr0ZSo6uvAG8BzIjJaRHxet8tTwB9V9cuA1Rd68S4MKFvklS3xkkzMEHddwI24/6kfBrWcmzMZKPS+lzHLEkFk3Q+k4vbcPmB/8znSLsWdelcC/DfwDFDd2krej+k5uDNFdgIPAZd7TXZwX/hCr2vketxeMcBw3J7gXtwe3UOq+nYLm/obcCru4HGjZOAeb7tf4VodP2ohVgWewO0NPxEw637a8Zmr6k7gIi+GEu89vRuwyM9xZ7KU4pLGP4Kq+BXwE6+P+PshNjETyMftBT+PO5b073Bia8V3cT/m63E/ok8Df/HmTQI+FJG9uONE31PVDUAW8CdcctiIe7+zCe1FYGSIbro9Xr3bcf9r5wQcF7oAd/78K7j/ib/iTjT4blAd7+D+zosCyhZ6ZW0+bTSC9ojIPuAzXNfoRar6l1bWaXQp0N7uq04joY/pme5E3GmDq1U14i0S0/2IyLXAKFW9OdqxdCUi0geX7CYEHjuMRZYIuiERmYQ7kLUBd6rgPOA4r1vGGGMOYFe/dk/9cF0XvXHnbt9gScAY0xxrERhjTJyzg8XGGBPnulzXUE5Ojubn50c7DGOM6VKWLl26U1VzQ83rcokgPz+fJUtiZhwqY4zpEkSk2WsZrGvIGGPinCUCY4yJc5YIjDEmzkX0GIGInI67K5EPeERV7wmxzHTcsACJwE5VDWtoX2NM56mtraWoqIiqqpi+QNYAKSkp5OXlkZiY2PrCnoglAm/Y3wdxo0MWAYtF5AVVXRWwTDZuPJvTVXWTd0m2MSbGFBUVkZmZSX5+Ps3fK8lEm6pSUlJCUVERQ4cG3821eZHsGpoMrFXV9d5gZnNx9z8N9E3gH6q6CZpue2eMiTFVVVX07t3bkkCMExF69+7d5pZbJBPBQA68IUoRB96IA9w45z3F3SN3qYhcHqoiEblWRJaIyJLi4uIIhWuMaYklga6hPX+nSCaCUNEEj2fhB44GzsLd9/SnInLQTTBUdY6qFqhqQW5uyOshWrd9Jbz+c6jc3b71jTGmm4pkIijiwDs85bH/Xp+By7yiqvu88eAXEKmbPO/aAIvuc8/GmC6lpKSE8ePHM378ePr168fAgQObpmtqWr6//ZIlS7jpppta3cbxxx/f6jLhePvttzn77LM7pK7OEsmzhhYDw0VkKO7WeZfgjgkE+ifwB++OUEm4Wxf+NiLR9PBu1FRaBAMnRmQTxpjI6N27N5988gkAd911FxkZGXz/+/vv/1NXV4ffH/rnrKCggIKCgla38d5773VIrF1RxFoEqloH3Ai8CnwOPKuqK0XkehG53lvmc9xdjJYDH+FOMV0RkYB6eI2TskO9TawxJhbMmjWLW2+9lZNOOonbb7+djz76iOOPP54JEyZw/PHHs2aNu6Vw4B76XXfdxVVXXcX06dMZNmwYDzzwQFN9GRkZTctPnz6dCy+8kJEjR3LppZfSOErz/PnzGTlyJFOnTuWmm25qdc9/165dnHfeeYwdO5Zjjz2W5cuXA/DOO+80tWgmTJhAeXk527ZtY9q0aYwfP56jjjqKhQsXtlh3R4rodQSqOh93z9PAsoeDpu8F7o1kHACk9QJ/imsRGGPa7ecvrmTV1rIOrXPUgCzu/ProNq/3xRdf8Prrr+Pz+SgrK2PBggX4/X5ef/11fvSjH/Hcc88dtM7q1at56623KC8v54gjjuCGG2446Jz7jz/+mJUrVzJgwACmTJnCu+++S0FBAddddx0LFixg6NChzJw5s9X47rzzTiZMmMC8efN48803ufzyy/nkk0+YPXs2Dz74IFOmTGHv3r2kpKQwZ84cvva1r/HjH/+Y+vp6Kioq2vx5tFeXG3Su3URc95AlAmO6jYsuugifzwdAaWkpV1xxBV9++SUiQm1tbch1zjrrLJKTk0lOTqZPnz5s376dvLy8A5aZPHlyU9n48eMpLCwkIyODYcOGNZ2fP3PmTObMmdNifIsWLWpKRieffDIlJSWUlpYyZcoUbr31Vi699FJmzJhBXl4ekyZN4qqrrqK2tpbzzjuP8ePHH8pH0ybxkwjAEoExHaA9e+6Rkp6e3vT6pz/9KSeddBLPP/88hYWFTJ8+PeQ6ycnJTa99Ph91dXVhLdOem3iFWkdEuOOOOzjrrLOYP38+xx57LK+//jrTpk1jwYIF/Otf/+Kyyy7jtttu4/LLQ55R3+Hia6whSwTGdFulpaUMHOguVXrsscc6vP6RI0eyfv16CgsLAXjmmWdaXWfatGk89dRTgDv2kJOTQ1ZWFuvWrWPMmDHcfvvtFBQUsHr1ajZu3EifPn245ppruPrqq1m2bFmHv4fmxFeLICsP9m6HuhrwJ0U7GmNMB/rBD37AFVdcwX333cfJJ5/c4fWnpqby0EMPcfrpp5OTk8PkyZNbXeeuu+7iyiuvZOzYsaSlpfH4448DcP/99/PWW2/h8/kYNWoUZ5xxBnPnzuXee+8lMTGRjIwMnnjiiQ5/D83pcvcsLigo0HbfmGbZk/DCjfC9T6FnfofGZUx39vnnn3PkkUdGO4yo27t3LxkZGagq3/nOdxg+fDi33HJLtMM6SKi/l4gsVdWQ59HGX9cQWPeQMaZd/vSnPzF+/HhGjx5NaWkp1113XbRD6hDx1TXUlAjsWgJjTNvdcsstMdkCOFTx1SLI8sa8K93c8nLGGBNH4isRJKVBWm/rGjLGmADxlQjAtQpsmAljjGkSf4mgxyBrERhjTIA4TAR5drDYmC5m+vTpvPrqqweU3X///Xz7299ucZ3GU83PPPNM9uzZc9Ayd911F7Nnz25x2/PmzWPVqqY77PKzn/2M119/vQ3RhxZLw1XHYSIYCNWlUFUa7UiMMWGaOXMmc+fOPaBs7ty5YQ38Bm7U0Ozs7HZtOzgR/Nd//Rennnpqu+qKVXGYCOxaAmO6mgsvvJCXXnqJ6upqAAoLC9m6dStTp07lhhtuoKCggNGjR3PnnXeGXD8/P5+dO3cCcPfdd3PEEUdw6qmnNg1VDe4agUmTJjFu3DguuOACKioqeO+993jhhRe47bbbGD9+POvWrWPWrFn8/e9/B+CNN95gwoQJjBkzhquuuqopvvz8fO68804mTpzImDFjWL16dYvvL9rDVcfXdQQAvQ5zzzu/hL6xM3iWMV3Gy3fAV591bJ39xsAZ9zQ7u3fv3kyePJlXXnmFc889l7lz53LxxRcjItx999306tWL+vp6TjnlFJYvX87YsWND1rN06VLmzp3Lxx9/TF1dHRMnTuToo48GYMaMGVxzzTUA/OQnP+HPf/4z3/3udznnnHM4++yzufDCCw+oq6qqilmzZvHGG28wYsQILr/8cv74xz9y8803A5CTk8OyZct46KGHmD17No888kiz7y/aw1XHX4sg9wiQBNixqvVljTExI7B7KLBb6Nlnn2XixIlMmDCBlStXHtCNE2zhwoWcf/75pKWlkZWVxTnnnNM0b8WKFZxwwgmMGTOGp556ipUrV7YYz5o1axg6dCgjRrjbrF9xxRUsWLCgaf6MGTMAOProo5sGqmvOokWLuOyyy4DQw1U/8MAD7NmzB7/fz6RJk3j00Ue56667+Oyzz8jMzGyx7nDEX4sgMRV6DbNEYEx7tbDnHknnnXcet956K8uWLaOyspKJEyeyYcMGZs+ezeLFi+nZsyezZs2iqqqqxXpEJGT5rFmzmDdvHuPGjeOxxx7j7bffbrGe1sZpaxzKurmhrlurqzOHq46/FgFAn1Gw3RKBMV1JRkYG06dP56qrrmpqDZSVlZGenk6PHj3Yvn07L7/8cot1TJs2jeeff57KykrKy8t58cUXm+aVl5fTv39/amtrm4aOBsjMzKS8vPygukaOHElhYSFr164F4Mknn+TEE09s13uL9nDV8dciAJcIPn8RaitdC8EY0yXMnDmTGTNmNHURjRs3jgkTJjB69GiGDRvGlClTWlx/4sSJXHzxxYwfP54hQ4ZwwgknNM37xS9+wTHHHMOQIUMYM2ZM04//JZdcwjXXXMMDDzzQdJAYICUlhUcffZSLLrqIuro6Jk2axPXXX9+u9xXt4arjaxjqRqv+Cc9eDte+DQMmdEhcxnRnNgx112LDUIejzyj3vOPz6MZhjDExID4TQa9h4EuG7S2fFWCMMfEgPhNBgs+dRmotAmPC1tW6keNVe/5O8ZkIwHUP2SmkxoQlJSWFkpISSwYxTlUpKSkhJSWlTevF51lDAH1HwfK5ULEL0npFOxpjYlpeXh5FRUUUFxdHOxTTipSUFPLy8tq0TkQTgYicDvwO8AGPqOo9QfOnA/8ENnhF/1DV/4pkTE36eMNLbF8BQ6d1yiaN6aoSExMZOnRotMMwERKxRCAiPuBB4DSgCFgsIi+oanB/zEJV7fyxWPOOBgQ2vm+JwBgT1yJ5jGAysFZV16tqDTAXODeC22tRaWUtn2zeQ1VtvStI7Qn9joKNi6IVkjHGxIRIJoKBQOBd4ou8smDHicinIvKyiERsONAFXxRz3oPvsnlXwEh9Q6bC5o+grjpSmzXGmJgXyUQQamSn4FMOlgFDVHUc8HtgXsiKRK4VkSUisqS9B6syU1wvWFlVwOBP+VOhrgq2HPpYHcYY01VFMhEUAYMCpvOArYELqGqZqu71Xs8HEkUkJ7giVZ2jqgWqWpCbm9uuYDJTEgEor6rdXzjkePdcaN1Dxpj4FclEsBgYLiJDRSQJuAR4IXABEekn3piwIjLZi6ckEsFkhWoRpPWCvnacwBgT3yJ21pCq1onIjcCruNNH/6KqK0Xkem/+w8CFwA0iUgdUApdohK5YyUoN0SIA1z209HGoqwF/UiQ2bYwxMS2i1xF43T3zg8oeDnj9B+APkYyhUeMxgvKqoBtEDJkCHz4MW5bCkOM6IxRjjIkpcTPERGqiD1+CHNwiGDoNfEmw+qXoBGaMMVEWN4lARMhM8R/cIkjNhsNOgZXzoKEhGqEZY0xUxU0iAEInAoDR50NZEWw5xBveGGNMFxRfiSA58eCuIYAjznD3J1j5fOcHZYwxURZfiSDFf+Dpo41SsuDwU617yBgTl+IqEWSlJlJWGaJFAK57qHwrbP6gc4Myxpgoi6tE0OwxAnDdQ0mZsOzJzg3KGGOiLK4SQVZKM8cIAJIzYOxFsPIfULm7cwMzxpgoiqtEkJniZ291XfO32zv6SjcI3adzOzcwY4yJorhLBA0K+2rqQy/QfywMPBqWPAp2b1ZjTJyIs0TQzHhDgY6+EnaugY3vdlJUxhgTXXGVCLKaEkEzB4wBjroA0nrDwt90UlTGGBNdcZUImm5O09wppABJaTDlZlj3JmyyU0mNMd1fXCaCFlsEAJOuhvRceOuXnRCVMcZEV5wlAtc1VNbSMQKApHTXKtjwjt29zBjT7cVVIsgKt0UAUHAVZOXB/NugvpXEYYwxXVhcJYLMcA4WN0pKg7Nmw45V8H6n3DvHGGOiIq4SQUpiAv5QN6dpzhFnwJFfh7d/Dbs2RDY4Y4yJkrhKBCJCVmpieC2CRmf8D/gSYd4N0NDMhWjGGNOFxVUigMaB59rQ5581AM6cDZveh0X3RS4wY4yJkrhMBCHvSdCSsd9wF5q9fQ8U2V3MjDHdS/wlgubuUtYSETjrPtc6mPtN2LMpMsEZY0wUxF8iaOmeBC1JzYZvPgu1VfDUN6CqtMNjM8aYaIjDRNDGg8WB+hwJFz8BJV/C0xdD9d6ODc4YY6IgDhOBv/Uri1sybDrM+BNs/gie/gbU7Ouw2IwxJhriLhFkpSayt7qOhoZDuN/AUTNgxhx3JtHTF0NNRccFaIwxnSyiiUBETheRNSKyVkTuaGG5SSJSLyIXRjIecMNMqMK+mnZ2DzUacyGcP8fdt+BvlgyMMV1XxBKBiPiAB4EzgFHATBEZ1cxyvwZejVQsgcIegTQcYy+C8x6GDQvhyfOgfPuh12mMMZ0ski2CycBaVV2vqjXAXODcEMt9F3gO2BHBWJqEPQJpuMZdDBc9Bl99BnOmw+bFHVOvMcZ0kkgmgoHA5oDpIq+siYgMBM4HHm6pIhG5VkSWiMiS4uLiQwoqO9Ulgt37OnBE0dHnwdWvgc8Pf/kavPUrG7HUGNNlRDIRSIiy4CO09wO3q2qLg/io6hxVLVDVgtzc3EMKKjczGYCde6sPqZ6D9BsD1y2EMRfBO/fAn0+D4i86dhvGGBMBkUwERcCggOk8YGvQMgXAXBEpBC4EHhKR8yIYU1MiKC7v4EQA7qKzGf8L33gCdm+E/z0B3vu9tQ6MMTEtkolgMTBcRIaKSBJwCfBC4AKqOlRV81U1H/g78G1VnRfBmOiRmkiiTyju6BZBoFHnwrc/gGEnwWs/gYenunsg6yGcsmqMMRESsUSgqnXAjbizgT4HnlXVlSJyvYhcH6nttkZEyMlIZmckWgSBMvvCzL/BJU9DbQU8eT48egasf9sSgjEmpvgjWbmqzgfmB5WFPDCsqrMiGUug3MzkyLYIGonAyLPgsFPg4ydh4X3wxLkw6FiYfrtrMUioQynGGNN54u7KYoDcjOTIHCNoTmIKTL4GbvrY3dtgzybXQnjkFPj8RajvgGsajDGmneIzEWR2ciJo1JgQvvcJnH0/7NsJz3wL7j8K3vxvd4DZGGM6WVwmgpyMZEr21RzaeEOHwp8MBVfCd5e5Ywj9xsCC2fC7ca7raOljsPfQrpcwxphwRfQYQazKzUymvkHZXVFD74zk6AXi87tjCCPPgj2b4eO/wvK58OL34KVbYPDxMOocOPIcyOofvTiNMd1a3CYCgOK91dFNBIGyB8FJP4Tpd8D2FbDqBXf84OUfwMu3w6DJbgjsIVMgbxIkpUU7YmNMNxGXiSAnY/9FZSP7RTmYYCKuq6jfGDj5x+7q5FXzYM18WHAv6K8hIRHyClxSyJ8Cg46BpPRoR26M6aLiMhFE9OrijpY7Ak78gXtUlcKmD2HjIihcBIt+CwtnQ4IfBkx0SWHIVBgwHtJzoh25MaaLsETQlaT0gBH/zz0Aqsth84cuKRS+64azWPRbNy+9D/Qdvf/RZxTkjnRnLhljTIC4TATpST5SE30dP/BcZ0vOhMNPdQ9wt80sWuKOMWxf6R6LH4G6KjdfEqD34V5iaEwSo6DHYEiIyxPIjDHEaSIQEXIyk7pei6A1Sekw7ET3aNRQD7vWe8lhlUsOWz+Glc/vX8afAtlDoNdQ6DkUeuYHvB7iTnc1xnRbcZkIwLu6uKu3CMKR4IOc4e4x+vz95dXlsGM17FgJJWth1wbYXejutla7L6ACgcz+kDUAegyErDzveSD0yHPlGX3ddowxXVL8JoLMZDbs3Nf6gt1VciYMmuQegVRhX7FLCrs2wO4NbkiM0iLXovjy324QvUCSAGk5kNEH0nNdYsjIdccpGsvSekFqL0jt6bZtYywZEzPiOhF8tGFXtMOIPSLuxzujj7t2IZgqVO6Gsi1QugXKiqBsG+zb4a6G3rcDSta558ZjE8ESEl1CSPMSQ2ovSOu5P1EEJo3G12m9rIvKmAiJ20SQk5HM7opaausbSPTZgdKwibgf5bRe7lqH5qhCdZmXHIqhchdU7HJJ5IDXu13rY+vHrry55AGQmAbJWZCc4Y6HJGV6rzP2Pwe+Ts70lmssy9y/bmK6HSA3xhO3iaDxFNKSvTX062GnVHY4EXe6a0oPyDk8/PVqKlxCqNztJYvA17tdcqneCzV73VlSZVvd68ay4G6r5gP0EkKaO1juTw7xnBy63BeqPAX8SaHr8iUFLNNYFrdfPROD4va/MTfg6mJLBDEkKc09euS1b/2GepcgmpJDuZtuTBTV5QHzvOXqa1xLpK7ae65xF+81TXvP9dX7Xx8q8YWfaHxJ7qLBBJ/38LuH+ALKg54lYLkEv2v9BE5LUF2BdUsz5U3rBtUVcl2fHQfqQuI2ETT++G8trWRMXo8oR2M6TIIPUrLcI1JUg5JHdctJ44DnwPLA5YOTUTVU7HRJqb7aJbiGemioA/WeG+oOLG+oA2Lo7nehks0BSaaZ5CTiko0kAAGvm8qlmfLA5SWMesKtK6i+VpfHey0HPsPBZa3O48B5/ce54WU6WNwmgsG93KBtm3eF25VgjEdk/958rGloaDlRNM0LKm+oD1qvztUVcr26VtYNtc1wY6rzbuWqoA3eQw98bggubwhanmbKg+oKuY1w6wpavrNMuTl6iUBE0oFKVW0QkRHASOBlVa3t8Ig6SY/URDJT/JYITPeSkAAkgC8x2pHEl+BkhQYkm8ZEEVwWah4tr5cYmVGHw20RLABOEJGewBvAEuBi4NKIRNUJRITBvdLYZInAGHOoRFzXVhcV7vlzoqoVwAzg96p6PjAqcmF1DksExhjThkQgIsfhWgD/8sq6/PGFQb3S2Ly7Mnq3rDTGmBgQbiK4Gfgh8LyqrhSRYcBbEYuqkwzqlUZNXQM7utvgc8YY0wZh7dWr6jvAOwAikgDsVNWbIhlYZ2g8c2jTrgq7lsAYE7fCahGIyNMikuWdPbQKWCMit0U2tMgLTATGGBOvwu0aGqWqZcB5wHxgMHBZayuJyOkiskZE1orIHSHmnysiy0XkExFZIiJT2xL8oRqYnYqIJQJjTHwLNxEkikgiLhH807t+oMUjrCLiAx4EzsCdYTRTRILPNHoDGKeq44GrgEfCD/3QJfkTGNAj1a4lMMbEtXATwf8ChUA6sEBEhgBlrawzGVirqutVtQaYC5wbuICq7lVtumoinShcHz+oV6q1CIwxcS2sRKCqD6jqQFU9U52NwEmtrDYQ2BwwXeSVHUBEzheR1bjTUq8KVZGIXOt1HS0pLi4OJ+Sw2bUExph4F+7B4h4icl/jj7GI/Aa3B9/iaiHKDtrjV9XnVXUkrtvpF6EqUtU5qlqgqgW5ubnhhBy2wb3SKC6vprKmvkPrNcaYriLcrqG/AOXAN7xHGfBoK+sUAYMCpvOArc0trKoLgMNEJCfMmDrEoMbB53Zbq8AYE5/CTQSHqeqdXn//elX9OTCslXUWA8NFZKiIJAGXAC8ELiAih4u4sVZFZCKQBJS07S0cmsZTSAvj+f7Fxpi4Fm4iqAw8tVNEpgCVLa2gqnXAjcCrwOfAs95VydeLyPXeYhcAK0TkE9wZRhcHHDzuFMP7ZgLwxfbyztysMcbEjHDHC7oeeEJEGu/gshu4orWVVHU+7rqDwLKHA17/Gvh1mDFEREayn7yeqaz+yhKBMSY+hTvExKfAOBHJ8qbLRORmYHkEY+s0I/tlssYSgTEmToXbNQS4BOBdYQxwawTiiYqR/bJYv3Mf1XV25pAxJv60KREE6TZ3pj6iXyb1DcraHXujHYoxxnS6Q0kE3WYQ/5H93AFj6x4yxsSjFo8RiEg5oX/wBUiNSERRkJ+TTpIvwRKBMSYutZgIVDWzswKJpkRfAof1ybAzh4wxcelQuoa6lSP7ZbL6q9bG0TPGmO7HEoHniH6ZbC+rZk9FTbRDMcaYTmWJwHOEd8DYuoeMMfHGEoFn9AB30fRnRaVRjsQYYzqXJQJPbmYyg3qlsnTj7miHYowxncoSQYCjB/dk6abddPK4d8YYE1WWCAJMHNKT4vJqina3OLCqMcZ0K5YIAkwc3BOAZZuse8gYEz8sEQQY2S+TtCQfy+w4gTEmjlgiCOD3JTAuL5tlm/ZEOxRjjOk0lgiCTBySzaptZVTU1EU7FGOM6RSWCIIcPaQn9Q3Kp5vtegJjTHywRBBk4uCeiMAH60uiHYoxxnQKSwRBstOSGJuXzYIvi6MdijHGdApLBCGcODyHTzfvobSiNtqhGGNMxFkiCGHaiFwaFBat3RntUIwxJuIsEYQwflA2mSl+Fnxh3UPGmO7PEkEIfl8CUw/PYcGXxTbukDGm27NE0IxpI3LZVlrF2h17ox2KMcZEVEQTgYicLiJrRGStiNwRYv6lIrLce7wnIuMiGU9bTBuRC8Cbq3dEORJjjImsiCUCEfEBDwJnAKOAmSIyKmixDcCJqjoW+AUwJ1LxtNXA7FTGDOzBvz7bFu1QjDEmoiLZIpgMrFXV9apaA8wFzg1cQFXfU9XGEd4+APIiGE+bnT22P8uLStlUUhHtUIwxJmIimQgGApsDpou8suZcDbwcaoaIXCsiS0RkSXFx553Jc9bY/gC89NnWTtumMcZ0tkgmAglRFvIUHBE5CZcIbg81X1XnqGqBqhbk5uZ2YIgty+uZxvhB2bz0qXUPGWO6r0gmgiJgUMB0HnDQrrWIjAUeAc5V1Zgb4Ofssf1Zta2M9cV29pAxpnuKZCJYDAwXkaEikgRcArwQuICIDAb+AVymql9EMJZ2a+we+ucn1j1kjOmeIpYIVLUOuBF4FfgceFZVV4rI9SJyvbfYz4DewEMi8omILIlUPO3Vv0cqJwzP4dklm6lvsIvLjDHdT0SvI1DV+ao6QlUPU9W7vbKHVfVh7/V/qGpPVR3vPQoiGU97fXPyYLaVVvHOF3ZNgTGm+7Eri8Nw6qi+5GQk8/SHm6IdijHGdDhLBGFI9CVwUUEeb67ewbbSymiHY4wxHcoSQZgumTSIBoW/WavAGNPNWCII05De6Zx6ZF8ef38je6vtxvbGmO7DEkEbfPukwyitrLVWgTGmW7FE0AYTB/fkuGG9eWTReqrr6qMdjjHGdAhLBG307ZMOY3tZNc8t3RLtUIwxpkNYImijqYfnMGFwNr974wsqauxYgTGm67NE0EYiwo/PPJLtZdU8snBDtMMxxphDZomgHQrye3H66H48/M46dpRXRTscY4w5JJYI2un2M0ZSU9fAfa/F5Fh5xhgTNksE7TQ0J51Zx+fzzJLNfLxpd+srGGNMjLJEcAhuPm0EfTKT+ek/V9jIpMaYLssSwSHISPbzk7NGsWJLGU++XxjtcIwxpl0sERyis8f2Z9qIXO55ZTXr7C5mxpguyBLBIRIR7r1wLCmJPm555hNq6xuiHZIxxrSJJYIO0DcrhXtmjGF5USm/sbOIjDFdjCWCDnL6Uf2ZOXkQD7+zjldWbIt2OMYYEzZLBB3ornNGM35QNrc++ylfbC+PdjjGGBMWSwQdKNnv4+FvHU16sp+rH19McXl1tEMyxphWWSLoYP16pPDI5QXsLK/hqscWs89uYmOMiXGWCCJg3KBs/vDNCazcWsoNTy2jqtbuXWCMiV2WCCLklCP7cs8FY1nwRTE3/HWp3cjGGBOzLBFE0DcKBvGrGWN4a00x1z6x1O5fYIyJSZYIImzm5MH8+oIxLPyymG898iF7KmqiHZIxxhwgoolARE4XkTUislZE7ggxf6SIvC8i1SLy/UjGEk0XTxrMQ5dOZMWWMi7443s2FIUxJqZELBGIiA94EDgDGAXMFJFRQYvtAm4CZkcqjlhx+lH9efLqyeypqOXcP7zLayu/inZIxhgDRLZFMBlYq6rrVbUGmAucG7iAqu5Q1cVAbQTjiBnHDOvNi9+dymG56Vz75FJ+89oaG77aGBN1kUwEA4HNAdNFXllcG5CdyjPXHcfFBYP4/ZtrufKxxWwvs9tdGmOiJ5KJQEKUtWv3V0SuFZElIrKkuLj4EMOKvpREH/dcMIZfnj+GjzaUcNp97/Dc0iJUrXVgjOl8kUwERcCggOk8YGt7KlLVOapaoKoFubm5HRJctIkI3zxmMC9/bxpH9MvkP//vU/7j8SXWOjDGdLpIJoLFwHARGSoiScAlwAsR3F6XNDQnnbnXHsdPzx7Fu+t2cupv3mHOgnV2AZoxptNELBGoah1wI/Aq8DnwrKquFJHrReR6ABHpJyJFwK3AT0SkSESyIhVTrPIlCFdPHcrL35tGQX5Pfjl/Nafdt4CXP9tm3UXGmIiTrvZDU1BQoEuWLIl2GBG14Iti7v7X56zZXk7BkJ7cctoIjj+sNyKhDrsYY0zrRGSpqhaEmmdXFsegaSNy+ddNU/nVjDFs2lXBpY98yAV/fI+3Vu+wFoIxpsNZiyDGVdXW839Li3j47XVs2VPJUQOzuGrKUM4a259kvy/a4RljuoiWWgSWCLqImroG5n28hYcXrGN98T5yMpKYOXkw3zxmMP17pEY7PGNMjLNE0I00NCjvrtvJ4+8V8sbqHSSIMH1ELhccnccpR/axVoIxJqSWEoG/s4MxhyYhQThheC4nDM9l864KnvpwE89/XMQbq3fQIzWRc8YN4Oyx/SnI74UvwQ4uG2NaZy2CbqC+QVm0difPLS3i1ZVfUV3XQO/0JE4b1Zevje7H8Yf3tpaCMXHOWgTdnC9BOHFELieOyGVvdR3vrCnm1ZVf8dLybcxdvJmMZD8njezDySNzmXJ4Dn0yU6IdsjEmhliLoBurrqvnvXUlvLriK/69ajsl+9xNcY7om8nU4TlMHZ7DMUN7kZZk+wPGdHd2sNjQ0KCs2lbGwi93smhtMYsLd1NT10CiTxiXl83RQ3o2PXpnJEc7XGNMB7NEYA5SVVvP4sJdLPpyJx8V7mLFllJq693/wrCcdI4e0pPxg7MZM7AHR/TLtGMMxnRxdozAHCQl0dd09hG4xPDZllKWFO5m6cbdvP75dv5vaREAiT5hRN9MjhrQg6PyejCqfxYj+maQmZIYzbdgjOkglggM4BLDpPxeTMrvBYCqUrS7ks+2lPLZllJWbCnl1VVf8cyS/fcaGtAjheF9MxnRN8N7zmR4nwzSk+3fypiuxL6xJiQRYVCvNAb1SuPMMf0Blxy27Knk823lfLG9nC+3l7Nm+17eX19CTV1D07oDs1MZmpNOfk4a+b3T3SPH1WVdTMbEHksEJmwiQl7PNPJ6pnHaqL5N5XX1DWzaVcEX2/fy5fZyvtyxl40l+3jhk62UVdUFrA8DeqSSn5NGXnYaA7JTGdgzlYHZ7tGvRwpJfhsH0ZjOZonAHDK/L4FhuRkMy83g9KP6HTBvT0UNG3buY2NJhfe8j8KSCt5cs4Pi8uoDlhWBPpnJLjH0TGNAdgr9slLok5lCn6xk+mQm0yczhdQka1UY05EsEZiIyk5LYsLgJCYM7nnQvOq6erbtqWLLnkr32O2et+6pZHnRHl5ZUdl0JlOgzGS/lxgOTBB9spLJyUimV3oSvdKT6JmWZC0MY8JgicBETbLfR35OOvk56SHnNzQouytq2FFe7R5lVewor6a4vJod5VXsKKvm40172FFeRVVtQ8g6MpP99ExPomd6Er295NArPTFo2s3PSkmkR2qiJQ8TdywRmJiVkCD0zkimd0YyR/ZvfjlVpayqjuLyKkr21rBrXw27KmrYva+Gkn3ueVdFLTvKq1jzVTkl+6qbTRwAKYkJTUkhKzWRrBR/wOvGcn/QMomkJ/tIT/aT7E+wu8mZLsUSgenyRIQeqe5H+fA+4a1TWVN/QLLYU1FDWWUtpZW1lFXVBbyuZefeGtYV76OsqpayyloaWrkG058gpCf7yUj2NyWHjGQ/6Ul+77Ur27/MgWXpSX5SE32kJCWQmugjNdGH32etFBM5lghMXEpN8jEwyZ2t1Baqyr6aepckvEdj8thbVcu+mnr2Vtexr7qu6XlfdT3lVXVsL6tiX/X++XWtZZQASb4EUhITSE1yiSE1yU9qqOlEHylJPtIS/aR6iSQl0Udakp+UxARSEn0k+xNI8ieQ7HevkxMDXvsTLOnEIUsExrSBiJDh7cm3NYkEUlWq6xqaEsXe6jr21bjkUVlTT0VNPZW19VR5zxU19VTV1lMZPF1bz+59tVTV7l+nsrb+gOs62sqXIE1JIdnvIzkxgSTfwQmjcV5yUFJJ8vlI8ieQ6BPvufEhJDW+9gdN+xJI8kvAsm6biV6ZP0Gsuy2CLBEYEwUiQoq3t947o+Prr29QlxQCEkZFjUsQ1XX1VNc2UN34uq6B6lr37OYHlge8Dli3vKpuf12N87w62tLSaYskL5kk+gMShc9LFL4EknwBicSfQGKC4EtwZb4Ewe8T/AmC30ss/oQE/D5vmQTB500fsEzjdNM8r66meS2v4wtcLuC1z4stQYiJBGeJwJhuyJewv+XS2erqG6itV2rqG6htfNS56bqG/a9rAx41dXrgdL1SWxc0Xd/QVNY03cz6FZX11Dc0UFev1DUodfUN3rM33dBAfb1S29BAfYOGPE25s/gThAQvufhE8HmJJEHkwHkJwszJg/mPE4Z1fAwdXqMxJq75fQn4fZBK17nwT1VpUKgLSh71DUptgwYlDfe8P7G4dQLn1TYo9Q0uIe5fdn/SaVD11mmgXt38Bm+5+lAPb5mcCA0Rb4nAGBP3RASfgC/BRzyOmWinBxhjTJyLaCIQkdNFZI2IrBWRO0LMFxF5wJu/XEQmRjIeY4wxB4tYIhARH/AgcAYwCpgpIqOCFjsDGO49rgX+GKl4jDHGhBbJFsFkYK2qrlfVGmAucG7QMucCT6jzAZAtIi0MJmCMMaajRTIRDAQ2B0wXeWVtXQYRuVZElojIkuLi4g4P1Bhj4lkkE0GoqySCT9YNZxlUdY6qFqhqQW5ubocEZ4wxxolkIigCBgVM5wFb27GMMcaYCIpkIlgMDBeRoSKSBFwCvBC0zAvA5d7ZQ8cCpaq6LYIxGWOMCRKxSydUtU5EbgReBXzAX1R1pYhc781/GJgPnAmsBSqAK1urd+nSpTtFZGM7w8oBdrZz3c5iMXYMi7FjWIyHLlbiG9LcDFGN3hgbnU1ElqhqQbTjaInF2DEsxo5hMR66WI8P7MpiY4yJe5YIjDEmzsVbIpgT7QDCYDF2DIuxY1iMhy7W44uvYwTGGGMOFm8tAmOMMUEsERhjTJyLm0TQ2pDY0SAig0TkLRH5XERWisj3vPJeIvJvEfnSe+4Z5Th9IvKxiLwUo/Fli8jfRWS191keF4Mx3uL9jVeIyN9EJCXaMYrIX0Rkh4isCChrNiYR+aH3/VkjIl+LYoz3en/r5SLyvIhkx1qMAfO+LyIqIjnRjLE1cZEIwhwSOxrqgP9U1SOBY4HveHHdAbyhqsOBN7zpaPoe8HnAdKzF9zvgFVUdCYzDxRozMYrIQOAmoEBVj8JdYHlJDMT4GHB6UFnImLz/y0uA0d46D3nfq2jE+G/gKFUdC3wB/DAGY0REBgGnAZsCyqIVY4viIhEQ3pDYnU5Vt6nqMu91Oe4HbCAutse9xR4HzotKgICI5AFnAY8EFMdSfFnANODPAKpao6p7iKEYPX4gVUT8QBpuTK2oxqiqC4BdQcXNxXQuMFdVq1V1A240gMnRiFFVX1PVOm/yA9wYZTEVo+e3wA84cCDNqMTYmnhJBGENdx1NIpIPTAA+BPo2jrnkPfeJYmj34/6ZGwLKYim+YUAx8KjXffWIiKTHUoyqugWYjdsz3IYbU+u1WIoxQHMxxep36CrgZe91zMQoIucAW1T106BZMRNjoHhJBGENdx0tIpIBPAfcrKpl0Y6nkYicDexQ1aXRjqUFfmAi8EdVnQDsI/pdVQfw+tnPBYYCA4B0EflWdKNqs5j7DonIj3Hdq081FoVYrNNjFJE04MfAz0LNDlEW9d+ieEkEMTvctYgk4pLAU6r6D694e+Od2rznHVEKbwpwjogU4rrTThaRv8ZQfOD+tkWq+qE3/XdcYoilGE8FNqhqsarWAv8Ajo+xGBs1F1NMfYdE5ArgbOBS3X8xVKzEeBgu6X/qfXfygGUi0o/YifEA8ZIIwhkSu9OJiOD6tj9X1fsCZr0AXOG9vgL4Z2fHBqCqP1TVPFXNx31mb6rqt2IlPgBV/QrYLCJHeEWnAKuIoRhxXULHikia9zc/BXc8KJZibNRcTC8Al4hIsogMxd1n/KMoxIeInA7cDpyjqhUBs2IiRlX9TFX7qGq+990pAiZ6/6sxEeNBVDUuHrjhrr8A1gE/jnY8XkxTcc3C5cAn3uNMoDfujI0vvedeMRDrdOAl73VMxQeMB5Z4n+M8oGcMxvhzYDWwAngSSI52jMDfcMcsanE/Vle3FBOuu2MdsAY4I4oxrsX1szd+Zx6OtRiD5hcCOdGMsbWHDTFhjDFxLl66howxxjTDEoExxsQ5SwTGGBPnLBEYY0ycs0RgjDFxzhKBMR4RqReRTwIeHXaFsojkhxqd0phY4I92AMbEkEpVHR/tIIzpbNYiMKYVIlIoIr8WkY+8x+Fe+RARecMbF/8NERnslff1xsn/1Hsc71XlE5E/efcleE1EUr3lbxKRVV49c6P0Nk0cs0RgzH6pQV1DFwfMK1PVycAfcCOy4r1+Qt24+E8BD3jlDwDvqOo43LhHK73y4cCDqjoa2ANc4JXfAUzw6rk+Mm/NmObZlcXGeERkr6pmhCgvBE5W1fXeIIFfqWpvEdkJ9FfVWq98m6rmiEgxkKeq1QF15AP/VnfDF0TkdiBRVf9bRF4B9uKGx5inqnsj/FaNOYC1CIwJjzbzurllQqkOeF3P/mN0Z+HuoHc0sNS7eY0xncYSgTHhuTjg+X3v9Xu4UVkBLgUWea/fAG6Apvs9ZzVXqYgkAINU9S3cDYCygYNaJcZEku15GLNfqoh8EjD9iqo2nkKaLCIf4naeZnplNwF/EZHbcHdJu9Ir/x4wR0Suxu3534AbnTIUH/BXEemBu2nJb9XdatOYTmPHCIxphXeMoEBVd0Y7FmMiwbqGjDEmzlmLwBhj4py1CIwxJs5ZIjDGmDhnicAYY+KcJQJjjIlzlgiMMSbO/X8FHYHYf0oJkQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_count_tfidf, label='Training loss')\n",
    "plt.plot(dev_loss_count_tfidf, label='Validation loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.title('Training loss vs Validation loss (BOW - TFIDF)')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.855\n",
      "Precision: 0.8585858585858586\n",
      "Recall: 0.85\n",
      "F1-Score: 0.8542713567839195\n"
     ]
    }
   ],
   "source": [
    "preds_te_count = predict_class(test_tf_idf, weight)\n",
    "\n",
    "print('Accuracy:', accuracy_score(test_labels,preds_te_count))\n",
    "print('Precision:', precision_score(test_labels,preds_te_count))\n",
    "print('Recall:', recall_score(test_labels,preds_te_count))\n",
    "print('F1-Score:', f1_score(test_labels,preds_te_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 negative and positive Words from BOW-TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('worst',)\n",
      "('boring',)\n",
      "('unfortunately',)\n",
      "('supposed',)\n",
      "('waste',)\n",
      "('ridiculous',)\n",
      "('awful',)\n",
      "('poor',)\n",
      "('fails',)\n",
      "('minute',)\n"
     ]
    }
   ],
   "source": [
    "top_negative_words = weight_tfidf.argsort()[:10]\n",
    "for i in top_negative_words:\n",
    "    print(vocab_word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('hilarious',)\n",
      "('perfectly',)\n",
      "('t',)\n",
      "('memorable',)\n",
      "('terrific',)\n",
      "('s', 'life')\n",
      "('overall',)\n",
      "('very', 'well')\n",
      "('excellent',)\n",
      "('definitely',)\n"
     ]
    }
   ],
   "source": [
    "top_positive_words = weight_tfidf.argsort()[::-1][:10]\n",
    "for i in top_positive_words:\n",
    "    print(vocab_word[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the top +ve and -ve words seems to be more accurate compared to the BOW-Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now repeat the training and evaluation process for BOW-tfidf, BOCN-count, BOCN-tfidf, BOW+BOCN including hyperparameter tuning for each model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOCN - Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate hyperparameters using logspace\n",
    "lr_c = np.logspace(-7, -4, num = 5)\n",
    "alpha_c = np.logspace(-6, -3, num = 5)\n",
    "\n",
    "# lr_c = np.linspace(0.000001, 0.0001, num = 5,endpoint=False)\n",
    "# alpha_c = np.logspace(-5, -1, num = 5,endpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e-07, 5.62341325e-07, 3.16227766e-06, 1.77827941e-05,\n",
       "       1.00000000e-04])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper Parameters -- Learning Rate - 1e-07  Regularization Parameter- 1e-06\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1e-07  Regularization Parameter- 5.623413251903491e-06\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1e-07  Regularization Parameter- 3.1622776601683795e-05\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1e-07  Regularization Parameter- 0.00017782794100389227\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1e-07  Regularization Parameter- 0.001\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 5.62341325190349e-07  Regularization Parameter- 1e-06\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 5.62341325190349e-07  Regularization Parameter- 5.623413251903491e-06\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 5.62341325190349e-07  Regularization Parameter- 3.1622776601683795e-05\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 5.62341325190349e-07  Regularization Parameter- 0.00017782794100389227\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 5.62341325190349e-07  Regularization Parameter- 0.001\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 3.162277660168379e-06  Regularization Parameter- 1e-06\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 3.162277660168379e-06  Regularization Parameter- 5.623413251903491e-06\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 3.162277660168379e-06  Regularization Parameter- 3.1622776601683795e-05\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 3.162277660168379e-06  Regularization Parameter- 0.00017782794100389227\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 3.162277660168379e-06  Regularization Parameter- 0.001\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1.778279410038923e-05  Regularization Parameter- 1e-06\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1.778279410038923e-05  Regularization Parameter- 5.623413251903491e-06\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1.778279410038923e-05  Regularization Parameter- 3.1622776601683795e-05\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1.778279410038923e-05  Regularization Parameter- 0.00017782794100389227\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1.778279410038923e-05  Regularization Parameter- 0.001\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.0001  Regularization Parameter- 1e-06\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.0001  Regularization Parameter- 5.623413251903491e-06\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.0001  Regularization Parameter- 3.1622776601683795e-05\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.0001  Regularization Parameter- 0.00017782794100389227\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.0001  Regularization Parameter- 0.001\n"
     ]
    }
   ],
   "source": [
    "#hyper paramater tuning for BOCN count\n",
    "train_loss_history =[]\n",
    "validation_loss_history=[]\n",
    "hyperparam_history=[]\n",
    "#\n",
    "for lr_i in lr_c:\n",
    "    for alpha_i in alpha_c:\n",
    "        print('\\nHyper Parameters -- Learning Rate -',lr_i,' Regularization Parameter-',alpha_i)\n",
    "        weight, train_loss_count, dev_loss_count = SGD(X_tr=train_char_count_vector,\n",
    "                                             Y_tr=train_labels,\n",
    "                                             X_dev=dev_char_count_vector,\n",
    "                                             Y_dev=dev_labels,\n",
    "                                             lr=lr_i,\n",
    "                                             alpha=alpha_i,\n",
    "                                             tolerance=0.00001,\n",
    "                                             epochs=100,\n",
    "                                             print_progress=False)\n",
    "        if len(train_loss_count) > 5:\n",
    "            train_loss_history.append(train_loss_count[-1])\n",
    "            validation_loss_history.append(dev_loss_count[-1])\n",
    "            hyperparam_history.append([lr_i,alpha_i])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOCN\n",
      "Best Training loss - 0.545651012297725,best_validation_loss-0.5882516831593606,\n",
      "best_hyperparams-[3.162277660168379e-06, 1e-06]\n"
     ]
    }
   ],
   "source": [
    "#get the best hyper parameters\n",
    "temp = np.array(train_loss_history)\n",
    "train_loss_history_sortidx = np.argsort(temp)\n",
    "\n",
    "best_train_loss_BOCN = temp[train_loss_history_sortidx[0]]\n",
    "best_validation_loss_BOCN = validation_loss_history[train_loss_history_sortidx[0]]\n",
    "best_hyperparams_BOCN=hyperparam_history[train_loss_history_sortidx[0]]\n",
    "\n",
    "print(\"BOCN\")\n",
    "print(f'Best Training loss - {best_train_loss_BOCN},best_validation_loss-{best_validation_loss_BOCN},\\nbest_hyperparams-{best_hyperparams_BOCN}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:          0 | Training loss: 0.6542845694411505 | Validation loss: 0.6657195350794239\n",
      "Epoch:          1 | Training loss: 0.6230437026470058 | Validation loss: 0.6372112595440422\n",
      "Epoch:          2 | Training loss: 0.5978921883310198 | Validation loss: 0.6188192740618566\n",
      "Epoch:          3 | Training loss: 0.5837228648604418 | Validation loss: 0.610550383736873\n",
      "Epoch:          4 | Training loss: 0.5546239444480642 | Validation loss: 0.5889601019749006\n",
      "Epoch:          5 | Training loss: 0.545651012297725 | Validation loss: 0.5882516831593606\n"
     ]
    }
   ],
   "source": [
    "weight_c, train_loss_count_c, dev_loss_count_c = SGD(X_tr=train_char_count_vector,\n",
    "                                             Y_tr=train_labels,\n",
    "                                             X_dev=dev_char_count_vector,\n",
    "                                             Y_dev=dev_labels,\n",
    "                                             lr=best_hyperparams_BOCN[0],\n",
    "                                             alpha=best_hyperparams_BOCN[1],\n",
    "                                             tolerance=1.0e-06,\n",
    "                                             epochs=100,\n",
    "                                             print_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_count_c, label='Training loss')\n",
    "plt.plot(dev_loss_count_c, label='Validation loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.title('Training loss vs Validation loss (BOCN-Count)')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7575\n",
      "Precision: 0.8601398601398601\n",
      "Recall: 0.615\n",
      "F1-Score: 0.7172011661807579\n"
     ]
    }
   ],
   "source": [
    "preds_te_count = predict_class(test_char_count_vector, weight_c)\n",
    "\n",
    "print('Accuracy:', accuracy_score(test_labels,preds_te_count))\n",
    "print('Precision:', precision_score(test_labels,preds_te_count))\n",
    "print('Recall:', recall_score(test_labels,preds_te_count))\n",
    "print('F1-Score:', f1_score(test_labels,preds_te_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 negative and positive characters BOCN (raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ba\n",
      "ha\n",
      "ad\n",
      "lo\n",
      "ed\n",
      "tt\n",
      "te\n",
      "oo\n",
      "nt\n",
      "in\n"
     ]
    }
   ],
   "source": [
    "top_negative_words = weight_c.argsort()[:10]\n",
    "for i in top_negative_words:\n",
    "    print(vocab_char[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an\n",
      "nd\n",
      "and\n",
      "is\n",
      "al\n",
      "ti\n",
      "tr\n",
      "fe\n",
      "ce\n",
      "ar\n"
     ]
    }
   ],
   "source": [
    "top_positive_words = weight_c.argsort()[::-1][:10]\n",
    "for i in top_positive_words:\n",
    "    print(vocab_char[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOCN - TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_ctf = np.linspace(0.00001, 0.0001, num = 5,endpoint=False)\n",
    "alpha_ctf = np.logspace(-7, -3, num = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0e-05, 2.8e-05, 4.6e-05, 6.4e-05, 8.2e-05])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_ctf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper Parameters -- Learning Rate - 1e-05  Regularization Parameter- 1e-07\n",
      "Epoch:          0 | Training loss: 0.6923097050658443 | Validation loss: 0.691434878834223\n",
      "Epoch:          1 | Training loss: 0.6916736978092416 | Validation loss: 0.6902072141306621\n",
      "Epoch:          2 | Training loss: 0.691041310581203 | Validation loss: 0.6889905217994519\n",
      "Epoch:          3 | Training loss: 0.6904293177719095 | Validation loss: 0.6878384738718647\n",
      "Epoch:          4 | Training loss: 0.6898063584677643 | Validation loss: 0.6866520309275623\n",
      "Epoch:          5 | Training loss: 0.6891657728388537 | Validation loss: 0.6855150351283098\n",
      "Epoch:          6 | Training loss: 0.6885489178259216 | Validation loss: 0.6844006351054539\n",
      "Epoch:          7 | Training loss: 0.6879275119448535 | Validation loss: 0.6831700715041151\n",
      "Epoch:          8 | Training loss: 0.6873171366132588 | Validation loss: 0.6820315145289538\n",
      "Epoch:          9 | Training loss: 0.6866864582322433 | Validation loss: 0.6809531801897143\n",
      "Epoch:         10 | Training loss: 0.6862197228097303 | Validation loss: 0.6797612973997704\n",
      "Epoch:         11 | Training loss: 0.6854641319524455 | Validation loss: 0.6787645681967001\n",
      "Epoch:         12 | Training loss: 0.6848652056186443 | Validation loss: 0.6776410314056708\n",
      "Epoch:         13 | Training loss: 0.6843401293405951 | Validation loss: 0.6764415696966785\n",
      "Epoch:         14 | Training loss: 0.6836434450614824 | Validation loss: 0.675696076464408\n",
      "Epoch:         15 | Training loss: 0.6830407045632522 | Validation loss: 0.6746179936356802\n",
      "Epoch:         16 | Training loss: 0.6825403661660226 | Validation loss: 0.6732300449948206\n",
      "Epoch:         17 | Training loss: 0.681876258372899 | Validation loss: 0.6723268981748405\n",
      "Epoch:         18 | Training loss: 0.681249970079766 | Validation loss: 0.6715401405834323\n",
      "Epoch:         19 | Training loss: 0.6806558030405516 | Validation loss: 0.6705914409730732\n",
      "Epoch:         20 | Training loss: 0.6800745470153491 | Validation loss: 0.6694446595296744\n",
      "Epoch:         21 | Training loss: 0.6794769141824601 | Validation loss: 0.6686046834306596\n",
      "Epoch:         22 | Training loss: 0.6788897893023201 | Validation loss: 0.6677988376257517\n",
      "Epoch:         23 | Training loss: 0.6783253158903522 | Validation loss: 0.6671754869504061\n",
      "Epoch:         24 | Training loss: 0.6778244008140156 | Validation loss: 0.6651791353379863\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1e-05  Regularization Parameter- 1e-06\n",
      "Epoch:          0 | Training loss: 0.6923097050763879 | Validation loss: 0.6914348788447668\n",
      "Epoch:          1 | Training loss: 0.691673697844029 | Validation loss: 0.6902072141654512\n",
      "Epoch:          2 | Training loss: 0.6910413106574468 | Validation loss: 0.6889905218756996\n",
      "Epoch:          3 | Training loss: 0.6904293179056733 | Validation loss: 0.6878384740056346\n",
      "Epoch:          4 | Training loss: 0.6898063586683408 | Validation loss: 0.6866520311281487\n",
      "Epoch:          5 | Training loss: 0.6891657731298699 | Validation loss: 0.6855150354193367\n",
      "Epoch:          6 | Training loss: 0.6885489182201734 | Validation loss: 0.6844006354997185\n",
      "Epoch:          7 | Training loss: 0.6879275124537159 | Validation loss: 0.6831700720129951\n",
      "Epoch:          8 | Training loss: 0.6873171372551232 | Validation loss: 0.6820315151708384\n",
      "Epoch:          9 | Training loss: 0.6866864590238947 | Validation loss: 0.6809531809813876\n",
      "Epoch:         10 | Training loss: 0.6862197237624768 | Validation loss: 0.6797612983525468\n",
      "Epoch:         11 | Training loss: 0.6854641330866821 | Validation loss: 0.6787645693309627\n",
      "Epoch:         12 | Training loss: 0.6848652069460749 | Validation loss: 0.6776410327331328\n",
      "Epoch:         13 | Training loss: 0.6843401308752639 | Validation loss: 0.6764415712313864\n",
      "Epoch:         14 | Training loss: 0.6836434468236008 | Validation loss: 0.6756960782265586\n",
      "Epoch:         15 | Training loss: 0.6830407065634412 | Validation loss: 0.6746179956359061\n",
      "Epoch:         16 | Training loss: 0.6825403684160866 | Validation loss: 0.6732300472449346\n",
      "Epoch:         17 | Training loss: 0.6818762608917499 | Validation loss: 0.6723269006937395\n",
      "Epoch:         18 | Training loss: 0.6812499728826888 | Validation loss: 0.6715401433864007\n",
      "Epoch:         19 | Training loss: 0.6806558061406246 | Validation loss: 0.670591444073195\n",
      "Epoch:         20 | Training loss: 0.6800745504254704 | Validation loss: 0.6694446629398536\n",
      "Epoch:         21 | Training loss: 0.6794769179188442 | Validation loss: 0.6686046871671014\n",
      "Epoch:         22 | Training loss: 0.6788897933793415 | Validation loss: 0.6677988417028322\n",
      "Epoch:         23 | Training loss: 0.6783253203232799 | Validation loss: 0.66717549138339\n",
      "Epoch:         24 | Training loss: 0.677824405608899 | Validation loss: 0.6651791401329571\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1e-05  Regularization Parameter- 1e-05\n",
      "Epoch:          0 | Training loss: 0.6923097051818231 | Validation loss: 0.6914348789502034\n",
      "Epoch:          1 | Training loss: 0.6916736981919024 | Validation loss: 0.6902072145133415\n",
      "Epoch:          2 | Training loss: 0.6910413114198861 | Validation loss: 0.6889905226381788\n",
      "Epoch:          3 | Training loss: 0.6904293192433114 | Validation loss: 0.6878384753433329\n",
      "Epoch:          4 | Training loss: 0.6898063606741076 | Validation loss: 0.6866520331340117\n",
      "Epoch:          5 | Training loss: 0.6891657760400322 | Validation loss: 0.6855150383296043\n",
      "Epoch:          6 | Training loss: 0.688548922162692 | Validation loss: 0.6844006394423664\n",
      "Epoch:          7 | Training loss: 0.6879275175423407 | Validation loss: 0.6831700771017956\n",
      "Epoch:          8 | Training loss: 0.6873171436737661 | Validation loss: 0.6820315215896834\n",
      "Epoch:          9 | Training loss: 0.6866864669404099 | Validation loss: 0.6809531888981213\n",
      "Epoch:         10 | Training loss: 0.6862197332899403 | Validation loss: 0.6797613078803134\n",
      "Epoch:         11 | Training loss: 0.6854641444290483 | Validation loss: 0.6787645806735906\n",
      "Epoch:         12 | Training loss: 0.6848652202203802 | Validation loss: 0.6776410460077525\n",
      "Epoch:         13 | Training loss: 0.6843401462219518 | Validation loss: 0.6764415865784642\n",
      "Epoch:         14 | Training loss: 0.6836434644447845 | Validation loss: 0.6756960958480659\n",
      "Epoch:         15 | Training loss: 0.6830407265653301 | Validation loss: 0.6746180156381628\n",
      "Epoch:         16 | Training loss: 0.6825403909167261 | Validation loss: 0.6732300697460732\n",
      "Epoch:         17 | Training loss: 0.6818762860802593 | Validation loss: 0.6723269258827301\n",
      "Epoch:         18 | Training loss: 0.6812500009119166 | Validation loss: 0.6715401714160862\n",
      "Epoch:         19 | Training loss: 0.6806558371413549 | Validation loss: 0.6705914750744126\n",
      "Epoch:         20 | Training loss: 0.6800745845266851 | Validation loss: 0.6694446970416468\n",
      "Epoch:         21 | Training loss: 0.6794769552826839 | Validation loss: 0.6686047245315189\n",
      "Epoch:         22 | Training loss: 0.6788898341495554 | Validation loss: 0.6677988824736368\n",
      "Epoch:         23 | Training loss: 0.6783253646525567 | Validation loss: 0.6671755357132299\n",
      "Epoch:         24 | Training loss: 0.6778244535577324 | Validation loss: 0.665179188082664\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1e-05  Regularization Parameter- 0.0001\n",
      "Epoch:          0 | Training loss: 0.6923097062361756 | Validation loss: 0.6914348800045693\n",
      "Epoch:          1 | Training loss: 0.6916737016706358 | Validation loss: 0.6902072179922435\n",
      "Epoch:          2 | Training loss: 0.6910413190442798 | Validation loss: 0.6889905302629703\n",
      "Epoch:          3 | Training loss: 0.6904293326196923 | Validation loss: 0.6878384887203164\n",
      "Epoch:          4 | Training loss: 0.6898063807317748 | Validation loss: 0.6866520531926423\n",
      "Epoch:          5 | Training loss: 0.6891658051416554 | Validation loss: 0.6855150674322812\n",
      "Epoch:          6 | Training loss: 0.6885489615878766 | Validation loss: 0.6844006788688444\n",
      "Epoch:          7 | Training loss: 0.6879275684285886 | Validation loss: 0.6831701279897985\n",
      "Epoch:          8 | Training loss: 0.6873172078601965 | Validation loss: 0.6820315857781347\n",
      "Epoch:          9 | Training loss: 0.6866865461055628 | Validation loss: 0.6809532680654595\n",
      "Epoch:         10 | Training loss: 0.6862198285645762 | Validation loss: 0.6797614031579786\n",
      "Epoch:         11 | Training loss: 0.6854642578527109 | Validation loss: 0.6787646940998687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:         12 | Training loss: 0.6848653529634348 | Validation loss: 0.677641178753951\n",
      "Epoch:         13 | Training loss: 0.6843402996888326 | Validation loss: 0.6764417400492438\n",
      "Epoch:         14 | Training loss: 0.6836436406566218 | Validation loss: 0.6756962720631384\n",
      "Epoch:         15 | Training loss: 0.6830409265842201 | Validation loss: 0.6746182156607327\n",
      "Epoch:         16 | Training loss: 0.6825406159231215 | Validation loss: 0.6732302947574592\n",
      "Epoch:         17 | Training loss: 0.6818765379653544 | Validation loss: 0.6723271777726376\n",
      "Epoch:         18 | Training loss: 0.6812502812041953 | Validation loss: 0.6715404517129432\n",
      "Epoch:         19 | Training loss: 0.6806561471486594 | Validation loss: 0.670591785086592\n",
      "Epoch:         20 | Training loss: 0.680074925538834 | Validation loss: 0.6694450380595811\n",
      "Epoch:         21 | Training loss: 0.6794773289210821 | Validation loss: 0.6686050981756952\n",
      "Epoch:         22 | Training loss: 0.6788902418516957 | Validation loss: 0.6677992901816858\n",
      "Epoch:         23 | Training loss: 0.6783258079453262 | Validation loss: 0.6671759790116272\n",
      "Epoch:         24 | Training loss: 0.6778249330460681 | Validation loss: 0.6651796675797358\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1e-05  Regularization Parameter- 0.001\n",
      "Epoch:          0 | Training loss: 0.6923097167797008 | Validation loss: 0.6914348905482265\n",
      "Epoch:          1 | Training loss: 0.691673736457972 | Validation loss: 0.6902072527812657\n",
      "Epoch:          2 | Training loss: 0.6910413952882155 | Validation loss: 0.6889906065108856\n",
      "Epoch:          3 | Training loss: 0.6904294663835021 | Validation loss: 0.6878386224901507\n",
      "Epoch:          4 | Training loss: 0.6898065813084486 | Validation loss: 0.6866522537789492\n",
      "Epoch:          5 | Training loss: 0.6891660961578894 | Validation loss: 0.6855153584590522\n",
      "Epoch:          6 | Training loss: 0.6885493558397295 | Validation loss: 0.6844010731336292\n",
      "Epoch:          7 | Training loss: 0.687928077291076 | Validation loss: 0.683170636869836\n",
      "Epoch:          8 | Training loss: 0.6873178497245095 | Validation loss: 0.6820322276626579\n",
      "Epoch:          9 | Training loss: 0.6866873377571051 | Validation loss: 0.6809540597388554\n",
      "Epoch:         10 | Training loss: 0.6862207813109543 | Validation loss: 0.6797623559346494\n",
      "Epoch:         11 | Training loss: 0.6854653920893634 | Validation loss: 0.6787658283626766\n",
      "Epoch:         12 | Training loss: 0.6848666803940123 | Validation loss: 0.6776425062159691\n",
      "Epoch:         13 | Training loss: 0.684341834357683 | Validation loss: 0.6764432747570811\n",
      "Epoch:         14 | Training loss: 0.683645402775047 | Validation loss: 0.6756980342139162\n",
      "Epoch:         15 | Training loss: 0.683042926773185 | Validation loss: 0.674620215886496\n",
      "Epoch:         16 | Training loss: 0.6825428659871532 | Validation loss: 0.6732325448713979\n",
      "Epoch:         17 | Training loss: 0.6818790568163965 | Validation loss: 0.6723296966718039\n",
      "Epoch:         18 | Training loss: 0.6812530841270898 | Validation loss: 0.6715432546816189\n",
      "Epoch:         19 | Training loss: 0.6806592472218295 | Validation loss: 0.6705948852085101\n",
      "Epoch:         20 | Training loss: 0.6800783356604672 | Validation loss: 0.6694484482390679\n",
      "Epoch:         21 | Training loss: 0.6794810653052319 | Validation loss: 0.6686088346176254\n",
      "Epoch:         22 | Training loss: 0.6788943188732884 | Validation loss: 0.667803367262367\n",
      "Epoch:         23 | Training loss: 0.6783302408732385 | Validation loss: 0.6671804119958188\n",
      "Epoch:         24 | Training loss: 0.6778297279296698 | Validation loss: 0.6651844625506971\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 2.8000000000000003e-05  Regularization Parameter- 1e-07\n",
      "Epoch:          0 | Training loss: 0.6911768756206623 | Validation loss: 0.6892302600501271\n",
      "Epoch:          1 | Training loss: 0.6894317452382112 | Validation loss: 0.6859391525909951\n",
      "Epoch:          2 | Training loss: 0.6876679329905059 | Validation loss: 0.682789478059807\n",
      "Epoch:          3 | Training loss: 0.6863640083746503 | Validation loss: 0.6808713630727476\n",
      "Epoch:          4 | Training loss: 0.6842526972718471 | Validation loss: 0.6765752282910193\n",
      "Epoch:          5 | Training loss: 0.6826342343146746 | Validation loss: 0.6743517486735209\n",
      "Epoch:          6 | Training loss: 0.6809244020794973 | Validation loss: 0.6714344414530597\n",
      "Epoch:          7 | Training loss: 0.6794057435260588 | Validation loss: 0.6675631509589303\n",
      "Epoch:          8 | Training loss: 0.677637592567806 | Validation loss: 0.6651681194998359\n",
      "Epoch:          9 | Training loss: 0.6760045584013277 | Validation loss: 0.6626180311894926\n",
      "Epoch:         10 | Training loss: 0.6759585382831026 | Validation loss: 0.6588111251526313\n",
      "Epoch:         11 | Training loss: 0.6727815699134192 | Validation loss: 0.6583234705675726\n",
      "Epoch:         12 | Training loss: 0.6714078060627973 | Validation loss: 0.6544960566888548\n",
      "Epoch:         13 | Training loss: 0.6704392313566504 | Validation loss: 0.6511285323955552\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 2.8000000000000003e-05  Regularization Parameter- 1e-06\n",
      "Epoch:          0 | Training loss: 0.6911768756888703 | Validation loss: 0.6892302601183387\n",
      "Epoch:          1 | Training loss: 0.6894317454893746 | Validation loss: 0.6859391528421698\n",
      "Epoch:          2 | Training loss: 0.6876679335544769 | Validation loss: 0.6827894786237961\n",
      "Epoch:          3 | Training loss: 0.6863640093851923 | Validation loss: 0.6808713640833011\n",
      "Epoch:          4 | Training loss: 0.6842526988095946 | Validation loss: 0.6765752298287928\n",
      "Epoch:          5 | Training loss: 0.6826342365237753 | Validation loss: 0.6743517508826574\n",
      "Epoch:          6 | Training loss: 0.6809244050678819 | Validation loss: 0.6714344444414913\n",
      "Epoch:          7 | Training loss: 0.6794057474002357 | Validation loss: 0.6675631548331793\n",
      "Epoch:          8 | Training loss: 0.6776375974458425 | Validation loss: 0.6651681243779454\n",
      "Epoch:          9 | Training loss: 0.6760045643946203 | Validation loss: 0.6626180371828788\n",
      "Epoch:         10 | Training loss: 0.6759585455076329 | Validation loss: 0.6588111323773379\n",
      "Epoch:         11 | Training loss: 0.6727815784566652 | Validation loss: 0.6583234791109117\n",
      "Epoch:         12 | Training loss: 0.6714078160341236 | Validation loss: 0.6544960666603477\n",
      "Epoch:         13 | Training loss: 0.6704392428677829 | Validation loss: 0.6511285439068933\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 2.8000000000000003e-05  Regularization Parameter- 1e-05\n",
      "Epoch:          0 | Training loss: 0.6911768763709497 | Validation loss: 0.6892302608004539\n",
      "Epoch:          1 | Training loss: 0.6894317480010087 | Validation loss: 0.6859391553539166\n",
      "Epoch:          2 | Training loss: 0.687667939194187 | Validation loss: 0.6827894842636867\n",
      "Epoch:          3 | Training loss: 0.6863640194906133 | Validation loss: 0.6808713741888366\n",
      "Epoch:          4 | Training loss: 0.6842527141870696 | Validation loss: 0.676575245206529\n",
      "Epoch:          5 | Training loss: 0.682634258614782 | Validation loss: 0.6743517729740227\n",
      "Epoch:          6 | Training loss: 0.6809244349517274 | Validation loss: 0.6714344743258053\n",
      "Epoch:          7 | Training loss: 0.6794057861420058 | Validation loss: 0.6675631935756704\n",
      "Epoch:          8 | Training loss: 0.6776376462262089 | Validation loss: 0.6651681731590411\n",
      "Epoch:          9 | Training loss: 0.676004624327546 | Validation loss: 0.66261809711674\n",
      "Epoch:         10 | Training loss: 0.6759586177529374 | Validation loss: 0.6588112046244052\n",
      "Epoch:         11 | Training loss: 0.672781663889126 | Validation loss: 0.6583235645443029\n",
      "Epoch:         12 | Training loss: 0.671407915747387 | Validation loss: 0.6544961663752757\n",
      "Epoch:         13 | Training loss: 0.6704393579791077 | Validation loss: 0.651128659020275\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 2.8000000000000003e-05  Regularization Parameter- 0.0001\n",
      "Epoch:          0 | Training loss: 0.6911768831917434 | Validation loss: 0.6892302676216054\n",
      "Epoch:          1 | Training loss: 0.6894317731173495 | Validation loss: 0.6859391804713835\n",
      "Epoch:          2 | Training loss: 0.6876679955912888 | Validation loss: 0.6827895406625938\n",
      "Epoch:          3 | Training loss: 0.6863641205448248 | Validation loss: 0.6808714752441921\n",
      "Epoch:          4 | Training loss: 0.6842528679618193 | Validation loss: 0.6765753989838899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:          5 | Training loss: 0.6826344795248511 | Validation loss: 0.6743519938876767\n",
      "Epoch:          6 | Training loss: 0.6809247337901828 | Validation loss: 0.6714347731689472\n",
      "Epoch:          7 | Training loss: 0.6794061735597072 | Validation loss: 0.6675635810005807\n",
      "Epoch:          8 | Training loss: 0.6776381340298742 | Validation loss: 0.6651686609699995\n",
      "Epoch:          9 | Training loss: 0.6760052236568077 | Validation loss: 0.662618696455356\n",
      "Epoch:         10 | Training loss: 0.675959340205987 | Validation loss: 0.658811927095083\n",
      "Epoch:         11 | Training loss: 0.672782518213738 | Validation loss: 0.6583244188782209\n",
      "Epoch:         12 | Training loss: 0.6714089128800269 | Validation loss: 0.6544971635245633\n",
      "Epoch:         13 | Training loss: 0.670440509092366 | Validation loss: 0.6511298101541014\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 2.8000000000000003e-05  Regularization Parameter- 0.001\n",
      "Epoch:          0 | Training loss: 0.6911769513996805 | Validation loss: 0.6892303358331213\n",
      "Epoch:          1 | Training loss: 0.6894320242807602 | Validation loss: 0.6859394316460553\n",
      "Epoch:          2 | Training loss: 0.687668559562315 | Validation loss: 0.6827901046516712\n",
      "Epoch:          3 | Training loss: 0.6863651310869597 | Validation loss: 0.6808724857977678\n",
      "Epoch:          4 | Training loss: 0.6842544057093595 | Validation loss: 0.6765769367575412\n",
      "Epoch:          5 | Training loss: 0.682636688625615 | Validation loss: 0.6743542030242896\n",
      "Epoch:          6 | Training loss: 0.6809277221748563 | Validation loss: 0.6714377616004831\n",
      "Epoch:          7 | Training loss: 0.6794100477368991 | Validation loss: 0.6675674552498603\n",
      "Epoch:          8 | Training loss: 0.6776430120667776 | Validation loss: 0.665173539079834\n",
      "Epoch:          9 | Training loss: 0.6760112169497647 | Validation loss: 0.6626246898418566\n",
      "Epoch:         10 | Training loss: 0.6759665647369333 | Validation loss: 0.6588191518023107\n",
      "Epoch:         11 | Training loss: 0.6727910614604428 | Validation loss: 0.6583329622179841\n",
      "Epoch:         12 | Training loss: 0.6714188842071648 | Validation loss: 0.6545071350181777\n",
      "Epoch:         13 | Training loss: 0.6704520202258621 | Validation loss: 0.6511413214932799\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 4.6e-05  Regularization Parameter- 1e-07\n",
      "Epoch:          0 | Training loss: 0.6900790098050679 | Validation loss: 0.6871827744568333\n",
      "Epoch:          1 | Training loss: 0.6872174250479163 | Validation loss: 0.6817711670025566\n",
      "Epoch:          2 | Training loss: 0.6843560621607965 | Validation loss: 0.6768177359618249\n",
      "Epoch:          3 | Training loss: 0.6830128733192897 | Validation loss: 0.6763484371630406\n",
      "Epoch:          4 | Training loss: 0.6790357884514501 | Validation loss: 0.6688415989723484\n",
      "Epoch:          5 | Training loss: 0.6762455022731914 | Validation loss: 0.6640660622376228\n",
      "Epoch:          6 | Training loss: 0.6735817563265669 | Validation loss: 0.6598601120989158\n",
      "Epoch:          7 | Training loss: 0.6712934753988162 | Validation loss: 0.6538210618384803\n",
      "Epoch:          8 | Training loss: 0.6684378957921223 | Validation loss: 0.6514233864062866\n",
      "Epoch:          9 | Training loss: 0.6660262696743723 | Validation loss: 0.64696418932815\n",
      "Epoch:         10 | Training loss: 0.6672305745449177 | Validation loss: 0.6402276905337101\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 4.6e-05  Regularization Parameter- 1e-06\n",
      "Epoch:          0 | Training loss: 0.690079009981679 | Validation loss: 0.6871827746334518\n",
      "Epoch:          1 | Training loss: 0.6872174257187775 | Validation loss: 0.6817711676734381\n",
      "Epoch:          2 | Training loss: 0.6843560636600912 | Validation loss: 0.6768177374611564\n",
      "Epoch:          3 | Training loss: 0.6830128760037553 | Validation loss: 0.6763484398475083\n",
      "Epoch:          4 | Training loss: 0.6790357925413432 | Validation loss: 0.6688416030622585\n",
      "Epoch:          5 | Training loss: 0.6762455081032969 | Validation loss: 0.6640660680678105\n",
      "Epoch:          6 | Training loss: 0.6735817642020699 | Validation loss: 0.6598601199745205\n",
      "Epoch:          7 | Training loss: 0.6712934855973692 | Validation loss: 0.6538210720371825\n",
      "Epoch:          8 | Training loss: 0.6684379085923691 | Validation loss: 0.6514233992066746\n",
      "Epoch:          9 | Training loss: 0.6660262853551859 | Validation loss: 0.6469642050091781\n",
      "Epoch:         10 | Training loss: 0.6672305934019336 | Validation loss: 0.6402277093912276\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 4.6e-05  Regularization Parameter- 1e-05\n",
      "Epoch:          0 | Training loss: 0.6900790117477907 | Validation loss: 0.6871827763996373\n",
      "Epoch:          1 | Training loss: 0.687217432427389 | Validation loss: 0.681771174382256\n",
      "Epoch:          2 | Training loss: 0.6843560786530373 | Validation loss: 0.6768177524544705\n",
      "Epoch:          3 | Training loss: 0.6830129028484114 | Validation loss: 0.6763484666921847\n",
      "Epoch:          4 | Training loss: 0.6790358334402748 | Validation loss: 0.6688416439613613\n",
      "Epoch:          5 | Training loss: 0.6762455664043516 | Validation loss: 0.6640661263696869\n",
      "Epoch:          6 | Training loss: 0.6735818429570987 | Validation loss: 0.6598601987305672\n",
      "Epoch:          7 | Training loss: 0.6712935875828991 | Validation loss: 0.6538211740242038\n",
      "Epoch:          8 | Training loss: 0.6684380365948358 | Validation loss: 0.6514235272105539\n",
      "Epoch:          9 | Training loss: 0.6660264421633219 | Validation loss: 0.6469643618194578\n",
      "Epoch:         10 | Training loss: 0.6672307819720914 | Validation loss: 0.6402278979664047\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 4.6e-05  Regularization Parameter- 0.0001\n",
      "Epoch:          0 | Training loss: 0.6900790294089062 | Validation loss: 0.6871827940614921\n",
      "Epoch:          1 | Training loss: 0.6872174995135037 | Validation loss: 0.6817712414704329\n",
      "Epoch:          2 | Training loss: 0.6843562285824988 | Validation loss: 0.6768179023876137\n",
      "Epoch:          3 | Training loss: 0.6830131712949731 | Validation loss: 0.6763487351389503\n",
      "Epoch:          4 | Training loss: 0.6790362424295935 | Validation loss: 0.6688420529523902\n",
      "Epoch:          5 | Training loss: 0.6762461494149024 | Validation loss: 0.6640667093884531\n",
      "Epoch:          6 | Training loss: 0.6735826305073928 | Validation loss: 0.6598609862910382\n",
      "Epoch:          7 | Training loss: 0.6712946074382058 | Validation loss: 0.6538221938944259\n",
      "Epoch:          8 | Training loss: 0.6684393166195143 | Validation loss: 0.6514248072493587\n",
      "Epoch:          9 | Training loss: 0.6660280102446969 | Validation loss: 0.646965929922269\n",
      "Epoch:         10 | Training loss: 0.6672326676736882 | Validation loss: 0.6402297837181931\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 4.6e-05  Regularization Parameter- 0.001\n",
      "Epoch:          0 | Training loss: 0.6900792060200632 | Validation loss: 0.6871829706800408\n",
      "Epoch:          1 | Training loss: 0.6872181703746628 | Validation loss: 0.6817719123522146\n",
      "Epoch:          2 | Training loss: 0.6843577278771558 | Validation loss: 0.6768194017190854\n",
      "Epoch:          3 | Training loss: 0.6830158557606887 | Validation loss: 0.6763514196067052\n",
      "Epoch:          4 | Training loss: 0.6790403323229681 | Validation loss: 0.6688461428628679\n",
      "Epoch:          5 | Training loss: 0.6762519795207376 | Validation loss: 0.6640725395764437\n",
      "Epoch:          6 | Training loss: 0.6735905060108492 | Validation loss: 0.6598688618962666\n",
      "Epoch:          7 | Training loss: 0.6713048059920353 | Validation loss: 0.6538323925974092\n",
      "Epoch:          8 | Training loss: 0.6684521168673722 | Validation loss: 0.6514376076384789\n",
      "Epoch:          9 | Training loss: 0.6660436910599014 | Validation loss: 0.646981610951835\n",
      "Epoch:         10 | Training loss: 0.6672515246915662 | Validation loss: 0.6402486412379873\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 6.4e-05  Regularization Parameter- 1e-07\n",
      "Epoch:          0 | Training loss: 0.6890082884992939 | Validation loss: 0.6852497538513705\n",
      "Epoch:          1 | Training loss: 0.6850031427855531 | Validation loss: 0.6777608405547185\n",
      "Epoch:          2 | Training loss: 0.6811454632324884 | Validation loss: 0.6710280625497193\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 6.4e-05  Regularization Parameter- 1e-06\n",
      "Epoch:          0 | Training loss: 0.6890082888353909 | Validation loss: 0.6852497541874786\n",
      "Epoch:          1 | Training loss: 0.6850031440752572 | Validation loss: 0.6777608418444507\n",
      "Epoch:          2 | Training loss: 0.6811454661038723 | Validation loss: 0.6710280654211647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper Parameters -- Learning Rate - 6.4e-05  Regularization Parameter- 1e-05\n",
      "Epoch:          0 | Training loss: 0.6890082921963608 | Validation loss: 0.6852497575485573\n",
      "Epoch:          1 | Training loss: 0.6850031569722981 | Validation loss: 0.6777608547417715\n",
      "Epoch:          2 | Training loss: 0.6811454948177134 | Validation loss: 0.6710280941356197\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 6.4e-05  Regularization Parameter- 0.0001\n",
      "Epoch:          0 | Training loss: 0.6890083258060601 | Validation loss: 0.685249791159346\n",
      "Epoch:          1 | Training loss: 0.6850032859427085 | Validation loss: 0.6777609837149818\n",
      "Epoch:          2 | Training loss: 0.681145781956124 | Validation loss: 0.6710283812801704\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 6.4e-05  Regularization Parameter- 0.001\n",
      "Epoch:          0 | Training loss: 0.6890086619030563 | Validation loss: 0.6852501272672363\n",
      "Epoch:          1 | Training loss: 0.6850045756468457 | Validation loss: 0.6777622734471157\n",
      "Epoch:          2 | Training loss: 0.6811486533403414 | Validation loss: 0.6710312527257892\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 8.2e-05  Regularization Parameter- 1e-07\n",
      "Epoch:          0 | Training loss: 0.6879443522844564 | Validation loss: 0.6833853806582519\n",
      "Epoch:          1 | Training loss: 0.6828077352825561 | Validation loss: 0.6739729299810047\n",
      "Epoch:          2 | Training loss: 0.6780452208025601 | Validation loss: 0.6654338412430537\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 8.2e-05  Regularization Parameter- 1e-06\n",
      "Epoch:          0 | Training loss: 0.6879443528298523 | Validation loss: 0.6833853812036628\n",
      "Epoch:          1 | Training loss: 0.682807737385804 | Validation loss: 0.6739729320842867\n",
      "Epoch:          2 | Training loss: 0.6780452254708268 | Validation loss: 0.6654338459114123\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 8.2e-05  Regularization Parameter- 1e-05\n",
      "Epoch:          0 | Training loss: 0.6879443582838121 | Validation loss: 0.6833853866577718\n",
      "Epoch:          1 | Training loss: 0.6828077584182822 | Validation loss: 0.6739729531171043\n",
      "Epoch:          2 | Training loss: 0.6780452721534947 | Validation loss: 0.6654338925949989\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 8.2e-05  Regularization Parameter- 0.0001\n",
      "Epoch:          0 | Training loss: 0.6879444128234098 | Validation loss: 0.6833854411988625\n",
      "Epoch:          1 | Training loss: 0.6828079687430649 | Validation loss: 0.673973163445282\n",
      "Epoch:          2 | Training loss: 0.678045738980177 | Validation loss: 0.6654343594308656\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 8.2e-05  Regularization Parameter- 0.001\n",
      "Epoch:          0 | Training loss: 0.6879449582193945 | Validation loss: 0.6833859866097776\n",
      "Epoch:          1 | Training loss: 0.6828100719909611 | Validation loss: 0.6739752667271302\n",
      "Epoch:          2 | Training loss: 0.678050407247231 | Validation loss: 0.665439027789766\n"
     ]
    }
   ],
   "source": [
    "#hyper paramater tuning for BOCN-tfidf\n",
    "train_loss_history =[]\n",
    "validation_loss_history=[]\n",
    "hyperparam_history=[]\n",
    "#\n",
    "for lr_i in lr_ctf:\n",
    "    for alpha_i in alpha_ctf:\n",
    "        print('\\nHyper Parameters -- Learning Rate -',lr_i,' Regularization Parameter-',alpha_i)\n",
    "        weight, train_loss_count, dev_loss_count = SGD(X_tr=train_char_tf_idf,\n",
    "                                             Y_tr=train_labels,\n",
    "                                             X_dev=dev_char_tf_idf,\n",
    "                                             Y_dev=dev_labels,\n",
    "                                             lr=lr_i,\n",
    "                                             alpha=alpha_i,\n",
    "                                             tolerance=0.0001,\n",
    "                                             epochs=300,\n",
    "                                             print_progress=True)\n",
    "        if len(train_loss_count) > 5:\n",
    "            train_loss_history.append(train_loss_count[-1])\n",
    "            validation_loss_history.append(dev_loss_count[-1])\n",
    "            hyperparam_history.append([lr_i,alpha_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOCN-TFIDF\n",
      "Best Training loss - 0.6672305745449177,best_validation_loss-0.6402276905337101,\n",
      "best_hyperparams-[4.6e-05, 1e-07]\n"
     ]
    }
   ],
   "source": [
    "#get the best hyper parameters\n",
    "temp = np.array(train_loss_history)\n",
    "train_loss_history_sortidx = np.argsort(temp)\n",
    "\n",
    "best_train_loss_BOCN_tfidf = temp[train_loss_history_sortidx[0]]\n",
    "best_validation_loss_BOCN_tfidf = validation_loss_history[train_loss_history_sortidx[0]]\n",
    "best_hyperparams_BOCN_tfidf=hyperparam_history[train_loss_history_sortidx[0]]\n",
    "\n",
    "print(\"BOCN-TFIDF\")\n",
    "print(f'Best Training loss - {best_train_loss_BOCN_tfidf},best_validation_loss-{best_validation_loss_BOCN_tfidf},\\nbest_hyperparams-{best_hyperparams_BOCN_tfidf}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:          0 | Training loss: 0.6900790098050679 | Validation loss: 0.6871827744568333\n",
      "Epoch:          1 | Training loss: 0.6872174250479163 | Validation loss: 0.6817711670025566\n",
      "Epoch:          2 | Training loss: 0.6843560621607965 | Validation loss: 0.6768177359618249\n",
      "Epoch:          3 | Training loss: 0.6830128733192897 | Validation loss: 0.6763484371630406\n",
      "Epoch:          4 | Training loss: 0.6790357884514501 | Validation loss: 0.6688415989723484\n",
      "Epoch:          5 | Training loss: 0.6762455022731914 | Validation loss: 0.6640660622376228\n",
      "Epoch:          6 | Training loss: 0.6735817563265669 | Validation loss: 0.6598601120989158\n",
      "Epoch:          7 | Training loss: 0.6712934753988162 | Validation loss: 0.6538210618384803\n",
      "Epoch:          8 | Training loss: 0.6684378957921223 | Validation loss: 0.6514233864062866\n",
      "Epoch:          9 | Training loss: 0.6660262696743723 | Validation loss: 0.64696418932815\n",
      "Epoch:         10 | Training loss: 0.6672305745449177 | Validation loss: 0.6402276905337101\n"
     ]
    }
   ],
   "source": [
    "#train with best hyperparameters (BOCN-TFIDF)\n",
    "weight_c, train_loss_count_c, dev_loss_count_c = SGD(X_tr=train_char_tf_idf,\n",
    "                                             Y_tr=train_labels,\n",
    "                                             X_dev=dev_char_tf_idf,\n",
    "                                             Y_dev=dev_labels,\n",
    "                                             lr=best_hyperparams_BOCN_tfidf[0],\n",
    "                                             alpha=best_hyperparams_BOCN_tfidf[1],\n",
    "                                             tolerance=0.0001,\n",
    "                                             epochs=100,\n",
    "                                             print_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABCu0lEQVR4nO3dd3wVVfr48c+TAoEkECCEFrp0aSEUARHpCAICFlQQC7ZVV13buuvq7n5d97e6LrqKLmIXF12aoAhIF7FQpSMtQCiBUEMoac/vjxngEm9CgNxMyvN+vfLKvWfmzjz33mSeOefMnCOqijHGGJNdkNcBGGOMKZwsQRhjjPHLEoQxxhi/LEEYY4zxyxKEMcYYvyxBGGOM8csSRCEkIl+LyB35ve5FxtBVRBLze7uFlYgsEJF73Me3icjsvKx7CfupJSLHRST4UmPNZdsqIlfk93Zz2d99IjK6oPZXXIjIIyLyd6/jyAtLEPnE/ac/85MlIid9nt92MdtS1b6q+mF+r1ucicjvRWSRn/JoEUkTkSvzui1VHa+qvfIprgQR6eGz7Z2qGqGqmfmxfa+ISCngj8DL7vM6boI68zefJCJjRCTU5zWlReQlEdnp/n9sFpEnRUSybbu3iCwSkRQROSAiC0VkgLtspLufJ7O9JlFEuuYWr4i84O4z1f1e3hOROvn2ofjfr78TrbHA7SISE8h95wdLEPnE/aePUNUIYCdwvU/Z+DPriUiId1EWax8DHUWkbrbyW4A1qrrWg5iKs4HARlXdna08yv0faA5cBfzGZ9n/gO7AdUAkMBy4F3jtzAoiMtRd7yMgFqgC/Am43mc7h4CnRaTcRcQ7ERgA3AqUB1oCy914CpSqngK+BkYU9L4vmqraTz7/AAlAD/dxVyAReBrYh3MgqwB8CRwADruPY31evwC4x308ElgMvOKuux3oe4nr1gUWASnAHOBN4JMc3kNXINHneRN3X0eAdcAAn2XXAevd7e4GnnDLo933dgTnn/pbIMjPvt4GXslW9gXwuPv4aXe7KcAmoHsOMc8G/pSt7CfgkUv5zH2W9QQ2AkeBN4CFPuvWB+YBB4FkYDzOQRL3u84CTgLHgaeAOoACIe461YFp7uezBRjls98XgM9xDpYp7ucen8vfnQJXuI/Lu687AOzAOdsPcpdd4b6Ho27Mn7nlAvwL2O8uWw1cmcO+3gP+6PP8vPfllv0DGOs+7g6cAmpm2057INONSXBOrp7M5T2OxPkbnw4871OeCHTN4TU93O+gZi7bze17+AD4v1z+NxKAJ9zP6yjwGRAGhLv7zXK//+NAdfc1twHzvT5WXejHahAFoypQEaiNc8YUBLzvPq+F80f0Ri6vb49zYIzG+ad7N3u1PI/rfopzwKyEc/AZnpfg3WaC6TgH4BjgYWC8iDRyV3kXuE9VI4ErcQ6YAL/D+cetjHMm+CzOQSS7T4Gbz8QpIhWAXsAEdx8PAW3d7ffG+Yf050Pf9+S+thXwXy7+Mz+zjWhgEs4BNhrYCnTyXQV4CecA0wSoifPZoqrDOb82+Q8/u/gvzmdUHRgK/E1EfM9qBwATgCicA9gFY3b9GydJ1AOuwTlbvdNd9lec77ICzln6v93yXkAXoKG7v5txEp8/zXH+zvwSkeo439UPblFP4EdV3eW7nqr+iPP+uwONcD6/iXl4f88Bj4lIxTys2wP4Kfu+s7nQ93AhNwF9cE7CWgAjVTUV6Avs0XOtCXvc9Tfg1GIKNUsQBSML52zntKqeVNWDqjpJVU+oagrwIs4/cU52qOo76rRbfwhUwzng5nldEakFtMU5w05T1cU4B5y86ABEAH93XzsP5wx8mLs8HWgqIuVU9bCqrvAprwbUVtV0Vf1W3dOnbL7FSRxXu8+HAt+7/0yZQGl3+6GqmqCqW3OIc4r7Xju6z0cAX6vqgUv4zM+4DlivqhNVNR0YjVMTBEBVt6jqN+53ewB4NY/bRURqAp2Bp1X1lKquAsZxfuJerKoz3O/zY/JwUHE7wG8Gfq+qKaqaAPzTZ7vpOImyurvfxT7lkUBjQFR1g6ruzWE3UTi1muySReQITo0vlXMH+2ggp23tdZdX8nmeK/ezmo1Tu7yQSrltM4/fw4W8rqp7VPUQzslUqwusn4KTwAs1SxAF44A67Y4AiEhZEfmPiOwQkWM4zT5RuVzZ4ntAOuE+jLjIdasDh3zKAHI7o/JVHdilqlk+ZTuAGu7jITgH0h1uh+JVbvnLONX12SKyTUSe8bdxN2lM4FzCuRWnqQZV3QI8inNWvl9EJrhnp/62cwKn/XqEWxu5DSdJXspnft57zxbr2eciEuPGtNvd7ic4B7u8OPOd+B5ofT9X8Pk+gRNAWB76saKBUu62/G33KZyaz08isk5E7nLf2zycGsqbQJKIjM2lnf8wTjL51b5VNQooC3wHzHTLk3FOFvyp5i4/6PM8L/4EPCAiVX0Ls10wUsvdbm7bzMv3cCHZv6ec/j/PiMRpjirULEEUjOxnzb/DqU63V9VyONV6cP5pA2UvUFFEyvqU1czja/cANUXE9++lFs5ZIqq6VFUH4jQ/TcVpN8c9e/2dqtbD6WR8PJdq+3+BoSJSG6eZbNKZBar6qap2xjnrVeD/5RLrhzjV/Z44/4RfuuWX+pnvxedzchOP7+f2khtTC3e7t2fbZm7DJe/B+U58D7RnP9fLkMy5WsKvtquq+1R1lKpWB+4Dxoh7eayqvq6qbYBmOE1N510t5GO1u9wvVT2J03Z/ldtMNwdo756tnyUi7XA+z3k4TVa7cE44LkhVNwKTcZoufcsjfH52uvtuJyKxOWzqQt9DKk7CO+O8hHShMHMobwL8fBHb8YQlCG9E4rSBH3HbUJ8P9A5VdQewDHjBveTvKs6/MiQ3P+L8kzwlIqHu5YTX4/QRlBLnvoHybhPMMZxmIUSkv4hc4R5Uz5T7vbxTVVfidKiOA2ap6hF3G41EpJuIlMbp5DyZ0zZc3+J0io8FJqhqmlt+qZ/5V0AzERnsnrk/wvkHiEiczscjIlKDXx9Qk3D6AX7FbRNfArwkImEi0gK4G7f2dKnc5qjPgRdFJNJNuo/j1G4QkRt9DpaHcQ5imSLSVkTau31OqTifd06f9QxyaUpzv6/hOGfWB1V1DjAXmCQizUQkWEQ6uO/1LVXd7NbOHgeeE5E7RaSciASJSGcRGZvDrv6M07cSlcvnMQf4BpgiIm1EJMT9XO4Xkbvy8D2sAq4TkYpubeXRnPblRxJQSUSyNyddg3MlU6FmCcIbo4EyOGd6P3CuGh5ot+FcengQ+D+cqy1OX+hF7kF2AE6HWzIwBhjhnsGBcyBIcJtY7sc5iwZogHP2dhz4Hhijqgty2dV/cToUP/UpKw383d3vPpxayrO/funZWBXn6p3a7u8zRnMJn7mqJgM3ujEcdN/Tdz6r/BmIw2ku+ArnjNbXS8AfReSIiDzhZxfDcK4A2oPTh/K8qn6Tl9gu4GGcg/w2nKt+PsW58gicvqgfReQ4Tj/Ub1V1O1AOeAcnaezAeb+v5LD96UBjP819R9ztJuH8rQ3w6XcaAszH+eyP4ySsd91YAVDViTj9J3fhfCZJOH+rX/gLwo37Y5wrhnIzFCepfYbzXa0F4nH+PiH37+FjnLP9BJx+j88usC/f+Dbi/F1vc/8GqotIGE6TbKG/f0n89xmakkBEPsO5lj3gNRhT/IjIvUBTVX3U61iKEhF5GOeS26e8juVCLEGUICLSFuc67+04lzROBa5ym3eMMeY8dldvyVIVpwmkEs413w9YcjDG5MRqEMYYY/yyTmpjjDF+FasmpujoaK1Tp47XYRhjTJGxfPnyZFWt7G9ZsUoQderUYdmyZV6HYYwxRYaI7MhpmTUxGWOM8csShDHGGL8sQRhjjPGrWPVBGGMKVnp6OomJiZw6derCKxtPhYWFERsbS2ho6IVXdlmCMMZcssTERCIjI6lTpw6S4xxWxmuqysGDB0lMTKRu3eyz8uYsoE1MItJHRDaJyJac5gIQZ1LvVe649At9yn8rImvd8kcDGacx5tKcOnWKSpUqWXIo5ESESpUqXXRNL2A1CHciljdxxuVPBJaKyDRVXe+zThTOyKB9VHWniMS45VcCo4B2QBowU0S+UtXNgYrXGHNpLDkUDZfyPQWyBtEO2KKq29zhoicAA7Otcysw2Z3UA1Xd75Y3AX5wp4fMwJlg/YZABfranM3M3ZBEembWhVc2xpgSIpAJogbnT2mZyK+n8GsIVBCRBSKyXERGuOVrgS4iUsmdAe068j772UVJPZ3Bxz/s4O4Pl3HVS3P5vy/Xs2HvsUDsyhiTzw4ePEirVq1o1aoVVatWpUaNGmefp6Wl5fraZcuW8cgjj1xwHx07drzgOnmxYMEC+vfvny/bKiiB7KT2V5/JPjJgCNAG6I4zmcv3IvKDqm4Qkf+HMwvUcZzJOjL87sQZk/5egFq1al10kOGlQ/j+991YsOkAE5fv4sPvExi3eDvNqpdjSFwsA1tVp1JE6YverjEm8CpVqsSqVasAeOGFF4iIiOCJJ87Ny5SRkUFIiP/DXHx8PPHx8Rfcx5IlS/Il1qIokDWIRM4/64/Fma0p+zozVTXVnblrEdASQFXfVdU4Ve2CM4eB3/4HVR2rqvGqGl+5st/hRC4oNDiInk2r8J/h8fz4bA9euL4pQSL85cv1tP/bXEZ9tIyZa/eRlmFNUMYUdiNHjuTxxx/n2muv5emnn+ann36iY8eOtG7dmo4dO7Jp0ybg/DP6F154gbvuuouuXbtSr149Xn/99bPbi4iIOLt+165dGTp0KI0bN+a2227jzGjYM2bMoHHjxnTu3JlHHnnkgjWFQ4cOMWjQIFq0aEGHDh1YvXo1AAsXLjxbA2rdujUpKSns3buXLl260KpVK6688kq+/fbbfP/MchLIGsRSoIGI1MWZ/PsWnD4HX18Ab7hz/ZbCmaz+XwAiEqOq+0WkFjAYZ/rCgKsYXoqRneoyslNdNu1LYdKKRCav2M0365OoUDaUga1qMLRNLM2ql7POOWN8/Hn6Otbvyd/m2abVy/H89c0u+nW//PILc+bMITg4mGPHjrFo0SJCQkKYM2cOzz77LJMmTfrVazZu3Mj8+fNJSUmhUaNGPPDAA7+6Z2DlypWsW7eO6tWr06lTJ7777jvi4+O57777WLRoEXXr1mXYsGEXjO/555+ndevWTJ06lXnz5jFixAhWrVrFK6+8wptvvkmnTp04fvw4YWFhjB07lt69e/OHP/yBzMxMTpw4cdGfx6UKWIJQ1QwReQiYBQQD76nqOhG5313+ttuUNBNYDWQB41R1rbuJSSJSCUgHfqOqhwMVa04aVY3k2eua8FTvRny7OZmJKxL59MedfLAkgUZVIhnaJpaBrasTExlW0KEZY3Jx4403EhwcDMDRo0e544472Lx5MyJCenq639f069eP0qVLU7p0aWJiYkhKSiI2Nva8ddq1a3e2rFWrViQkJBAREUG9evXO3l8wbNgwxo4dm2t8ixcvPpukunXrxsGDBzl69CidOnXi8ccf57bbbmPw4MHExsbStm1b7rrrLtLT0xk0aBCtWrW6nI/mogT0RjlVnYEzUbhv2dvZnr8MvOzntVcHMraLERIcxLWNY7i2cQxHT6QzffUeJi5P5MUZG/j7zI1c07AyQ+Ji6d4khrDQYK/DNcYTl3KmHyjh4eFnHz/33HNce+21TJkyhYSEBLp27er3NaVLn+trDA4OJiPj192e/ta5lEnX/L1GRHjmmWfo168fM2bMoEOHDsyZM4cuXbqwaNEivvrqK4YPH86TTz7JiBEj/Gw1/9md1BepfNlQbu9Qm9s71GbL/uNMWpHIlBW7mbdxBeXCQhjQqjpD4mJpVTPKmqCMKQSOHj1KjRrOBZQffPBBvm+/cePGbNu2jYSEBOrUqcNnn312wdd06dKF8ePH89xzz7FgwQKio6MpV64cW7dupXnz5jRv3pzvv/+ejRs3UqZMGWrUqMGoUaNITU1lxYoVliCKgitiIni6T2Oe6NWIJVuTmbg8kf8tS+STH3ZSv3I4Q9rEMrh1LFXLWxOUMV556qmnuOOOO3j11Vfp1q1bvm+/TJkyjBkzhj59+hAdHU27du0u+JoXXniBO++8kxYtWlC2bFk+/PBDAEaPHs38+fMJDg6madOm9O3blwkTJvDyyy8TGhpKREQEH330Ub6/h5wUqzmp4+Pj1esJg46dSmfG6r1MWpHI0oTDBAl0uiKaoW1i6d2sqjVBmWJlw4YNNGnSxOswPHf8+HEiIiJQVX7zm9/QoEEDHnvsMa/D+hV/35eILFdVv9f7Wg0in5ULC+WWdrW4pV0tEpJTmbwikUkrdvPbCauILB1CvxbVGNomlja1K1gTlDHFxDvvvMOHH35IWloarVu35r777vM6pHxhNQiAY3sgshoE6ICdlaX8sP0gk5bv5uu1ezmRlkmdSmUZEhfLDXE1iK1QNiD7NSbQrAZRtFgN4mKdPALvdIPqrWHAvyE8Ot93ERQkdKwfTcf60fxlYDO+XruPict38c9vfuGf3/xCx/qVGBIXS9/mVSlbyr4SY0zhYDPKlS4HHR+GLXNgzFXwy+yA7i68dAhD28Qy4d6r+Papa3m8Z0MSD5/kd//7mbb/N4cn/vczP2w7SFZW8anZGWOKJmtiOiNpHUy6B/avh7ajoOdfoFTBNP2oKksTDjNpeSJfrdnL8dMZxFYow+C4WIbE1aB2pfALb8QYD1gTU9FysU1MliB8pZ+CuX+BH96E6EYw5B2o1jL/AsyDk2mZzFq3j0krElm8JRlVaFenIkPbOE1QkWF5ny7QmECzBFG0XGyCsCYmX6Fh0OdvMHwKnD4G73SHxf+CrMwCC6FMqWAGta7Bx3e357unu/Fk70YkHz/NU5NW0/bFOTz22SoWb04m05qgjKFr167MmjXrvLLRo0fz4IMP5vqaMyeS1113HUeOHPnVOi+88AKvvPJKrvueOnUq69efnf+MP/3pT8yZM+ciovevMA0LbgnCn/rd4IEl0KgvzHkBPhwAR3Zd8GX5rXpUGX5z7RXM/d01TH6wI4PjYpmzIYnb3/2Rq//fPF6etZFtB44XeFzGFBbDhg1jwoQJ55VNmDAhTwPmgTMKa1RU1CXtO3uC+Mtf/kKPHj0uaVuFlSWInJStCDd9BAPHwN5V8FYnWDPRk1BEhLhaFfjbDc1Z+oce/HtYaxpWjeStBVvp9s+FDB7zHeN/3MHRk/4HITOmuBo6dChffvklp0+fBiAhIYE9e/bQuXNnHnjgAeLj42nWrBnPP/+839fXqVOH5ORkAF588UUaNWpEjx49zg4JDs49Dm3btqVly5YMGTKEEydOsGTJEqZNm8aTTz5Jq1at2Lp1KyNHjmTiROcYMXfuXFq3bk3z5s256667zsZXp04dnn/+eeLi4mjevDkbN27M9f15PSy4XVOZGxFofRvU7giT74VJd8Mvs+C6l6FMlCchhYUGc33L6lzfsjpJx04xdeVuJi5P5A9T1vLn6evp1bQKQ9vEcnWDygQH2Y14pgB9/QzsW5O/26zaHPr+PcfFlSpVol27dsycOZOBAwcyYcIEbr75ZkSEF198kYoVK5KZmUn37t1ZvXo1LVq08Lud5cuXM2HCBFauXElGRgZxcXG0adMGgMGDBzNq1CgA/vjHP/Luu+/y8MMPM2DAAPr378/QoUPP29apU6cYOXIkc+fOpWHDhowYMYK33nqLRx99FIDo6GhWrFjBmDFjeOWVVxg3blyO78/rYcGtBpEXFevCnV/DtX+AtZPg7c6QsNjrqKhSLoz7rqnP7Me6MO2hTgxrW5PFW5IZ+f5SrnppLi/N2MAvSSleh2lMQPk2M/k2L33++efExcXRunVr1q1bd15zUHbffvstN9xwA2XLlqVcuXIMGDDg7LK1a9dy9dVX07x5c8aPH8+6detyjWfTpk3UrVuXhg0bAnDHHXewaNGis8sHDx4MQJs2bUhISMh1W4sXL2b48OGA/2HBX3/9dY4cOUJISAht27bl/fff54UXXmDNmjVERkbmuu28sBpEXgWHwDVPOf0Tk0fBB/2h86PQ9VkIKeVpaCJCi9goWsRG8Wy/JszfuJ+JyxMZt3g7/1m0jRax5RnaJpbrW1SnQri3sZpiLJcz/UAaNGgQjz/+OCtWrODkyZPExcWxfft2XnnlFZYuXUqFChUYOXIkp06dynU7OQ19M3LkSKZOnUrLli354IMPWLBgQa7budCVoWeGDM9pSPELbasghwW3GsTFio2H+76FuOHOFU7v9oADv3gd1VmlQ4Lpc2U1xt3Rlh+f7c5z/ZuSnqn86Yt1tPvbHB74ZDlz1ieRnmnTp5riISIigq5du3LXXXedrT0cO3aM8PBwypcvT1JSEl9//XWu2+jSpQtTpkzh5MmTpKSkMH369LPLUlJSqFatGunp6YwfP/5seWRkJCkpv66hN27cmISEBLZs2QLAxx9/zDXXXHNJ7+3MsOCA32HBn376aeLj49m4cSM7duwgJiaGUaNGcffdd7NixYpL2qcvq0FcitIRzrAcDXrDtIfhP12g11+h7T0BG8/pUkRHlObuznW5u3Nd1u05yqTlu/li1W6+XruP6IhStKldgUZVImlUtRyNqkZSp1JZQoLtnMEUPcOGDWPw4MFnm5patmxJ69atadasGfXq1aNTp065vj4uLo6bb76ZVq1aUbt2ba6++tx8ZX/9619p3749tWvXpnnz5meTwi233MKoUaN4/fXXz3ZOA4SFhfH+++9z4403kpGRQdu2bbn//vsv6X15PSy43Sh3uVL2wdQHYetcaNALBr4JETEFG8NFSM/MYsGmA0z/eQ9r9xwlITmVM7dUlAoJokFMhJs0nJ/GVctRpVxpG3nW+GU3yhUtNlhfQYusCrdNhKXvwOznnPGcBr7h3ENRCIUGB9GzaRV6Nq0CwKn0TLbsP87GfSn8kpTCxn0pfLc1mckrd599TfkyodmSRiQNq0ZSzu7qNqZYswSRH4KCoP19ULcLTBoF/70F2twJvV+EUoV7HKWw0GCurFGeK2uUP6/8yIk0Nu1LYZObNDbtS2Hqyt2knD7XqVa9fJibNMo5SaNKJPVjwikdYpMiGVMcWILITzFNYNRcmPdXWPIGJHwLg9+BGnFeR3bRosqWon29SrSvV+lsmaqy5+gpNu07djZpbNqXwuItyaRnOu1UIUFC3ejwczWNKk4zVWyFMgTZfRnFkqpaE2QRcCndCdYHESjbFsLUB+B4EnR9Bjo/DkHF88w6PTOL7cmpbtI4xqZ9x9mUdIxdh06eXadsqWAaVomkUZVIrqpfif4tqlmHeDGwfft2IiMjqVSpkiWJQkxVOXjwICkpKdStW/e8ZTaaq1dOHoYvH4N1U6BmBxj8H6hQx+uoCszx0xn8kpTCL/vONVNtSkrhUGoa9SqH81iPhvRrXs1qFkVYeno6iYmJF7zHwHgvLCyM2NhYQkPP7zu0BOElVVj9Ocx4wnnc7xVocXOhuhy2IKkqs9cn8ersX9iUlELjqpH8rlcjejSJsTNQYzxgCaIwOLwDptwHO7+HZjdA/39BmQpeR+WZzCzly9V7GD1nM9uTU2lZM4onejWk8xXRliiMKUCWIAqLrEzn7usFL0FEFRj0FtS7tDssi4uMzCwmrUjk9blb2H3kJO3qVuTJ3o1oW6ei16EZUyJYgihsdq9wxnM6uBWu+g10+yOElvE6Kk+dzshkwk+7eGP+Fg6knOaahpX5Xa+GtIiN8jo0Y4o1SxCFUVoqzPoDLH8fKtaD61+Huldf+HXF3Mm0TD76PoG3Fm7lyIl0ejerwuM9G9Go6uWPTGmM+TVLEIXZtgUw/bdwOAHajISef4Gw8hd4UfGXciqd9xYnMO7bbRxPy2BAy+o81qMhdaIL942HxhQ1liAKu7QTMP9F+GGM0zfR75/QuJ/XURUKh1PT+M+ibXywZDvpmcqNbWJ5uHsDakSV7CY5Y/KLJYiiYvdymPYIJK2FpoOcmesK8cB/BWl/yinGzN/Kpz/uBODW9rV48Nr6xESGeRyZMUWbZwlCRPoArwHBwDhV/dWMIiLSFRgNhALJqnqNW/4YcA+gwBrgTlXN9W6cIp8gADLT4bvRsPAfEFoW+rwELYeV2Psmstt95CRvzNvM58sSCQ0W7uhYh/u71LeJkIy5RJ4kCBEJBn4BegKJwFJgmKqu91knClgC9FHVnSISo6r7RaQGsBhoqqonReRzYIaqfpDbPotFgjjjwCanNrHrB2cWu/6joUJtr6MqNBKSUxk95xe++HkPEaVCuPtqZ96LSBth1piLkluCCORgOO2ALaq6TVXTgAnAwGzr3ApMVtWdAKq632dZCFBGREKAssCeAMZa+FRu5MyDfd0rsOsnGNMBvh/j3EthqBMdzuhbWjPr0S50uiKa0XM2c/U/5vP2wq2cTLPPyJj8EMgEUQPY5fM80S3z1RCoICILRGS5iIwAUNXdwCvATmAvcFRVZ/vbiYjcKyLLRGTZgQMH8v1NeCooCNqNggd/gDqdYdbv4d1esH+D15EVGg2rRPL28DZMf6gzrWpG8fevN3L1P+bzwXfbOZ1hicKYyxHIBOGv0Tx7e1YI0AboB/QGnhORhiJSAae2UReoDoSLyO3+dqKqY1U1XlXjK1eunH/RFyZRNeHWz2HwODi0Dd6+Gua/BBmnvY6s0GgeW54P7mzH/+6/ivqVw3lh+nq6vbKQz5butPm3jblEgUwQiUBNn+ex/LqZKBGYqaqpqpoMLAJaAj2A7ap6QFXTgclAxwDGWviJQIsb4aGl0GwQLPy7Mxf2rqVeR1aotK1TkQn3duCTu9sTHVmapyetoeerC/li1W4ys4rPFXvGFIRAJoilQAMRqSsipYBbgGnZ1vkCuFpEQkSkLNAe2IDTtNRBRMqKM3Jbd7fchEfDkHFOjeL0cXi3J3z9jPPYACAidG4QzdQHO/LOiHjCQoP57YRV9H1tER99n8Ch1DSvQzSmSAj0Za7X4VzCGgy8p6ovisj9AKr6trvOk8CdQBbOpbCj3fI/AzcDGcBK4B5VzbVNpVhdxZQXp1Ngzp+d+bDL14LrR8MV3b2OqtDJylK+WrOXN+dvYeO+FEKChK6NYhgcV4NujWMICy2eEzkZkxd2o1xxt+N7mPYwHNzs3DPR+29Q1kZD9Wf9nmNMWZnIF6v2sD/lNJFhIfRvUY0bWscSX7uCTV5kShxLECVB+ilY9LJzk12ZCtD3H868E3aDnV+ZWcp3W5KZsnI3M9fu42R6JrEVynBD6xrc0LoG9SpHeB2iMQXCEkRJsm+NU5vYsxIaXeeM61SuutdRFWqppzOYtW4fU1bu5rstyWQptKwZxZC4GvRvUZ2Kdpe2KcYsQZQ0mRnw41sw70UIDoWef4a4kc59FSZXScdO8cWq3Uxesdv6K0yJYAmipDq0zRmuI+FbqN0Zrn8Noq/wOqoiY8PeY0xZuZupK3dbf4UptixBlGSqsPJjmPVHyDgF1/4ernoYgkO8jqzIyMxSlmxNZvIK668wxY8lCAPH9sKMJ2Djl1C1BQx8A6q19DqqIien/orBrWtwfUvrrzBFjyUIc876L+CrJ+BEMpSLhbByzgx2YeWhtM/jHMvd5yF2IPTfX1GZG1rH0r2J9VeYosEShDnfycPOyLBHd8Gpo+7PsXOPTx/j18NmZRNaNu8JJXtyCa9c7Jq4rL/CFFWWIMzFycqCtBT/ieNsQjnqp9xneVZ6ztuvdAUMnwJRtQruPRWQM/0VU1bsZua6fZxIc/orBraqzoCWNWhUNdLrEI05jyUIU7BUnQ7xX9VOjkBqMiz4G5QuDyOnQ4U6XkcbMKmnM5i9fh+TV5zrr2hUJZIBrapzfYvq1KpU1usQjbEEYQqZPavg40FOM9Ud06FSfa8jCrgDKaeZsWYv037ew/IdhwFoVTOKAS2r079FNWLK2dzaxhuWIEzhs28NfDQQgkKdJFG5odcRFZjEwyeY/vNepv+8h/V7jyECHepWYkCr6vS9sipRZe0CAFNwLEGYwilpPXw0ABC4YxrENPE6ogK3ZX8K09xksT05ldBgoUuDygxoVZ0eTaoQXrp4deabwscShCm8DvwCH14PWRkw4guoeqXXEXlCVVm7+xjTft7Nl6v3svfoKcJCg+jepAoDWlana6PKlA6xy2ZN/rMEYQq3g1udJJF+AoZPheqtvI7IU1lZyrIdh5n2825mrNnHodQ0IsNC6NOsKgNaVeeqepUICbZxtUz+sARhCr9D250kcfqYcwlsjTZeR1QopGdm8d2WZKb9vIfZ65I4fjqD6IhSXNe8GgNaVieult1jYS6PJQhTNBzZCR/0d27ku30S1GzndUSFyqn0TBZs2s+0n/cwd8N+TmdkUSOqDP1bOsmiabVyiM3/YS6SJQhTdBxNdGoSx/fDbROh9lVeR1QopZxK55v1SUz7eQ+LNyeTkaXUrxzOgJY1uL5lNRtA0OSZJQhTtBzb6ySJY7vh1s+h7tVeR1SoHUpN4+u1e5m2ag8/JRxCFa6sUc69x6I61aPKeB2iKcQsQZii5/h++HAAHE6AYZ9C/W5eR1Qk7D16kq9WOzfkrU48CjjJonvjKnRvEsOV1ctbn4U5jyUIUzSlJjs30yVvhlvGQ4OeXkdUpGxPTuXrtXuZu2E/K3YeRhUqR5amW6MYujeJoXODaMqWsvssSjpLEKboOnHISRIHNsJNH0Gjvl5HVCQdSk1jwab9zN2wn0W/HCDldAalQoK4ql4lejSJ4drGMcRWsLGhSiJLEKZoO3kYPh4M+1bD0Peh6QCvIyrS0jKyWJZwiLkb9zN3QxIJB08A0LhqJN0ax9C9SRVa1Ywi2JqiSgRLEKboO3UUPhkKu5fDkHfgyiFeR1QsqCrbklOZt2E/czYksWzHYTKzlIrhpejaqDI9mlTh6gbRRIaFeh2qCRBLEKZ4OJ0C42+CXT/ADf+BFjd5HVGxc/REOgs3H2DehiTmbzrA0ZPphAYL7epWpFvjKvRoEkPtSuFeh2nykSUIU3ykpcKnN0PCYhg0Blrd6nVExVZGZhYrdh5h7sYk5m7Yz5b9xwGoXzmc7k2q0L1xDG1qV7BhP4o4SxCmeEk7ARNuhW0L4PrR0GakxwGVDDsOpjJvo9PR/eP2g6RnKuXCQujqXhXVtWEM5ctaU1RRYwnCFD/pp+Dz4bB5Nlz3CrQb5XVEJUrKqXQWb05mzob9zN+0n0OpaQQHCW1qV6C729Fdv3K4Df1RBFiCMMVTxmn430jYNAP6/B06POB1RCVSZpbyc+IR5m5wmqI27ksBoGGVCIbExXJD6xo2Y14hZgnCFF8ZaTDpLtgwHXr+FTo94nVEJd7uIyeZuyGJKSt3s3LnEYIEujSszJC4WHo2rUJYqM1rUZhYgjDFW2Y6TL4X1k2Gbs9Blye8jsi4th44zuQViUxesZu9R08RGRZC/xbVGdomlrhaUdYEVQh4liBEpA/wGhAMjFPVv/tZpyswGggFklX1GhFpBHzms1o94E+qOjq3/VmCKMEyM+CLB2H1Z3DNM9D1GbCDT6GRmaV8v/Ugk1Yk8vXavZxKz6JudDhD4mpwQ1wsNWxAQc94kiBEJBj4BegJJAJLgWGqut5nnShgCdBHVXeKSIyq7veznd1Ae1Xdkds+LUGUcFmZMO0RWPUJXP07pzZhSaLQSTmVztdr9jFxRSI/bT+ECHSsX4khcbH0ubKqjQ9VwHJLEIH8JtoBW1R1mxvEBGAgsN5nnVuByaq6EyB7cnB1B7ZeKDkYQ1AwDPg3BIfAt/+EzDSnX8KSRKESGRbKTW1rclPbmuw8eILJKxOZtCKRxz//meemrqVv82oMbRNLuzoVbeRZjwUyQdQAdvk8TwTaZ1unIRAqIguASOA1Vf0o2zq3AP/NaScici9wL0CtWrUuM2RT5AUFQf/REBQKS/7tND31ecmSRCFVq1JZHu3RkEe6NWBpwiEmrUhkxpp9TFyeSGyFMgyOi2VIXA27e9sjgWxiuhHorar3uM+HA+1U9WGfdd4A4nFqCWWA74F+qvqLu7wUsAdopqpJF9qnNTGZs1Rh1rPwwxiIv9u5VyLI7vgtCk6mZTJrnZMkvtuajCq0q1ORIW1qcF3zajYuVD7zqokpEajp8zwW52CffZ1kVU0FUkVkEdASp+8CoC+wIi/JwZjziEDvv0FwKHz3GmSlQ//XLEkUAWVKBTOodQ0Gta7BniMnmbJyN5NWJPL0pDU8P20dfZpVZUibWDrWj7YRZwMskAliKdBAROridDLfgtPn4OsL4A0RCQFK4TRB/ctn+TByaV4yJlci0OPPEFwKFr3sNDed6aMwRUL1qDL85toreLBrfVbuOsKk5YlM/3kPU1ftoVr5MG5oXYMhbWKpb3NwB0SgL3O9DucS1mDgPVV9UUTuB1DVt911ngTuBLJwLoUd7ZaXxenDqKeqR/OyP2tiMjla+A+Y/yJUudLpo6jZ1uuIzCU6lZ7J3A37mbh8F4s2J5OZpbSqGcWQNrEMaFHdxoO6SHajnDHg3G094ylI2Qvxd0H3P0GZKK+jMpdhf8opvli5h0krEtm4L4VSwUF0bxJDnyurcm3jGMpZf8UFWYIw5ozTKTDvRfjpPxBe2bnCqdlgu8qpiFNV1u05xsTliXy5ei/Jx08TGixcVT+aXk2r0LNpFarYeFB+WYIwJrs9K2H6o7B3FdTvDv3+CRXreh2VyQdZWcrKXYeZvS6JWev2nZ1StXWtKHo1rUrvZlWoZ30WZ1mCMMafrExYOg7m/tW5yqnLk9DxEQgp5XVkJp+oKpv3H2f2un3MWpfEmt1Od+YVMRH0alqF3s2q0iK2fIkeE8oShDG5ObYHvn4aNkyDyo2h/7+gdkevozIBsOfISb5Z79Qsftx+iMwspWq5MHo1q0KvplVpX68ioUVwhjxVveQkZwnCmLzYNBNmPAFHd0Hr4dDzL1C2otdRmQA5ciKNuRv2M3v9Phb+coBT6VmUCwuhe5Mq9G5WhS4NKxe6caFOpmWyLfk4W/YfZ+uBVLYeOM7W/cfJUmX2Y9dc0jYvO0GISDhwUlWzRKQh0Bj4WlXTLymiALEEYS5bWios+Dt8/6ZzhVOvF6HlLdaJXcydTMvk280HmLUuibkbkzhyIp3SIUFc3SCaXs2q0qNJFSqGF0zTo6qSfDzNOfgf8EkG+4+z+8jJs+sFCdSsWJb6lSNoEBPBM30bX1ItIj8SxHLgaqAC8AOwDDihqrdddDQBZAnC5Jt9a+HLRyFxKdTtAv1ehegGXkdlCkBGZhZLEw4za90+vlmfxO4jJwkSaFunIr2aVaVX0yrUrFg2X/az89CJ82oCW9zfx05lnF2vTGgw9WPCqV854txPTDh1KoXny+RL+ZEgVqhqnIg8DJRR1X+IyEpVbX3Z0eUjSxAmX2VlwfL3Yc6fIeMkdH4cOj8GoXa5ZElx5vLZM53cm5Kc6VSbVitHr2ZOJ3fjqpG5nrmnnEpn25kkcOA4W/ensuXAcXYcTCU989zxNyay9NmD/7lEEEG1cmEBHdU2PxLESuBBnGEw7lbVdSKyRlWb52+ol8cShAmIlCRn4L+1E6HSFU5tot6ltfeaoi0hOZXZ6/cxe10Sy3ceRhVqVSxLr6ZV6N6kCplZei4RuMlg37FTZ18fEiTUrlT27MHfSQTh1KscQfky3tzUlx8J4hrgd8B3qvr/RKQe8KiqFqoJgC1BmIDaMhe+ehwOJ0CLm53+iYjKXkdlPHIg5TRzNiQxe90+vttykLTMrLPLIkuHnEsAPjWC2pXKFrqrpPL1KiYRCQIiVPVYfgSXnyxBmIBLPwmLXnFGiC0V7lzp1Hq4jRJbwqWcSueHbYcILx3MFZUjqBxZusjcW5FbgsjTX7WIfCoi5dyrmdYDm9xB9owpWULLQPfn4IHvIKYpTH8EPrgO9m/wOjLjociwUHo2rULH+tHElAsrMsnhQvJ62tPUrTEMAmYAtYDhgQrKmEKvciMY+RUMeAMObIS3Ozud2WknvI7MmHyT1wQRKiKhOAniC/f+h+Jzh50xlyIoCOKGw0PLoPlNsPhVGNMBNs/xOjJj8kVeE8R/gAQgHFgkIrWBQtcHYYwnwqPhhrfgji+dyYnGD4H/3Qkp+7yOzJjLcslDbYhIiKpmXHjNgmOd1MZzGadh8Wj49p8QUtqZcyL+Lgi6/BuajAmE/OikLi8ir4rIMvfnnzi1CWOMr5DS0PVpePB7qN7aGdvp3Z6wZ5XXkRlz0fLaxPQekALc5P4cA94PVFDGFHmV6sOIL2DwO3B4B4ztCpPvgyO7vI7MmDzL641yq1S11YXKvGZNTKZQOnnE6cD+4W3neYf7nWE7bLpTUwhcdhMTcFJEOvtssBNwMpf1jTFnlIlybqh7eDlcORi+ex1ebwXfj3H6LIwppPKaIO4H3hSRBBFJAN4A7gtYVMYUR1E14Ya34b5FUK0VzPo9vNEW1kx0BgY0ppDJU4JQ1Z9VtSXQAmjhjuLaLaCRGVNcVWsBI6bC7ZOhdCRMuhvGdYPt33odmTHnuagBZFT1mM8YTI8HIB5jSo4ruju1iUFvwfH98GF/+PRmG7bDFBqXM8JY8RhsxBgvBQVDq1ud/okeL8COJfBWR5j2MBzb63V0poS7nARhQ20Yk19CyziTET2yCtrfD6v+C/+Og3kvwukUr6MzJVSuCUJEUkTkmJ+fFKB6AcVoTMkRXgn6vAQP/QQN+8Cif8DrreGndyCzUE0Bb0qAXBOEqkaqajk/P5GqGlJQQRpT4lSsBze+D/fMg+iGzh3ZYzrAhulwicPjGHOxbJYTYwqz2DbOsOLDJoAEw2e3w3u9YeePXkdmSgBLEMYUdiLQqC88sASuf82Z8vS9XvDZcDi41evoTDFmCcKYoiI4BNqMhEdWQtdnnTmy32wHXz0Bxw94HZ0phgKaIESkj4hsEpEtIvJMDut0FZFVIrJORBb6lEeJyEQR2SgiG0TkqkDGakyRUSrcGTH2t6sg7g5Y9p7Tkb3oZZvRzuSrgCUIEQkG3gT6Ak2BYSLSNNs6UcAYYICqNgNu9Fn8GjBTVRsDLQG7e8gYXxEx0P9VePAHqHcNzPs/59LYFR9BVqbX0ZliIJA1iHbAFlXdpqppwARgYLZ1bgUmq+pOAFXdDyAi5YAuwLtueZqqHglgrMYUXZUbwi3j4c6ZUD7WucnurU7wy2y74slclkAmiBqA7+D3iW6Zr4ZABRFZICLLRWSEW14POAC8LyIrRWSciNgERcbkpvZVcPc3cOOHkHkaPr0RPhoARxO9jswUUYFMEP6G4sh+OhMCtAH6Ab2B50SkoVseB7zlDgyYCuTUh3HvmZnuDhywjjpTwolAs0Hw4I/Q9x+weyX8pwtsW3jBlxqTXSATRCJQ0+d5LLDHzzozVTVVVZOBRTj9DYlAoqqeudh7Ik7C+BVVHauq8aoaX7ly5Xx9A8YUWSGloP19cO98CK8MHw9y5qGwJidzEQKZIJYCDUSkroiUAm4BpmVb5wvgahEJEZGyQHtgg6ruA3aJSCN3ve7A+gDGakzxFN0A7pkDTa6Hb56D/420sZ1MngVsuAxVzRCRh4BZQDDwnqquE5H73eVvq+oGEZkJrAaygHGqutbdxMPAeDe5bAPuDFSsxhRrpSOdfokl/4Y5zzvDid8y3kkexuQiT3NSFxU2J7UxF7BtIUy8EzLS4Ia3nJqFKdHyY05qY0xxUO8aZ5Ki6AbOuE5z/mz3TJgcWYIwpqQpHwt3fu3chb34VfhkCJw45HVUphCyBGFMSRQaBgNeh+tfhx3fwX+ugT2rvI7KFDKWIIwpydrcAXfNBM2Cd3vByvFeR2QKEUsQxpR0NdrAfQuhVnv44kH48jHIOO11VKYQsARhjIHwaLh9CnT6rTM67Af94Fj2+1pNSWMJwhjjCA6Bnn9x7pnYv8EZoiNhsddRGQ9ZgjDGnK/ZIBg1D8Ki4MMB8P2bNkRHCWUJwhjza5UbOUmiUV+Y9SxMvAvSUr2OyhQwSxDGGP/CysHNn0D352H9VBjXw+bALmEsQRhjciYCVz8Ot0+ClH0wtits+trrqEwBsQRhjLmw+t2cS2Er1oX/3gLzXrQhOkoASxDGmLyJqgV3zYJWt8Gif8CnN9kQHcWcJQhjTN6FloGBb0K/V52RYcd2hb2rvY7KBIglCGPMxRGBtnc7A/5lpsO7PeHnCV5HZQLAEoQx5tLUbOv0S9SIhyn3wYwnnXkmTLFhCcIYc+kiYmDEVLjqIfhpLHzYH47t9Toqk08sQRhjLk9wKPR+EYa+B/vWwNhrYMcSr6My+cAShDEmf1w5BO6ZC6Ui4MPr4avfOR3ZmeleR2YuUYjXARhjipEqTeHe+TDjKVj5CSwd54zp1LAPNO4HV3SHUuFeR2nySLQYDcIVHx+vy5Yt8zoMYww4YzdtnQcbv3Luvj51BELCoN61TrJo1NcZZtx4SkSWq2q8v2VWgzDGBEapcGhyvfOTmQE7lzjJYuNX8MvXIEFQs4OTLBr3c+7SNoWK1SCMMQVLFfatPpcsktY65THNziWLai2d+y1MwOVWg7AEYYzx1qHtsGkGbJzh1DI0C8rFnksWtTs6V0qZgLAEYYwpGlIPwi8znZrF1rmQcco6uQPM+iCMMUVDeCVofZvzk5YKW+ef67NYPcE6uQuYJQhjTOFUKhya9Hd+MjNg5/fWyV3ArInJGFO0qDp3bJ/t5F7jlJ/p5G42CKo08zTEosT6IIwxxdfhBKeDe+NX5zq5m98EPf8M5ap7HV2hZwnCGFMypCbDD2/Bkn9DUIgzXepVD0FomNeRFVq5JQgbi8kYU3yER0P35+A3P0L9a2HeX+HNdrBhutM0ZS5KQBOEiPQRkU0iskVEnslhna4iskpE1onIQp/yBBFZ4y6zaoExJu8q1oVbxsOILyC0LHx2O3w0EJLWex1ZkRKwBCEiwcCbQF+gKTBMRJpmWycKGAMMUNVmwI3ZNnOtqrbKqfpjjDG5qtcV7l8MfV+GvT/D252diY1sLu08CWQNoh2wRVW3qWoaMAEYmG2dW4HJqroTQFX3BzAeY0xJFBwC7e+FR1ZCm5HOCLP/buP8zszwOrpCLZAJogawy+d5olvmqyFQQUQWiMhyERnhs0yB2W75vTntRETuFZFlIrLswIED+Ra8MaaYKVsR+r8K933rXAb71e+cyY22f+t1ZIVWIBOEv5G2svcShQBtgH5Ab+A5EWnoLuukqnE4TVS/EZEu/naiqmNVNV5V4ytXrpxPoRtjiq2qV8Id0+Gmj+DUMWea1M9HwJGdXkdW6AQyQSQCNX2exwJ7/KwzU1VTVTUZWAS0BFDVPe7v/cAUnCYrY4y5fCLQdCA89BNc+wf4ZTa80RbmvegM8WGAwCaIpUADEakrIqWAW4Bp2db5ArhaREJEpCzQHtggIuEiEgkgIuFAL2BtAGM1xpREoWXgmqfg4WXQuD8s+oeTKNZMtMtiCWCCUNUM4CFgFrAB+FxV14nI/SJyv7vOBmAmsBr4CRinqmuBKsBiEfnZLf9KVWcGKlZjTAlXPhaGvgt3zoSylWDS3fB+X9izyuvIPGV3UhtjjK+sTGc+7bl/gRMHIW44dPsTRBTPPk67k9oYY/IqKBja3AEPL4cOD8KqT53LYr9/EzLTvY6uQFmCMMYYf8pEQZ+/wQNLIDYeZj0Lb3WEzXO8jqzAWIIwxpjcVG4Et0+CYZ9BVgaMHwKf3gwHt3odWcBZgjDGmAsRgUZ94MEfoMefIWExvNkeZj/n3EtRTFmCMMaYvAopDZ0fhYdXQIubYMnr8EY8rBwPWVleR5fvLEEYY8zFiqwCg8bAqHkQVQu+eBDGdYf9G7yOLF9ZgjDGmEtVow3cNRtu+A8c3QXv9oZtCy/8uiLCEoQxxlyOoCBoeQuMmu9McfrJEPj5M6+jyheWIIwxJj9E1YS7ZkKtDjDlXlj0SpEfrsMShDHG5JcyUc4lsc1vcqY7/fLRIj3nRIjXARhjTLESUhoGj3VqFN/+E47tgaHvQ+kIryO7aFaDMMaY/CYC3f8E/UfDljnwwXWQkuR1VBfNEoQxxgRK/J0wbAIkb4ZxPeDAJq8juiiWIIwxJpAa9oaRX0HGKXi3FyR853VEeWYJwhhjAq1GHNzzDYRXho8HORMSFQGWIIwxpiBUqAN3z3Zurpt0N3z3WqG/DNYShDHGFJSyFWH4VGh2A3zzJ5jxpDNBUSFll7kaY0xBCg2DIe9B+ZrOYH/HdsOQd6FUWa8j+xWrQRhjTEELCoJef4XrXoFNX8OH/eH4Aa+j+hVLEMYY45V2o+CW8ZC0Ht7tAclbvI7oPJYgjDHGS437wcgv4fRxeLcn7PzR64jOsgRhjDFei413LoMtEwUfDYD107yOCLAEYYwxhUPFenD3HKjaAj4fAT+85XVEliCMMabQCK8Ed0xzmp1mPgMzf+/pVKaWIIwxpjAJLQM3fQTtH4AfxsD/7oD0k56EYgnCGGMKm6Bg6Pt36P032DAdPhoIqQcLPowC36Mxxpi8ueo3cOMHsGeVc4XToW0FuntLEMYYU5g1G+T0S5w8BON6QuLyAtu1JQhjjCnsanWAu7+BUuHwQT/YOKNAdmsJwhhjioLoBnDPHIhpAp/dBj+9E/BdWoIwxpiiIiLGueu6QW+Y8YQzImwAL4MNaIIQkT4isklEtojIMzms01VEVonIOhFZmG1ZsIisFJEvAxmnMcYUGaXC4eZPIN6dU2LyPZBxOiC7Cthw3yISDLwJ9AQSgaUiMk1V1/usEwWMAfqo6k4Ricm2md8CG4BygYrTGGOKnOAQ6PdPiKoFc56HlH1w2/+c5JGPAlmDaAdsUdVtqpoGTAAGZlvnVmCyqu4EUNX9ZxaISCzQDxgXwBiNMaZoEoHOjzpzSVSsC6H5P59EIBNEDWCXz/NEt8xXQ6CCiCwQkeUiMsJn2WjgKSDXBjYRuVdElonIsgMHCt946sYYE1DNh8LAN52Ekc8COaOcv2izT8AaArQBugNlgO9F5AecxLFfVZeLSNfcdqKqY4GxAPHx8YV7gldjjClCApkgEoGaPs9jgT1+1klW1VQgVUQWAS2BOGCAiFwHhAHlROQTVb09gPEaY4zxEcgmpqVAAxGpKyKlgFuA7IOcfwFcLSIhIlIWaA9sUNXfq2qsqtZxXzfPkoMxxhSsgNUgVDVDRB4CZgHBwHuquk5E7neXv62qG0RkJrAap69hnKquDVRMxhhj8k5Ui0+zfXx8vC5btszrMIwxpsgQkeWqGu9vmd1JbYwxxi9LEMYYY/yyBGGMMcavYtUHISIHgB2X+PJoIDkfwykK7D0XfyXt/YK954tVW1Ur+1tQrBLE5RCRZTl11BRX9p6Lv5L2fsHec36yJiZjjDF+WYIwxhjjlyWIc8Z6HYAH7D0XfyXt/YK953xjfRDGGGP8shqEMcYYvyxBGGOM8avEJ4i8zJtdnIhITRGZLyIb3HnAf+t1TAWlpM1xLiJRIjJRRDa63/dVXscUaCLymPt3vVZE/isiYV7HlN9E5D0R2S8ia33KKorINyKy2f1dIT/2VaIThM+82X2BpsAwEWnqbVQBlwH8TlWbAB2A35SA93zGmTnOS4rXgJmq2hhnnpVi/d5FpAbwCBCvqlfijCJ9i7dRBcQHQJ9sZc8Ac1W1ATDXfX7ZSnSCIG/zZhcrqrpXVVe4j1NwDhrZp4ItdkraHOciUg7oArwLoKppqnrE06AKRghQRkRCgLL8epKyIk9VFwGHshUPBD50H38IDMqPfZX0BJGXebOLLRGpA7QGfvQ4lIIwmjzMcV6M1AMOAO+7zWrjRCTc66ACSVV3A68AO4G9wFFVne1tVAWmiqruBeckEIjJj42W9ASRl3mziyURiQAmAY+q6jGv4wkkEemPO8e517EUoBCcqXvfUtXWQCr51OxQWLnt7gOBukB1IFxEbCbKy1DSE0Re5s0udkQkFCc5jFfVyV7HUwA64cxxnoDTjNhNRD7xNqSASwQSVfVM7XAiTsIoznoA21X1gKqmA5OBjh7HVFCSRKQagPt7f35stKQniLzMm12siIjgtEtvUNVXvY6nIJTEOc5VdR+wS0QauUXdgfUehlQQdgIdRKSs+3fenWLeMe9jGnCH+/gO4Iv82GjA5qQuCnKaN9vjsAKtEzAcWCMiq9yyZ1V1hnchmQB5GBjvnvxsA+70OJ6AUtUfRWQisALnar2VFMNhN0Tkv0BXIFpEEoHngb8Dn4vI3TiJ8sZ82ZcNtWGMMcafkt7EZIwxJgeWIIwxxvhlCcIYY4xfliCMMcb4ZQnCGGOMX5YgjLkAEckUkVU+P/l2R7KI1PEdldOYwqRE3wdhTB6dVNVWXgdhTEGzGoQxl0hEEkTk/4nIT+7PFW55bRGZKyKr3d+13PIqIjJFRH52f84MAxEsIu+48xjMFpEy7vqPiMh6dzsTPHqbpgSzBGHMhZXJ1sR0s8+yY6raDngDZ8RY3McfqWoLYDzwulv+OrBQVVvijIt05q79BsCbqtoMOAIMccufAVq727k/MG/NmJzZndTGXICIHFfVCD/lCUA3Vd3mDoC4T1UriUgyUE1V093yvaoaLSIHgFhVPe2zjTrAN+5EL4jI00Coqv6fiMwEjgNTgamqejzAb9WY81gNwpjLozk8zmkdf077PM7kXN9gP5wZD9sAy91JcIwpMJYgjLk8N/v8/t59vIRzU13eBix2H88FHoCz82OXy2mjIhIE1FTV+TgTHUUBv6rFGBNIdkZizIWV8Rn5Fpx5ns9c6lpaRH7EOdka5pY9ArwnIk/izOp2ZhTV3wJj3RE3M3GSxd4c9hkMfCIi5XEmtvpXCZky1BQi1gdhzCVy+yDiVTXZ61iMCQRrYjLGGOOX1SCMMcb4ZTUIY4wxflmCMMYY45clCGOMMX5ZgjDGGOOXJQhjjDF+/X+C8k5z7oZZZwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_count_c, label='Training loss')\n",
    "plt.plot(dev_loss_count_c, label='Validation loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.title('Training loss vs Validation loss (BOCN-Count)')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.65\n",
      "Precision: 0.5920245398773006\n",
      "Recall: 0.965\n",
      "F1-Score: 0.7338403041825096\n"
     ]
    }
   ],
   "source": [
    "preds_te_count = predict_class(test_char_tf_idf, weight_c)\n",
    "\n",
    "print('Accuracy:', accuracy_score(test_labels,preds_te_count))\n",
    "print('Precision:', precision_score(test_labels,preds_te_count))\n",
    "print('Recall:', recall_score(test_labels,preds_te_count))\n",
    "print('F1-Score:', f1_score(test_labels,preds_te_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Negative and Positive Characters BOCN(TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fe\n",
      "and\n",
      "tr\n",
      "per\n",
      "ion\n",
      "wors\n",
      "tio\n",
      "rf\n",
      "tion\n",
      "man\n"
     ]
    }
   ],
   "source": [
    "top_negative_words = weight_c.argsort()[:10]\n",
    "for i in top_negative_words:\n",
    "    print(vocab_char[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th\n",
      "ba\n",
      "ha\n",
      "he\n",
      "tt\n",
      "in\n",
      "thi\n",
      "oo\n",
      "to\n",
      "ad\n"
     ]
    }
   ],
   "source": [
    "top_positive_words = weight_c.argsort()[::-1][:10]\n",
    "for i in top_positive_words:\n",
    "    print(vocab_char[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW+BOCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new train data array with BOW and BOCN count - (2800, 10000)\n",
      "new array with train labels - (2800,)\n",
      "new test data array with BOW and BOCN count - (800, 10000)\n",
      "new array with train labels - (800,)\n",
      "new validation data array with BOW and BOCN count - (400, 10000)\n",
      "new array with train labels - (400,)\n"
     ]
    }
   ],
   "source": [
    "#i have joined the BOW count vector with BOCN count vector and labels using hstack and vstack\n",
    "\n",
    "#training data\n",
    "train_BOW_BOCN_count_vector = np.vstack((train_count_vector,train_char_count_vector))\n",
    "\n",
    "print('new train data array with BOW and BOCN count -',train_BOW_BOCN_count_vector.shape)\n",
    "\n",
    "train_labels_new = np.hstack((train_labels,train_labels))\n",
    "\n",
    "print('new array with train labels -',train_labels_new.shape)\n",
    "\n",
    "#testing data\n",
    "test_BOW_BOCN_count_vector = np.vstack((test_count_vector,test_char_count_vector))\n",
    "\n",
    "print('new test data array with BOW and BOCN count -',test_BOW_BOCN_count_vector.shape)\n",
    "\n",
    "test_labels_new = np.hstack((test_labels,test_labels))\n",
    "\n",
    "print('new array with train labels -',test_labels_new.shape)\n",
    "\n",
    "#validation data\n",
    "dev_BOW_BOCN_count_vector = np.vstack((dev_count_vector,dev_char_count_vector))\n",
    "\n",
    "print('new validation data array with BOW and BOCN count -',dev_BOW_BOCN_count_vector.shape)\n",
    "\n",
    "dev_labels_new = np.hstack((dev_labels,dev_labels))\n",
    "\n",
    "print('new array with train labels -',dev_labels_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = np.logspace(-7, -4, num = 5)\n",
    "alpha = np.logspace(-6, -3, num = 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper Parameters -- Learning Rate - 1e-07  Regularization Parameter- 1e-06\n",
      "Epoch:          0 | Training loss: 0.6914988723371794 | Validation loss: 0.6910366921400016\n",
      "Epoch:          1 | Training loss: 0.6906433838251786 | Validation loss: 0.6901687257731097\n",
      "Epoch:          2 | Training loss: 0.6898480819482578 | Validation loss: 0.6899762427117031\n",
      "Epoch:          3 | Training loss: 0.6889836180180043 | Validation loss: 0.6887675108274282\n",
      "Epoch:          4 | Training loss: 0.6880862299850675 | Validation loss: 0.6885030132963333\n",
      "Epoch:          5 | Training loss: 0.6872352692115632 | Validation loss: 0.687810926729751\n",
      "Epoch:          6 | Training loss: 0.6864495400645508 | Validation loss: 0.6872754463998534\n",
      "Epoch:          7 | Training loss: 0.6855515977152251 | Validation loss: 0.6863246072840837\n",
      "Epoch:          8 | Training loss: 0.6847501892085522 | Validation loss: 0.6856497560338164\n",
      "Epoch:          9 | Training loss: 0.6839519394798088 | Validation loss: 0.6850338510460636\n",
      "Epoch:         10 | Training loss: 0.6831849832516707 | Validation loss: 0.6846160192225793\n",
      "Epoch:         11 | Training loss: 0.6824669829151961 | Validation loss: 0.683713474206681\n",
      "Epoch:         12 | Training loss: 0.6816166168220474 | Validation loss: 0.6832140060370623\n",
      "Epoch:         13 | Training loss: 0.6808497982559146 | Validation loss: 0.6826841662183735\n",
      "Epoch:         14 | Training loss: 0.680181505065827 | Validation loss: 0.682400522896496\n",
      "Epoch:         15 | Training loss: 0.6793597659850172 | Validation loss: 0.681577540404925\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1e-07  Regularization Parameter- 5.623413251903491e-06\n",
      "Epoch:          0 | Training loss: 0.691498872338859 | Validation loss: 0.6910366921416827\n",
      "Epoch:          1 | Training loss: 0.6906433838306654 | Validation loss: 0.6901687257785974\n",
      "Epoch:          2 | Training loss: 0.689848081958793 | Validation loss: 0.6899762427222381\n",
      "Epoch:          3 | Training loss: 0.6889836180373167 | Validation loss: 0.6887675108467376\n",
      "Epoch:          4 | Training loss: 0.6880862300136313 | Validation loss: 0.688503013324892\n",
      "Epoch:          5 | Training loss: 0.6872352692523371 | Validation loss: 0.6878109267705154\n",
      "Epoch:          6 | Training loss: 0.6864495401194747 | Validation loss: 0.6872754464547637\n",
      "Epoch:          7 | Training loss: 0.6855515977867784 | Validation loss: 0.6863246073556182\n",
      "Epoch:          8 | Training loss: 0.6847501892984516 | Validation loss: 0.6856497561236905\n",
      "Epoch:          9 | Training loss: 0.6839519395899328 | Validation loss: 0.6850338511561566\n",
      "Epoch:         10 | Training loss: 0.6831849833836406 | Validation loss: 0.6846160193545119\n",
      "Epoch:         11 | Training loss: 0.6824669830718992 | Validation loss: 0.6837134743633386\n",
      "Epoch:         12 | Training loss: 0.6816166170043061 | Validation loss: 0.683214006219268\n",
      "Epoch:         13 | Training loss: 0.6808497984658265 | Validation loss: 0.6826841664282237\n",
      "Epoch:         14 | Training loss: 0.6801815053049334 | Validation loss: 0.6824005231355319\n",
      "Epoch:         15 | Training loss: 0.6793597662556315 | Validation loss: 0.6815775406754585\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1e-07  Regularization Parameter- 3.1622776601683795e-05\n",
      "Epoch:          0 | Training loss: 0.6914988723483043 | Validation loss: 0.6910366921511367\n",
      "Epoch:          1 | Training loss: 0.6906433838615187 | Validation loss: 0.6901687258094569\n",
      "Epoch:          2 | Training loss: 0.6898480820180372 | Validation loss: 0.6899762427814814\n",
      "Epoch:          3 | Training loss: 0.6889836181459169 | Validation loss: 0.6887675109553214\n",
      "Epoch:          4 | Training loss: 0.6880862301742573 | Validation loss: 0.6885030134854894\n",
      "Epoch:          5 | Training loss: 0.6872352694816256 | Validation loss: 0.6878109269997512\n",
      "Epoch:          6 | Training loss: 0.6864495404283338 | Validation loss: 0.6872754467635473\n",
      "Epoch:          7 | Training loss: 0.6855515981891519 | Validation loss: 0.6863246077578863\n",
      "Epoch:          8 | Training loss: 0.6847501898039917 | Validation loss: 0.6856497566290901\n",
      "Epoch:          9 | Training loss: 0.683951940209206 | Validation loss: 0.6850338517752553\n",
      "Epoch:         10 | Training loss: 0.6831849841257621 | Validation loss: 0.6846160200964236\n",
      "Epoch:         11 | Training loss: 0.6824669839531048 | Validation loss: 0.6837134752442889\n",
      "Epoch:         12 | Training loss: 0.6816166180292226 | Validation loss: 0.6832140072438859\n",
      "Epoch:         13 | Training loss: 0.6808497996462477 | Validation loss: 0.6826841676082986\n",
      "Epoch:         14 | Training loss: 0.6801815066495278 | Validation loss: 0.6824005244797293\n",
      "Epoch:         15 | Training loss: 0.6793597677774078 | Validation loss: 0.68157754219678\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1e-07  Regularization Parameter- 0.00017782794100389227\n",
      "Epoch:          0 | Training loss: 0.6914988724014184 | Validation loss: 0.6910366922042996\n",
      "Epoch:          1 | Training loss: 0.6906433840350203 | Validation loss: 0.6901687259829924\n",
      "Epoch:          2 | Training loss: 0.6898480823511912 | Validation loss: 0.6899762431146305\n",
      "Epoch:          3 | Training loss: 0.6889836187566215 | Validation loss: 0.6887675115659339\n",
      "Epoch:          4 | Training loss: 0.6880862310775241 | Validation loss: 0.6885030143885945\n",
      "Epoch:          5 | Training loss: 0.6872352707710093 | Validation loss: 0.6878109282888376\n",
      "Epoch:          6 | Training loss: 0.6864495421651771 | Validation loss: 0.6872754484999655\n",
      "Epoch:          7 | Training loss: 0.6855516004518639 | Validation loss: 0.6863246100200061\n",
      "Epoch:          8 | Training loss: 0.6847501926468528 | Validation loss: 0.6856497594711596\n",
      "Epoch:          9 | Training loss: 0.6839519436916344 | Validation loss: 0.6850338552567036\n",
      "Epoch:         10 | Training loss: 0.6831849882990176 | Validation loss: 0.6846160242685003\n",
      "Epoch:         11 | Training loss: 0.6824669889084878 | Validation loss: 0.6837134801982367\n",
      "Epoch:         12 | Training loss: 0.6816166237927509 | Validation loss: 0.683214013005736\n",
      "Epoch:         13 | Training loss: 0.6808498062842444 | Validation loss: 0.6826841742443471\n",
      "Epoch:         14 | Training loss: 0.6801815142107374 | Validation loss: 0.6824005320387068\n",
      "Epoch:         15 | Training loss: 0.6793597763349839 | Validation loss: 0.6815775507517989\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1e-07  Regularization Parameter- 0.001\n",
      "Epoch:          0 | Training loss: 0.6914988727001015 | Validation loss: 0.6910366925032579\n",
      "Epoch:          1 | Training loss: 0.6906433850106921 | Validation loss: 0.6901687269588558\n",
      "Epoch:          2 | Training loss: 0.6898480842246549 | Validation loss: 0.6899762449880659\n",
      "Epoch:          3 | Training loss: 0.6889836221908662 | Validation loss: 0.6887675149996602\n",
      "Epoch:          4 | Training loss: 0.6880862361569663 | Validation loss: 0.6885030194671278\n",
      "Epoch:          5 | Training loss: 0.6872352780217468 | Validation loss: 0.6878109355379038\n",
      "Epoch:          6 | Training loss: 0.6864495519321642 | Validation loss: 0.6872754582645618\n",
      "Epoch:          7 | Training loss: 0.6855516131760294 | Validation loss: 0.6863246227408402\n",
      "Epoch:          8 | Training loss: 0.6847502086334362 | Validation loss: 0.6856497754532915\n",
      "Epoch:          9 | Training loss: 0.683951963274769 | Validation loss: 0.6850338748343248\n",
      "Epoch:         10 | Training loss: 0.6831850117669579 | Validation loss: 0.6846160477298112\n",
      "Epoch:         11 | Training loss: 0.6824670167746544 | Validation loss: 0.683713508056332\n",
      "Epoch:         12 | Training loss: 0.6816166562034521 | Validation loss: 0.6832140454069995\n",
      "Epoch:         13 | Training loss: 0.680849843612442 | Validation loss: 0.6826842115615895\n",
      "Epoch:         14 | Training loss: 0.6801815567305437 | Validation loss: 0.6824005745459604\n",
      "Epoch:         15 | Training loss: 0.6793598244577707 | Validation loss: 0.6815775988602063\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 5.62341325190349e-07  Regularization Parameter- 1e-06\n",
      "Epoch:          0 | Training loss: 0.6875404070696487 | Validation loss: 0.6880718502659505\n",
      "Epoch:          1 | Training loss: 0.6829362153623643 | Validation loss: 0.6843329130701133\n",
      "Epoch:          2 | Training loss: 0.6795681512922334 | Validation loss: 0.6826076228940033\n",
      "Epoch:          3 | Training loss: 0.6762240411512797 | Validation loss: 0.6784852302215958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper Parameters -- Learning Rate - 5.62341325190349e-07  Regularization Parameter- 5.623413251903491e-06\n",
      "Epoch:          0 | Training loss: 0.6875404071057634 | Validation loss: 0.6880718503020578\n",
      "Epoch:          1 | Training loss: 0.682936215501301 | Validation loss: 0.6843329132090143\n",
      "Epoch:          2 | Training loss: 0.6795681515937906 | Validation loss: 0.6826076231954704\n",
      "Epoch:          3 | Training loss: 0.6762240416723949 | Validation loss: 0.6784852307425561\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 5.62341325190349e-07  Regularization Parameter- 3.1622776601683795e-05\n",
      "Epoch:          0 | Training loss: 0.6875404073088509 | Validation loss: 0.6880718505051039\n",
      "Epoch:          1 | Training loss: 0.6829362162826 | Validation loss: 0.6843329139901126\n",
      "Epoch:          2 | Training loss: 0.6795681532895718 | Validation loss: 0.6826076248907443\n",
      "Epoch:          3 | Training loss: 0.6762240446028402 | Validation loss: 0.6784852336721312\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 5.62341325190349e-07  Regularization Parameter- 0.00017782794100389227\n",
      "Epoch:          0 | Training loss: 0.6875404084508966 | Validation loss: 0.6880718516469164\n",
      "Epoch:          1 | Training loss: 0.682936220676167 | Validation loss: 0.6843329183825506\n",
      "Epoch:          2 | Training loss: 0.6795681628256498 | Validation loss: 0.6826076344239698\n",
      "Epoch:          3 | Training loss: 0.6762240610819458 | Validation loss: 0.6784852501463433\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 5.62341325190349e-07  Regularization Parameter- 0.001\n",
      "Epoch:          0 | Training loss: 0.6875404148730905 | Validation loss: 0.6880718580678002\n",
      "Epoch:          1 | Training loss: 0.6829362453830097 | Validation loss: 0.6843329430830454\n",
      "Epoch:          2 | Training loss: 0.6795682164509576 | Validation loss: 0.6826076880332362\n",
      "Epoch:          3 | Training loss: 0.6762241537507653 | Validation loss: 0.6784853427876437\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 3.162277660168379e-06  Regularization Parameter- 1e-06\n",
      "Epoch:          0 | Training loss: 0.6687105911319241 | Validation loss: 0.6732567688769214\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 3.162277660168379e-06  Regularization Parameter- 5.623413251903491e-06\n",
      "Epoch:          0 | Training loss: 0.6687105921158331 | Validation loss: 0.6732567698605376\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 3.162277660168379e-06  Regularization Parameter- 3.1622776601683795e-05\n",
      "Epoch:          0 | Training loss: 0.6687105976487596 | Validation loss: 0.6732567753918187\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 3.162277660168379e-06  Regularization Parameter- 0.00017782794100389227\n",
      "Epoch:          0 | Training loss: 0.6687106287626918 | Validation loss: 0.6732568064964974\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 3.162277660168379e-06  Regularization Parameter- 0.001\n",
      "Epoch:          0 | Training loss: 0.6687108037291882 | Validation loss: 0.673256981410957\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1.778279410038923e-05  Regularization Parameter- 1e-06\n",
      "Epoch:          0 | Training loss: 0.6075508561538746 | Validation loss: 0.6298189802843563\n",
      "Epoch:          1 | Training loss: 0.573287308157641 | Validation loss: 0.6085986392903506\n",
      "Epoch:          2 | Training loss: 0.5442197672010288 | Validation loss: 0.590417154802116\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1.778279410038923e-05  Regularization Parameter- 5.623413251903491e-06\n",
      "Epoch:          0 | Training loss: 0.6075508779133073 | Validation loss: 0.6298190020375813\n",
      "Epoch:          1 | Training loss: 0.5732873691426807 | Validation loss: 0.6085987002583816\n",
      "Epoch:          2 | Training loss: 0.5442198726087041 | Validation loss: 0.5904172601818682\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1.778279410038923e-05  Regularization Parameter- 3.1622776601683795e-05\n",
      "Epoch:          0 | Training loss: 0.6075510002755894 | Validation loss: 0.629819124364954\n",
      "Epoch:          1 | Training loss: 0.573287712086761 | Validation loss: 0.6085990431068139\n",
      "Epoch:          2 | Training loss: 0.5442204653596203 | Validation loss: 0.5904178527757616\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1.778279410038923e-05  Regularization Parameter- 0.00017782794100389227\n",
      "Epoch:          0 | Training loss: 0.6075516883692607 | Validation loss: 0.6298198122623159\n",
      "Epoch:          1 | Training loss: 0.5732896406030109 | Validation loss: 0.6086009710851961\n",
      "Epoch:          2 | Training loss: 0.5442237986429019 | Validation loss: 0.5904211851760399\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 1.778279410038923e-05  Regularization Parameter- 0.001\n",
      "Epoch:          0 | Training loss: 0.6075555578040868 | Validation loss: 0.6298236805932138\n",
      "Epoch:          1 | Training loss: 0.5733004854457316 | Validation loss: 0.6086118129032656\n",
      "Epoch:          2 | Training loss: 0.5442425430698927 | Validation loss: 0.5904399246375394\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.0001  Regularization Parameter- 1e-06\n",
      "Epoch:          0 | Training loss: 0.7187504184632367 | Validation loss: 0.8178784760098141\n",
      "Epoch:          1 | Training loss: 0.46459429474166736 | Validation loss: 0.5627547775663957\n",
      "Epoch:          2 | Training loss: 0.4102320275083679 | Validation loss: 0.5438768023962375\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.0001  Regularization Parameter- 5.623413251903491e-06\n",
      "Epoch:          0 | Training loss: 0.7187510390189372 | Validation loss: 0.8178790967879026\n",
      "Epoch:          1 | Training loss: 0.4645955671472732 | Validation loss: 0.5627560496778525\n",
      "Epoch:          2 | Training loss: 0.4102339763897723 | Validation loss: 0.5438787507503511\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.0001  Regularization Parameter- 3.1622776601683795e-05\n",
      "Epoch:          0 | Training loss: 0.7187545286600358 | Validation loss: 0.8178825876795679\n",
      "Epoch:          1 | Training loss: 0.4646027224097177 | Validation loss: 0.5627632032861737\n",
      "Epoch:          2 | Training loss: 0.410244935755087 | Validation loss: 0.5438897071504911\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.0001  Regularization Parameter- 0.00017782794100389227\n",
      "Epoch:          0 | Training loss: 0.7187741523532082 | Validation loss: 0.8179022184051967\n",
      "Epoch:          1 | Training loss: 0.4646429594042212 | Validation loss: 0.5628034309788592\n",
      "Epoch:          2 | Training loss: 0.4103065647888421 | Validation loss: 0.5439513195098469\n",
      "\n",
      "Hyper Parameters -- Learning Rate - 0.0001  Regularization Parameter- 0.001\n",
      "Epoch:          0 | Training loss: 0.7188845044624522 | Validation loss: 0.8180126100607036\n",
      "Epoch:          1 | Training loss: 0.4648692285527607 | Validation loss: 0.563029647819462\n",
      "Epoch:          2 | Training loss: 0.41065313011197485 | Validation loss: 0.5442977910660111\n"
     ]
    }
   ],
   "source": [
    "#hyper paramater tuning for BOW-Count\n",
    "train_loss_history =[]\n",
    "validation_loss_history=[]\n",
    "hyperparam_history=[]\n",
    "#\n",
    "for lr_i in lr:\n",
    "    for alpha_i in alpha:\n",
    "        print('\\nHyper Parameters -- Learning Rate -',lr_i,' Regularization Parameter-',alpha_i)\n",
    "        weight, train_loss_count, dev_loss_count = SGD(X_tr=train_BOW_BOCN_count_vector,\n",
    "                                             Y_tr=train_labels_new,\n",
    "                                             X_dev=dev_BOW_BOCN_count_vector,\n",
    "                                             Y_dev=dev_labels_new,\n",
    "                                             lr=lr_i,\n",
    "                                             alpha=alpha_i,\n",
    "                                             tolerance=0.0001,\n",
    "                                             epochs=400,\n",
    "                                             print_progress=True)\n",
    "        if len(train_loss_count) > 5:\n",
    "            train_loss_history.append(train_loss_count[-1])\n",
    "            validation_loss_history.append(dev_loss_count[-1])\n",
    "            hyperparam_history.append([lr_i,alpha_i])\n",
    "     \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW-BOCN Count\n",
      "Best Training loss - 0.6672305745449177,best_validation_loss-0.6402276905337101,\n",
      "best_hyperparams-[4.6e-05, 1e-07]\n"
     ]
    }
   ],
   "source": [
    "#get the best hyper parameters\n",
    "temp = np.array(train_loss_history)\n",
    "train_loss_history_sortidx = np.argsort(temp)\n",
    "\n",
    "best_train_loss_BOW_BOCN = temp[train_loss_history_sortidx[0]]\n",
    "best_validation_loss_BOW_BOCN = validation_loss_history[train_loss_history_sortidx[0]]\n",
    "best_hyperparams_BOW_BOCN=hyperparam_history[train_loss_history_sortidx[0]]\n",
    "\n",
    "print(\"BOW-BOCN Count\")\n",
    "print(f'Best Training loss - {best_train_loss_BOCN_tfidf},best_validation_loss-{best_validation_loss_BOCN_tfidf},\\nbest_hyperparams-{best_hyperparams_BOCN_tfidf}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:          0 | Training loss: 0.6914988723371794 | Validation loss: 0.6910366921400016\n",
      "Epoch:          1 | Training loss: 0.6906433838251786 | Validation loss: 0.6901687257731097\n",
      "Epoch:          2 | Training loss: 0.6898480819482578 | Validation loss: 0.6899762427117031\n",
      "Epoch:          3 | Training loss: 0.6889836180180043 | Validation loss: 0.6887675108274282\n",
      "Epoch:          4 | Training loss: 0.6880862299850675 | Validation loss: 0.6885030132963333\n",
      "Epoch:          5 | Training loss: 0.6872352692115632 | Validation loss: 0.687810926729751\n",
      "Epoch:          6 | Training loss: 0.6864495400645508 | Validation loss: 0.6872754463998534\n",
      "Epoch:          7 | Training loss: 0.6855515977152251 | Validation loss: 0.6863246072840837\n",
      "Epoch:          8 | Training loss: 0.6847501892085522 | Validation loss: 0.6856497560338164\n",
      "Epoch:          9 | Training loss: 0.6839519394798088 | Validation loss: 0.6850338510460636\n",
      "Epoch:         10 | Training loss: 0.6831849832516707 | Validation loss: 0.6846160192225793\n",
      "Epoch:         11 | Training loss: 0.6824669829151961 | Validation loss: 0.683713474206681\n",
      "Epoch:         12 | Training loss: 0.6816166168220474 | Validation loss: 0.6832140060370623\n",
      "Epoch:         13 | Training loss: 0.6808497982559146 | Validation loss: 0.6826841662183735\n",
      "Epoch:         14 | Training loss: 0.680181505065827 | Validation loss: 0.682400522896496\n",
      "Epoch:         15 | Training loss: 0.6793597659850172 | Validation loss: 0.681577540404925\n",
      "Epoch:         16 | Training loss: 0.6788303406485361 | Validation loss: 0.6815098291993109\n",
      "Epoch:         17 | Training loss: 0.677904014957075 | Validation loss: 0.6803228785025351\n",
      "Epoch:         18 | Training loss: 0.6772225646321893 | Validation loss: 0.679711946813115\n"
     ]
    }
   ],
   "source": [
    "#train with best hyperparameters (BOW-BOCN)\n",
    "weight_c, train_loss_count_c, dev_loss_count_c = SGD(X_tr=train_BOW_BOCN_count_vector,\n",
    "                                             Y_tr=train_labels_new,\n",
    "                                             X_dev=dev_BOW_BOCN_count_vector,\n",
    "                                             Y_dev=dev_labels_new,\n",
    "                                             lr=best_hyperparams_BOW_BOCN[0],\n",
    "                                             alpha=best_hyperparams_BOW_BOCN[1],\n",
    "                                             tolerance=0.00001,\n",
    "                                             epochs=100,\n",
    "                                             print_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_count_c, label='Training loss')\n",
    "plt.plot(dev_loss_count_c, label='Validation loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.title('Training loss vs Validation loss (BOCN-Count)')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6275\n",
      "Precision: 0.580952380952381\n",
      "Recall: 0.915\n",
      "F1-Score: 0.7106796116504853\n"
     ]
    }
   ],
   "source": [
    "preds_te_count = predict_class(test_BOW_BOCN_count_vector, weight_c)\n",
    "print('Accuracy:', accuracy_score(test_labels_new,preds_te_count))\n",
    "print('Precision:', precision_score(test_labels_new,preds_te_count))\n",
    "print('Recall:', recall_score(test_labels_new,preds_te_count))\n",
    "print('F1-Score:', f1_score(test_labels_new,preds_te_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Full Results\n",
    "\n",
    "Add here your results:\n",
    "\n",
    "| LR | Precision  | Recall  | F1-Score  |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "| BOW-count  | 0.8349  | 0.834  | 0.8472  |\n",
    "| BOW-tfidf  | 0.8585  | 0.85  |  0.8542 |\n",
    "| BOCN-count  | 0.6775  | 0.935  | 0.7857  |\n",
    "| BOCN-tfidf  |0.5941   |0.965  |  0.7391 |\n",
    "| BOW+BOCN  | 0.5809  | 0.915  | 0.7106  |\n",
    "\n",
    "Please discuss why your best performing model is better than the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on the observations, BOW-tfidf is the best performing model out of all the models trained. The precision score is 0.8585 = 85.85% and Recall is 0.85. \n",
    "\n",
    "#### The model's performance is better than the other models because it is trained on ngrams model (unigram,bigram,trigram) with TFIDF weighting. we know that TFIDF performs better than raw frequencies in most of the cases. The BOCN models (count and TFIDF) worse than the BOW models. The BOW+BOCN(raw) model has the worst precision among the models. On contrary, BOW-TFIDF has the highest precision.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
